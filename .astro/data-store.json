[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","612f9676f55c45d1","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://figmentapp.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","docs",["Map",11,12,43,44,64,65,91,92,111,112,142,143,195,196,220,221,243,244,294,295,330,331,353,354,376,377,399,400,433,434,456,457,478,479,501,502,524,525,547,548,569,570,591,592,614,615,636,637,659,660,681,682,704,705,727,728,750,751,772,773,794,795,816,817,838,839,861,862,884,885,923,924,946,947,968,969,991,992,1013,1014,1035,1036,1058,1059,1081,1082,1104,1105,1127,1128,1150,1151,1173,1174,1194,1195,1217,1218,1242,1243,1263,1264,1286,1287,1309,1310,1331,1332,1354,1355,1376,1377,1399,1400,1422,1423,1463,1464,1486,1487,1509,1510,1532,1533,1555,1556,1577,1578,1599,1600,1623,1624,1645,1646,1668,1669,1691,1692],"export",{"id":11,"data":13,"body":21,"filePath":22,"digest":23,"rendered":24,"legacyId":42},{"title":14,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":18,"sidebar":19},"Exporting",true,"doc",false,[],{"hidden":17,"attrs":20},{},"# Exporting\n\nUse `File > Export` to export a sequence of images to disk. The export dialog will save all [Save image](/docs/nodes/save-image) nodes, so make sure you have those in your network.\n\n:::tip\nExporting will not work if you don't have any **Save Image** nodes in your network.\n:::\n\n## Frame rate\n\nBecause input nodes can have different settings, we set a _frame rate_ to give an indication of the speed at which to export. As an example, exporting 60 frames at a frame rate of 30fps would export a 2 second sequence. The frame rate is important for real-time nodes like webcam, because it will try to capture that many frames per second, if it can.\n\n## Options\n\nExport takes a number of options:\n\n- **Node** The node to export. Defaults to the Out node.\n- **Frames** The amount of frames to export.\n- **Frame rate** The frame rate to export, e.g. exporting 60 frames at a frame rate of 30fps would export a 2 second sequence.\n- **Folder** The folder to export to\n- **Prefix** The file prefix. Files will have this prefix and then a number, e.g. a prefix of `hands` will have files called `hands-0001.png` etc.\n- **Image Format** Choose between PNG and JPEG here. PNGs are lossless, JPEGs are smaller but with a reduced quality. For machine learning we often use JPEGs with a quality setting of 90.","src/content/docs/export.md","c6269af7360f32c6",{"html":25,"metadata":26},"\u003Ch1 id=\"exporting\">Exporting\u003C/h1>\n\u003Cp>Use \u003Ccode>File > Export\u003C/code> to export a sequence of images to disk. The export dialog will save all \u003Ca href=\"/docs/nodes/save-image\">Save image\u003C/a> nodes, so make sure you have those in your network.\u003C/p>\n\u003Cp>:::tip\nExporting will not work if you don’t have any \u003Cstrong>Save Image\u003C/strong> nodes in your network.\n:::\u003C/p>\n\u003Ch2 id=\"frame-rate\">Frame rate\u003C/h2>\n\u003Cp>Because input nodes can have different settings, we set a \u003Cem>frame rate\u003C/em> to give an indication of the speed at which to export. As an example, exporting 60 frames at a frame rate of 30fps would export a 2 second sequence. The frame rate is important for real-time nodes like webcam, because it will try to capture that many frames per second, if it can.\u003C/p>\n\u003Ch2 id=\"options\">Options\u003C/h2>\n\u003Cp>Export takes a number of options:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Node\u003C/strong> The node to export. Defaults to the Out node.\u003C/li>\n\u003Cli>\u003Cstrong>Frames\u003C/strong> The amount of frames to export.\u003C/li>\n\u003Cli>\u003Cstrong>Frame rate\u003C/strong> The frame rate to export, e.g. exporting 60 frames at a frame rate of 30fps would export a 2 second sequence.\u003C/li>\n\u003Cli>\u003Cstrong>Folder\u003C/strong> The folder to export to\u003C/li>\n\u003Cli>\u003Cstrong>Prefix\u003C/strong> The file prefix. Files will have this prefix and then a number, e.g. a prefix of \u003Ccode>hands\u003C/code> will have files called \u003Ccode>hands-0001.png\u003C/code> etc.\u003C/li>\n\u003Cli>\u003Cstrong>Image Format\u003C/strong> Choose between PNG and JPEG here. PNGs are lossless, JPEGs are smaller but with a reduced quality. For machine learning we often use JPEGs with a quality setting of 90.\u003C/li>\n\u003C/ul>",{"headings":27,"localImagePaths":38,"remoteImagePaths":39,"frontmatter":40,"imagePaths":41},[28,31,35],{"depth":29,"slug":30,"text":14},1,"exporting",{"depth":32,"slug":33,"text":34},2,"frame-rate","Frame rate",{"depth":32,"slug":36,"text":37},"options","Options",[],[],{"title":14},[],"export.md","index",{"id":43,"data":45,"body":50,"filePath":51,"digest":52,"rendered":53,"legacyId":63},{"title":46,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":47,"sidebar":48},"Welcome to Figment",[],{"hidden":17,"attrs":49},{},"# Welcome to Figment\n\nFigment is a **modular, node-based app for creative AI**. It fills the gap between pre-built large language and image models and code-heavy machine-learning frameworks, giving artists and researchers a visual environment to:\n\n- Load and curate datasets\n- Augment and transform images, video or sensor data\n- Train or fine-tune models\n- Run real-time inference and route the results to other creative software via OSC, MIDI or WebSockets\n\nEvery node is GPU-accelerated, so you can iterate at the speed of thought and deploy your pipelines in demanding contexts like live performances and long-running installations.\n\n![Figment Screenshot](/img/welcome/figment-screenshot.png)\n\nHead over to the **[Getting Started tutorial](/docs/tutorials/getting-started)** and build your first project in minutes.","src/content/docs/index.md","f35e2de1c52a6389",{"html":54,"metadata":55},"\u003Ch1 id=\"welcome-to-figment\">Welcome to Figment\u003C/h1>\n\u003Cp>Figment is a \u003Cstrong>modular, node-based app for creative AI\u003C/strong>. It fills the gap between pre-built large language and image models and code-heavy machine-learning frameworks, giving artists and researchers a visual environment to:\u003C/p>\n\u003Cul>\n\u003Cli>Load and curate datasets\u003C/li>\n\u003Cli>Augment and transform images, video or sensor data\u003C/li>\n\u003Cli>Train or fine-tune models\u003C/li>\n\u003Cli>Run real-time inference and route the results to other creative software via OSC, MIDI or WebSockets\u003C/li>\n\u003C/ul>\n\u003Cp>Every node is GPU-accelerated, so you can iterate at the speed of thought and deploy your pipelines in demanding contexts like live performances and long-running installations.\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/welcome/figment-screenshot.png\" alt=\"Figment Screenshot\">\u003C/p>\n\u003Cp>Head over to the \u003Cstrong>\u003Ca href=\"/docs/tutorials/getting-started\">Getting Started tutorial\u003C/a>\u003C/strong> and build your first project in minutes.\u003C/p>",{"headings":56,"localImagePaths":59,"remoteImagePaths":60,"frontmatter":61,"imagePaths":62},[57],{"depth":29,"slug":58,"text":46},"welcome-to-figment",[],[],{"title":46},[],"index.md","structuring",{"id":64,"data":66,"body":71,"filePath":72,"digest":73,"rendered":74,"legacyId":90},{"title":67,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":68,"sidebar":69},"Structuring your Figment project",[],{"hidden":17,"attrs":70},{},"# Structuring your Figment project\n\n## Relative Paths\n\nWhere possible, Figment will try to use relative paths. This only works if the file is saved first.\n\n## Out Node\n\nWhen rendering full screen, the last node should be the \"Out\" node. However, the Out node currently takes whatever input it gets and stretches it. Make sure the node before that is a \"resize\" node that is set to the resolution of the screen.","src/content/docs/structuring.md","c31671de9a16e9ba",{"html":75,"metadata":76},"\u003Ch1 id=\"structuring-your-figment-project\">Structuring your Figment project\u003C/h1>\n\u003Ch2 id=\"relative-paths\">Relative Paths\u003C/h2>\n\u003Cp>Where possible, Figment will try to use relative paths. This only works if the file is saved first.\u003C/p>\n\u003Ch2 id=\"out-node\">Out Node\u003C/h2>\n\u003Cp>When rendering full screen, the last node should be the “Out” node. However, the Out node currently takes whatever input it gets and stretches it. Make sure the node before that is a “resize” node that is set to the resolution of the screen.\u003C/p>",{"headings":77,"localImagePaths":86,"remoteImagePaths":87,"frontmatter":88,"imagePaths":89},[78,80,83],{"depth":29,"slug":79,"text":67},"structuring-your-figment-project",{"depth":32,"slug":81,"text":82},"relative-paths","Relative Paths",{"depth":32,"slug":84,"text":85},"out-node","Out Node",[],[],{"title":67},[],"structuring.md","tutorials",{"id":91,"data":93,"body":98,"filePath":99,"digest":100,"rendered":101,"legacyId":110},{"title":94,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":95,"sidebar":96},"Tutorials",[],{"hidden":17,"attrs":97},{},"# Tutorials\n\nThe basic tutorial is still a work in process. If you want to do some more advanced things, follow the [PIX2PIX tutorial](/docs/tutorials/pix2pix).","src/content/docs/tutorials/index.md","a57dc4972e6e5e50",{"html":102,"metadata":103},"\u003Ch1 id=\"tutorials\">Tutorials\u003C/h1>\n\u003Cp>The basic tutorial is still a work in process. If you want to do some more advanced things, follow the \u003Ca href=\"/docs/tutorials/pix2pix\">PIX2PIX tutorial\u003C/a>.\u003C/p>",{"headings":104,"localImagePaths":106,"remoteImagePaths":107,"frontmatter":108,"imagePaths":109},[105],{"depth":29,"slug":91,"text":94},[],[],{"title":94},[],"tutorials/index.md","tutorials/getting-started",{"id":111,"data":113,"body":118,"filePath":119,"digest":120,"rendered":121,"legacyId":141},{"title":114,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":115,"sidebar":116},"Getting Started with Figment",[],{"hidden":17,"attrs":117},{},"# Getting Started with Figment\n\nWatch the YouTube tutorial to get started with Figment:\n\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe  src=\"https://www.youtube-nocookie.com/embed/tWAMzHq9dPc?si=yDg5g8O6hXC1oiky&list=PLJegHlOw98OlCPFTEgKxcVHPwW_EYLOyu\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\n## Creating new nodes\n\n- Double-click anywhere on the canvas to create a new node.\n- Search for the node you want by scrolling through the list or typing the first characters of the name.\n- Hit enter or double-click to insert the node.\n\n## Parameters\n\n- Each node has specific parameters. Look in the node reference documentation to find out what they do.\n\n## Connecting\n\n- Connect the output of a node to the input of another node.\n- To disconnect, drag from the input node to the output node.\n\n\u003Cvideo muted autoplay style=\"width: 100%;\" src=\"/img/tutorials/getting-started/figment-connect-disconnect.mp4\">\u003C/video>\n\n## Exporting\n\n- Once you're done, you can export as an image sequence. Read the [documentation on exporting](/docs/export) for more info.","src/content/docs/tutorials/getting-started.md","af0ed944cb144794",{"html":122,"metadata":123},"\u003Ch1 id=\"getting-started-with-figment\">Getting Started with Figment\u003C/h1>\n\u003Cp>Watch the YouTube tutorial to get started with Figment:\u003C/p>\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe src=\"https://www.youtube-nocookie.com/embed/tWAMzHq9dPc?si=yDg5g8O6hXC1oiky&#x26;list=PLJegHlOw98OlCPFTEgKxcVHPwW_EYLOyu\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\u003Ch2 id=\"creating-new-nodes\">Creating new nodes\u003C/h2>\n\u003Cul>\n\u003Cli>Double-click anywhere on the canvas to create a new node.\u003C/li>\n\u003Cli>Search for the node you want by scrolling through the list or typing the first characters of the name.\u003C/li>\n\u003Cli>Hit enter or double-click to insert the node.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>Each node has specific parameters. Look in the node reference documentation to find out what they do.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"connecting\">Connecting\u003C/h2>\n\u003Cul>\n\u003Cli>Connect the output of a node to the input of another node.\u003C/li>\n\u003Cli>To disconnect, drag from the input node to the output node.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cvideo muted autoplay style=\"width: 100%;\" src=\"/img/tutorials/getting-started/figment-connect-disconnect.mp4\">\u003C/video>\u003C/p>\n\u003Ch2 id=\"exporting\">Exporting\u003C/h2>\n\u003Cul>\n\u003Cli>Once you’re done, you can export as an image sequence. Read the \u003Ca href=\"/docs/export\">documentation on exporting\u003C/a> for more info.\u003C/li>\n\u003C/ul>",{"headings":124,"localImagePaths":137,"remoteImagePaths":138,"frontmatter":139,"imagePaths":140},[125,127,130,133,136],{"depth":29,"slug":126,"text":114},"getting-started-with-figment",{"depth":32,"slug":128,"text":129},"creating-new-nodes","Creating new nodes",{"depth":32,"slug":131,"text":132},"parameters","Parameters",{"depth":32,"slug":134,"text":135},"connecting","Connecting",{"depth":32,"slug":30,"text":14},[],[],{"title":114},[],"tutorials/getting-started.md","tutorials/pix2pix",{"id":142,"data":144,"body":149,"filePath":150,"digest":151,"rendered":152,"legacyId":194},{"title":145,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":146,"sidebar":147},"Using Figment with PIX2PIX",[],{"hidden":17,"attrs":148},{},"# Using Figment with PIX2PIX\n\nFigment is an amazing tool for preparing data for machine learning models. We love [PIX2PIX](https://phillipi.github.io/pix2pix/) because it gives control and it can learn a lot from input data.\n\nThe best input data is _structurally similar_ to the input data, that is, there is a one-to-one relationship from the input to the output data. Here are some examples:\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/deoldify.jpg\" alt=\"Deoldify by Jason Antic\"/>\u003Cfigcaption>Deoldify by Jason Antic\u003C/figcaption>\u003C/figure>\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/fill-in-the-blanks.jpg\" alt=\"Fill in the blanks\"/>\u003Cfigcaption>\"Fill in the blanks\" — let the AI invent parts of the image by removing them\u003C/figcaption>\u003C/figure>\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/cats.jpg\" alt=\"Drawings to cats\"/>\u003Cfigcaption>Drawings to cats — create a photorealistic cat from a drawing\u003C/figcaption>\u003C/figure>\n\nThe _trick_ to making the training data is doing the _opposite_ transformation of what we're trying to acquire. So, as an example, to convert black and white image to color images, we're using existing color images and _removing_ the color information, then letting PIX2PIX learn the color mapping.\n\n## Video Tutorial\n\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe  src=\"https://www.youtube-nocookie.com/embed/CbB7kAb0UDM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\n## What we'll make\n\nWe're making a face generator that's built on artificial faces, using [This Person Does Not Exist](https://thispersondoesnotexist.com). In a way, we're creating a second-generation AI, based on existing AI.\n\nThis idea was actually developed by [Alexandra Fraser](https://www.alexandrafraser.eu) in her project [Maureen](https://algorithmicgaze.com/projects/maureen/).\n\n![Maureen by Alexandra Fraser](/img/tutorials/pix2pix/maureen.jpg)\n\n## Acquiring the data\n\nWe have a folder of data prepared that you can download. These are 5,000 images downloaded from the _This Person Does Not Exist_ website. Download the ZIP file here: [does-not-exist.zip](https://figmentapp.s3.amazonaws.com/datasets/does-not-exist.zip)\n\nHowever, we can also do this using a [Fetch Image node](/docs/nodes/fetch-image). In the case of this website, we can fetch the same URL repeatedly and get a different image every time:\n\n- Create a **Fetch Image** Node. Set the url to `https://thispersondoesnotexist.com/`, the \"refresh\" on and the refresh time to 1 second.\n- Create a **Save Image** Node. Choose the folder.\n- Select File > Render and render out as much images as you want. Set the framerate to 1 (same as the refresh time).\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/fetch-image.png\" alt=\"Screenshot of Fetch Image setup\"/>\u003Cfigcaption>Screenshot of Fetch Image setup\u003C/figcaption>\u003C/figure>\n\n## Setting up Figment\n\nCreate a new project folder, e.g. on your desktop. Open Figment and immediately save the file in the project folder.\n\nPut your images folder in the project folder as well.\n\nIn your new project, delete all nodes. We're going to start from scratch.\n\nCreate a [Load Image Folder node](/docs/nodes/load-image-folder), click the \"Choose\" button next to the folder, and select the images folder. The images should now be \"animating\":\n\n\u003Cvideo autoplay muted loop src=\"/img/tutorials/pix2pix/load-image-folder.mp4\" style=\"width: 100%;\">\u003C/video>\n\n\u003Cbr/>\n\u003Cbr/>\n\n:::tip\n\nYou could also use the [webcam node](/docs/nodes/webcam-image) to generate an artificial, creepy version of yourself!\n\n:::\n\nThe PIX2PIX algorithm requires the input to be square. We're going to be using `512x512` images, so we'll use [Resize node](/docs/nodes/resize) to mold them into shape.\n\n- Create a `Resize` node.\n- Set the width and height both to `512`.\n- Set the fit mode to `cover`.\n- Connect the output of the `Load Image Folder` node to the input of the `Resize` node.\n\nSince these are faces, we want to use a face detection algorithm. The [Detect Faces node](/docs/nodes/detect-faces) works well here. It uses [Google's MediaPipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh) to detect face landmarks. Set it up to draw the face mesh.\n\n- Create a `Detect Faces` node.\n- Turn off _Draw Contours_\n- Turn on _Draw Tesselation_.\n- Connect the output of `Resize` to the input of `Detect Faces`.\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/detect-faces.png\" alt=\"Screenshot of Detect Faces setup\"/>\u003Cfigcaption>Screenshot of Detect Faces setup\u003C/figcaption>\u003C/figure>\n\nOur PIX2PIX implementation requires the two images side-by-side. We'll do that with a [Stack node](/docs/nodes/stack). Note that our final size should be `1024x512`, so we'll take the output of `Resize` and `Detect Faces`, which are both `512x512`.\n\n- Create a `Stack` node.\n- Connect the output of `Resize` to the first input of `Stack`.\n- Connect the output of `Detect Faces` to the second input of `Stack`.\n\nThe finishing touch:\n\n- Create a \"Save Image\" node.\n- Set the folder to save to.\n- In template, use `image-#####.jpg` to save the images with a number.\n- Connect the output of `Stack` to the `Save Image` node.\n\nWe're ready to export. We'll export 5000 frames (as many as we have input images) to an \"input\" folder.\n\n:::info\n\nWhy is the exported folder called \"input\"? It's because it's the **input** for the next step, which is the PIX2PIX machine learning algorithm.\n\n:::\n\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/prepare.png\" alt=\"Figment Prepare Project Setup\"/>\u003Cfigcaption>Screenshot of Figment with the prepared pipeline\u003C/figcaption>\u003C/figure>\n\n## Training the model with Paperspace\n\nWe'll use [Paperspace](https://paperspace.com) to train our model. We'll set up a new project, and use their dataset feature to create a number of datasets. We'll use our own open-source pix2pix implementation to generate the model, then convert it to Figment.\n\nFirst, create an account on Paperspace. Once you're in the console, create a new project. You can call it anything you like. You should be on the notebooks tab:\n\n![Empty notebook tab](/img/tutorials/pix2pix/gradient-notebooks-empty.jpg)\n\nFirst, upload the data. Switch to the data tab:\n\n![Empty data tab](/img/tutorials/pix2pix/gradient-data-empty.jpg)\n\nChoose \"Add\" to create a new dataset. I called mine `does-not-exist`. Then, upload the 5,000 images you prepared.\n\n:::info\n\nData uploaded this way will be available in the `/storage` directory in a Paperspace notebook.\n\n:::\n\nGo to the Notebook tab and create a new notebook. Choose \"Tensorflow\". Select a GPU machine (you can choose a free one, though it will take a while for results to appear).\n\nIn the advanced options, set the workspace URL to:\n\n`https://github.com/figmentapp/pix2pix`\n\n![Set URL correctly](/img/tutorials/pix2pix/gradient-advanced-options.jpg)\n\nPress \"Start Notebook\" to boot up the notebook. It will say \"Starting\" for a bit, then show your files on the sidebar.\n\n#### Mount the data sources\n\nClick the \"data sources\" icon in the left sidebar. Find your data source (mine is called \"does-not-exist\" and click the \"Mount\" button)\n\n![Mount the data source](/img/tutorials/pix2pix/gradient-mount.jpg)\n\nOnce mounted, it should show the directory path for the dataset. Mine is `/datasets/does-not-exist`, so that is what we'll need to fill in.\n\n![Mounted data sources](/img/tutorials/pix2pix/gradient-mount-point.jpg)\n\n### Training with PIX2PIX\n\nOpen `pix2pix_train.ipynb`. The script is all set up, you just need to update the paths:\n\n- Find the cell that starts with `project_dir`. Set the project directory to `\"_projects/does_not_exist\"`.\n- Change the `dataset_dir` to `\"/datasets/does-not-exist\"`.\n- Leave the rest as-is\n\n![Training paths](/img/tutorials/pix2pix/gradient-train-paths.jpg)\n\nAt the very end, you might also want to tweak the number of steps the model runs. This is set quite high (500_000 steps), which will take a long time to run. Setting it to 100_000 steps might already give decent results.\n\nOnce all cells are setup, you can press \"Run all\" at the top to run all cells. Scroll through the script to see if there are any errors. If not, you should see training happening at the end of the notebook. There will be dots slowly moving across the screen, indicating progress.\n\nInitially the training will give random noise, as the model is untrained:\n\n![Training at step 0](/img/tutorials/pix2pix/gradient-train-step-0.jpg)\n\nBut after a while the results should improve:\n\n![Training at step 1000](/img/tutorials/pix2pix/gradient-train-step-1000.jpg)\n\nOnce training is completed, make sure the last cells are ran also.\n\nIf training takes too long, you can also stop the cell (press the stop button). You still get a working model; just make sure you execute the last two cells (saving the checkpoint and the generator).\n\nClose and shutdown the script. Make sure in \"kernel sessions\" you delete the notebook by clicking the thrash icon.\n\n![Delete session](/img/tutorials/pix2pix/gradient-session-delete.jpg)\n\n### Converting the model\n\nFigment requires the model in [Tensorflow.js](https://www.tensorflow.org/js/) format (TFJS). We've provided a conversion script that you can run once your model is done training.\n\nOpen `pix2pix_tfjs.ipynb`. In this script, make sure the paths are set correctly:\n\n- project_dir should be `_projects/does_not_exist`.\n- The rest of the paths should be correct.\n\n![Paths for tfjs script](/img/tutorials/pix2pix/gradient-tfjs-paths.jpg)\n\n:::info\n\nIf you see the output mention something about \"out of memory\", it means you're training script is still running. Make sure to shut it down with the \"trash\" icon under \"Kernel Sessions\".\n\n:::\n\n### Downloading the model\n\nFind the `tfjs.zip` file in the Files sidebar. It should be under `does-not-exist>output>v001`. Right-click it and choose \"Download\" (Left-clicking will show an error to the effect of \"I can't show this file\"). Once you have the file downloaded, unzip it.\n\n## Building the real-time script in Figment\n\nThe Figment real-time script is very similar to the generation script. Only you will now use the webcam as the input.\n\n- Create a Webcam Image node.\n- Create a Resize node and connect it to the output of the Webcam Image node. Make sure it's set to 512x512.\n- Create a Detect Faces node and connect it to the output of the Resize node. Turn off Draw Contours and turn on Draw Tesselation (as you did in the other example).\n- Create a Image to Image model (you might need to scroll down to find it). Connect it to the output of the Detect Faces node. For the model, choose the unzipped tfjs folder.\n\nYou should now see your own face being recreated with virtual faces from This Person Does Not Exist.\n\nHere's an example with a model that's trained for a number of days:\n\n\u003Cfigure>\n\u003Cvideo loop=\"true\" autoplay=\"true\" muted=\"true\" src=\"https://tag-site.s3-eu-central-1.amazonaws.com/maureen/maureen-2.mp4\" width=\"100%\"/>\n\u003Cfigcaption>Maureen and a realtime face, side-by-side.\u003C/figcaption>\n\u003C/figure>","src/content/docs/tutorials/pix2pix.md","d70aa4a13a2ba579",{"html":153,"metadata":154},"\u003Ch1 id=\"using-figment-with-pix2pix\">Using Figment with PIX2PIX\u003C/h1>\n\u003Cp>Figment is an amazing tool for preparing data for machine learning models. We love \u003Ca href=\"https://phillipi.github.io/pix2pix/\">PIX2PIX\u003C/a> because it gives control and it can learn a lot from input data.\u003C/p>\n\u003Cp>The best input data is \u003Cem>structurally similar\u003C/em> to the input data, that is, there is a one-to-one relationship from the input to the output data. Here are some examples:\u003C/p>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/deoldify.jpg\" alt=\"Deoldify by Jason Antic\">\u003Cfigcaption>Deoldify by Jason Antic\u003C/figcaption>\u003C/figure>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/fill-in-the-blanks.jpg\" alt=\"Fill in the blanks\">\u003Cfigcaption>\"Fill in the blanks\" — let the AI invent parts of the image by removing them\u003C/figcaption>\u003C/figure>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/cats.jpg\" alt=\"Drawings to cats\">\u003Cfigcaption>Drawings to cats — create a photorealistic cat from a drawing\u003C/figcaption>\u003C/figure>\n\u003Cp>The \u003Cem>trick\u003C/em> to making the training data is doing the \u003Cem>opposite\u003C/em> transformation of what we’re trying to acquire. So, as an example, to convert black and white image to color images, we’re using existing color images and \u003Cem>removing\u003C/em> the color information, then letting PIX2PIX learn the color mapping.\u003C/p>\n\u003Ch2 id=\"video-tutorial\">Video Tutorial\u003C/h2>\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe src=\"https://www.youtube-nocookie.com/embed/CbB7kAb0UDM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\u003Ch2 id=\"what-well-make\">What we’ll make\u003C/h2>\n\u003Cp>We’re making a face generator that’s built on artificial faces, using \u003Ca href=\"https://thispersondoesnotexist.com\">This Person Does Not Exist\u003C/a>. In a way, we’re creating a second-generation AI, based on existing AI.\u003C/p>\n\u003Cp>This idea was actually developed by \u003Ca href=\"https://www.alexandrafraser.eu\">Alexandra Fraser\u003C/a> in her project \u003Ca href=\"https://algorithmicgaze.com/projects/maureen/\">Maureen\u003C/a>.\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/maureen.jpg\" alt=\"Maureen by Alexandra Fraser\">\u003C/p>\n\u003Ch2 id=\"acquiring-the-data\">Acquiring the data\u003C/h2>\n\u003Cp>We have a folder of data prepared that you can download. These are 5,000 images downloaded from the \u003Cem>This Person Does Not Exist\u003C/em> website. Download the ZIP file here: \u003Ca href=\"https://figmentapp.s3.amazonaws.com/datasets/does-not-exist.zip\">does-not-exist.zip\u003C/a>\u003C/p>\n\u003Cp>However, we can also do this using a \u003Ca href=\"/docs/nodes/fetch-image\">Fetch Image node\u003C/a>. In the case of this website, we can fetch the same URL repeatedly and get a different image every time:\u003C/p>\n\u003Cul>\n\u003Cli>Create a \u003Cstrong>Fetch Image\u003C/strong> Node. Set the url to \u003Ccode>https://thispersondoesnotexist.com/\u003C/code>, the “refresh” on and the refresh time to 1 second.\u003C/li>\n\u003Cli>Create a \u003Cstrong>Save Image\u003C/strong> Node. Choose the folder.\u003C/li>\n\u003Cli>Select File > Render and render out as much images as you want. Set the framerate to 1 (same as the refresh time).\u003C/li>\n\u003C/ul>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/fetch-image.png\" alt=\"Screenshot of Fetch Image setup\">\u003Cfigcaption>Screenshot of Fetch Image setup\u003C/figcaption>\u003C/figure>\n\u003Ch2 id=\"setting-up-figment\">Setting up Figment\u003C/h2>\n\u003Cp>Create a new project folder, e.g. on your desktop. Open Figment and immediately save the file in the project folder.\u003C/p>\n\u003Cp>Put your images folder in the project folder as well.\u003C/p>\n\u003Cp>In your new project, delete all nodes. We’re going to start from scratch.\u003C/p>\n\u003Cp>Create a \u003Ca href=\"/docs/nodes/load-image-folder\">Load Image Folder node\u003C/a>, click the “Choose” button next to the folder, and select the images folder. The images should now be “animating”:\u003C/p>\n\u003Cp>\u003Cvideo autoplay muted loop src=\"/img/tutorials/pix2pix/load-image-folder.mp4\" style=\"width: 100%;\">\u003C/video>\u003C/p>\n\u003Cbr>\n\u003Cbr>\n\u003Cp>:::tip\u003C/p>\n\u003Cp>You could also use the \u003Ca href=\"/docs/nodes/webcam-image\">webcam node\u003C/a> to generate an artificial, creepy version of yourself!\u003C/p>\n\u003Cp>:::\u003C/p>\n\u003Cp>The PIX2PIX algorithm requires the input to be square. We’re going to be using \u003Ccode>512x512\u003C/code> images, so we’ll use \u003Ca href=\"/docs/nodes/resize\">Resize node\u003C/a> to mold them into shape.\u003C/p>\n\u003Cul>\n\u003Cli>Create a \u003Ccode>Resize\u003C/code> node.\u003C/li>\n\u003Cli>Set the width and height both to \u003Ccode>512\u003C/code>.\u003C/li>\n\u003Cli>Set the fit mode to \u003Ccode>cover\u003C/code>.\u003C/li>\n\u003Cli>Connect the output of the \u003Ccode>Load Image Folder\u003C/code> node to the input of the \u003Ccode>Resize\u003C/code> node.\u003C/li>\n\u003C/ul>\n\u003Cp>Since these are faces, we want to use a face detection algorithm. The \u003Ca href=\"/docs/nodes/detect-faces\">Detect Faces node\u003C/a> works well here. It uses \u003Ca href=\"https://google.github.io/mediapipe/solutions/face_mesh\">Google’s MediaPipe Face Mesh\u003C/a> to detect face landmarks. Set it up to draw the face mesh.\u003C/p>\n\u003Cul>\n\u003Cli>Create a \u003Ccode>Detect Faces\u003C/code> node.\u003C/li>\n\u003Cli>Turn off \u003Cem>Draw Contours\u003C/em>\u003C/li>\n\u003Cli>Turn on \u003Cem>Draw Tesselation\u003C/em>.\u003C/li>\n\u003Cli>Connect the output of \u003Ccode>Resize\u003C/code> to the input of \u003Ccode>Detect Faces\u003C/code>.\u003C/li>\n\u003C/ul>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/detect-faces.png\" alt=\"Screenshot of Detect Faces setup\">\u003Cfigcaption>Screenshot of Detect Faces setup\u003C/figcaption>\u003C/figure>\n\u003Cp>Our PIX2PIX implementation requires the two images side-by-side. We’ll do that with a \u003Ca href=\"/docs/nodes/stack\">Stack node\u003C/a>. Note that our final size should be \u003Ccode>1024x512\u003C/code>, so we’ll take the output of \u003Ccode>Resize\u003C/code> and \u003Ccode>Detect Faces\u003C/code>, which are both \u003Ccode>512x512\u003C/code>.\u003C/p>\n\u003Cul>\n\u003Cli>Create a \u003Ccode>Stack\u003C/code> node.\u003C/li>\n\u003Cli>Connect the output of \u003Ccode>Resize\u003C/code> to the first input of \u003Ccode>Stack\u003C/code>.\u003C/li>\n\u003Cli>Connect the output of \u003Ccode>Detect Faces\u003C/code> to the second input of \u003Ccode>Stack\u003C/code>.\u003C/li>\n\u003C/ul>\n\u003Cp>The finishing touch:\u003C/p>\n\u003Cul>\n\u003Cli>Create a “Save Image” node.\u003C/li>\n\u003Cli>Set the folder to save to.\u003C/li>\n\u003Cli>In template, use \u003Ccode>image-#####.jpg\u003C/code> to save the images with a number.\u003C/li>\n\u003Cli>Connect the output of \u003Ccode>Stack\u003C/code> to the \u003Ccode>Save Image\u003C/code> node.\u003C/li>\n\u003C/ul>\n\u003Cp>We’re ready to export. We’ll export 5000 frames (as many as we have input images) to an “input” folder.\u003C/p>\n\u003Cp>:::info\u003C/p>\n\u003Cp>Why is the exported folder called “input”? It’s because it’s the \u003Cstrong>input\u003C/strong> for the next step, which is the PIX2PIX machine learning algorithm.\u003C/p>\n\u003Cp>:::\u003C/p>\n\u003Cfigure>\u003Cimg src=\"/img/tutorials/pix2pix/prepare.png\" alt=\"Figment Prepare Project Setup\">\u003Cfigcaption>Screenshot of Figment with the prepared pipeline\u003C/figcaption>\u003C/figure>\n\u003Ch2 id=\"training-the-model-with-paperspace\">Training the model with Paperspace\u003C/h2>\n\u003Cp>We’ll use \u003Ca href=\"https://paperspace.com\">Paperspace\u003C/a> to train our model. We’ll set up a new project, and use their dataset feature to create a number of datasets. We’ll use our own open-source pix2pix implementation to generate the model, then convert it to Figment.\u003C/p>\n\u003Cp>First, create an account on Paperspace. Once you’re in the console, create a new project. You can call it anything you like. You should be on the notebooks tab:\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-notebooks-empty.jpg\" alt=\"Empty notebook tab\">\u003C/p>\n\u003Cp>First, upload the data. Switch to the data tab:\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-data-empty.jpg\" alt=\"Empty data tab\">\u003C/p>\n\u003Cp>Choose “Add” to create a new dataset. I called mine \u003Ccode>does-not-exist\u003C/code>. Then, upload the 5,000 images you prepared.\u003C/p>\n\u003Cp>:::info\u003C/p>\n\u003Cp>Data uploaded this way will be available in the \u003Ccode>/storage\u003C/code> directory in a Paperspace notebook.\u003C/p>\n\u003Cp>:::\u003C/p>\n\u003Cp>Go to the Notebook tab and create a new notebook. Choose “Tensorflow”. Select a GPU machine (you can choose a free one, though it will take a while for results to appear).\u003C/p>\n\u003Cp>In the advanced options, set the workspace URL to:\u003C/p>\n\u003Cp>\u003Ccode>https://github.com/figmentapp/pix2pix\u003C/code>\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-advanced-options.jpg\" alt=\"Set URL correctly\">\u003C/p>\n\u003Cp>Press “Start Notebook” to boot up the notebook. It will say “Starting” for a bit, then show your files on the sidebar.\u003C/p>\n\u003Ch4 id=\"mount-the-data-sources\">Mount the data sources\u003C/h4>\n\u003Cp>Click the “data sources” icon in the left sidebar. Find your data source (mine is called “does-not-exist” and click the “Mount” button)\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-mount.jpg\" alt=\"Mount the data source\">\u003C/p>\n\u003Cp>Once mounted, it should show the directory path for the dataset. Mine is \u003Ccode>/datasets/does-not-exist\u003C/code>, so that is what we’ll need to fill in.\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-mount-point.jpg\" alt=\"Mounted data sources\">\u003C/p>\n\u003Ch3 id=\"training-with-pix2pix\">Training with PIX2PIX\u003C/h3>\n\u003Cp>Open \u003Ccode>pix2pix_train.ipynb\u003C/code>. The script is all set up, you just need to update the paths:\u003C/p>\n\u003Cul>\n\u003Cli>Find the cell that starts with \u003Ccode>project_dir\u003C/code>. Set the project directory to \u003Ccode>\"_projects/does_not_exist\"\u003C/code>.\u003C/li>\n\u003Cli>Change the \u003Ccode>dataset_dir\u003C/code> to \u003Ccode>\"/datasets/does-not-exist\"\u003C/code>.\u003C/li>\n\u003Cli>Leave the rest as-is\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-train-paths.jpg\" alt=\"Training paths\">\u003C/p>\n\u003Cp>At the very end, you might also want to tweak the number of steps the model runs. This is set quite high (500_000 steps), which will take a long time to run. Setting it to 100_000 steps might already give decent results.\u003C/p>\n\u003Cp>Once all cells are setup, you can press “Run all” at the top to run all cells. Scroll through the script to see if there are any errors. If not, you should see training happening at the end of the notebook. There will be dots slowly moving across the screen, indicating progress.\u003C/p>\n\u003Cp>Initially the training will give random noise, as the model is untrained:\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-train-step-0.jpg\" alt=\"Training at step 0\">\u003C/p>\n\u003Cp>But after a while the results should improve:\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-train-step-1000.jpg\" alt=\"Training at step 1000\">\u003C/p>\n\u003Cp>Once training is completed, make sure the last cells are ran also.\u003C/p>\n\u003Cp>If training takes too long, you can also stop the cell (press the stop button). You still get a working model; just make sure you execute the last two cells (saving the checkpoint and the generator).\u003C/p>\n\u003Cp>Close and shutdown the script. Make sure in “kernel sessions” you delete the notebook by clicking the thrash icon.\u003C/p>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-session-delete.jpg\" alt=\"Delete session\">\u003C/p>\n\u003Ch3 id=\"converting-the-model\">Converting the model\u003C/h3>\n\u003Cp>Figment requires the model in \u003Ca href=\"https://www.tensorflow.org/js/\">Tensorflow.js\u003C/a> format (TFJS). We’ve provided a conversion script that you can run once your model is done training.\u003C/p>\n\u003Cp>Open \u003Ccode>pix2pix_tfjs.ipynb\u003C/code>. In this script, make sure the paths are set correctly:\u003C/p>\n\u003Cul>\n\u003Cli>project_dir should be \u003Ccode>_projects/does_not_exist\u003C/code>.\u003C/li>\n\u003Cli>The rest of the paths should be correct.\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cimg src=\"/img/tutorials/pix2pix/gradient-tfjs-paths.jpg\" alt=\"Paths for tfjs script\">\u003C/p>\n\u003Cp>:::info\u003C/p>\n\u003Cp>If you see the output mention something about “out of memory”, it means you’re training script is still running. Make sure to shut it down with the “trash” icon under “Kernel Sessions”.\u003C/p>\n\u003Cp>:::\u003C/p>\n\u003Ch3 id=\"downloading-the-model\">Downloading the model\u003C/h3>\n\u003Cp>Find the \u003Ccode>tfjs.zip\u003C/code> file in the Files sidebar. It should be under \u003Ccode>does-not-exist>output>v001\u003C/code>. Right-click it and choose “Download” (Left-clicking will show an error to the effect of “I can’t show this file”). Once you have the file downloaded, unzip it.\u003C/p>\n\u003Ch2 id=\"building-the-real-time-script-in-figment\">Building the real-time script in Figment\u003C/h2>\n\u003Cp>The Figment real-time script is very similar to the generation script. Only you will now use the webcam as the input.\u003C/p>\n\u003Cul>\n\u003Cli>Create a Webcam Image node.\u003C/li>\n\u003Cli>Create a Resize node and connect it to the output of the Webcam Image node. Make sure it’s set to 512x512.\u003C/li>\n\u003Cli>Create a Detect Faces node and connect it to the output of the Resize node. Turn off Draw Contours and turn on Draw Tesselation (as you did in the other example).\u003C/li>\n\u003Cli>Create a Image to Image model (you might need to scroll down to find it). Connect it to the output of the Detect Faces node. For the model, choose the unzipped tfjs folder.\u003C/li>\n\u003C/ul>\n\u003Cp>You should now see your own face being recreated with virtual faces from This Person Does Not Exist.\u003C/p>\n\u003Cp>Here’s an example with a model that’s trained for a number of days:\u003C/p>\n\u003Cfigure>\n\u003Cvideo loop=\"true\" autoplay=\"true\" muted=\"true\" src=\"https://tag-site.s3-eu-central-1.amazonaws.com/maureen/maureen-2.mp4\" width=\"100%\">\n\u003Cfigcaption>Maureen and a realtime face, side-by-side.\u003C/figcaption>\n\u003C/video>\u003C/figure>",{"headings":155,"localImagePaths":190,"remoteImagePaths":191,"frontmatter":192,"imagePaths":193},[156,158,161,164,167,170,173,177,181,184,187],{"depth":29,"slug":157,"text":145},"using-figment-with-pix2pix",{"depth":32,"slug":159,"text":160},"video-tutorial","Video Tutorial",{"depth":32,"slug":162,"text":163},"what-well-make","What we’ll make",{"depth":32,"slug":165,"text":166},"acquiring-the-data","Acquiring the data",{"depth":32,"slug":168,"text":169},"setting-up-figment","Setting up Figment",{"depth":32,"slug":171,"text":172},"training-the-model-with-paperspace","Training the model with Paperspace",{"depth":174,"slug":175,"text":176},4,"mount-the-data-sources","Mount the data sources",{"depth":178,"slug":179,"text":180},3,"training-with-pix2pix","Training with PIX2PIX",{"depth":178,"slug":182,"text":183},"converting-the-model","Converting the model",{"depth":178,"slug":185,"text":186},"downloading-the-model","Downloading the model",{"depth":32,"slug":188,"text":189},"building-the-real-time-script-in-figment","Building the real-time script in Figment",[],[],{"title":145},[],"tutorials/pix2pix.md","nodes/barrel-distortion",{"id":195,"data":197,"body":202,"filePath":203,"digest":204,"rendered":205,"legacyId":219},{"title":198,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":199,"sidebar":200},"Barrel Distortion",[],{"hidden":17,"attrs":201},{},"# Barrel Distortion\n\nThe node computes a barrel distortion on the input image.\n\n## Parameters\n\n- **Distortion**: The amount of distortion.\n- **Radius**: The radius of the barrel.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/barrel.jpg\" alt=\"Figment barrel distortion node example\"/>","src/content/docs/nodes/barrel-distortion.md","8dc64cc388d3847f",{"html":206,"metadata":207},"\u003Ch1 id=\"barrel-distortion\">Barrel Distortion\u003C/h1>\n\u003Cp>The node computes a barrel distortion on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Distortion\u003C/strong>: The amount of distortion.\u003C/li>\n\u003Cli>\u003Cstrong>Radius\u003C/strong>: The radius of the barrel.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/barrel.jpg\" alt=\"Figment barrel distortion node example\">",{"headings":208,"localImagePaths":215,"remoteImagePaths":216,"frontmatter":217,"imagePaths":218},[209,211,212],{"depth":29,"slug":210,"text":198},"barrel-distortion",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},"example","Example",[],[],{"title":198},[],"nodes/barrel-distortion.md","nodes/blur",{"id":220,"data":222,"body":227,"filePath":228,"digest":229,"rendered":230,"legacyId":242},{"title":223,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":224,"sidebar":225},"Blur",[],{"hidden":17,"attrs":226},{},"# Blur\n\nThe node renders blur on the input image.\n\n## Parameters\n\n- **Amount**: The amount of blur.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/blur.jpg\" alt=\"Figment blur node example\"/>","src/content/docs/nodes/blur.md","57f2d9f4d50e1538",{"html":231,"metadata":232},"\u003Ch1 id=\"blur\">Blur\u003C/h1>\n\u003Cp>The node renders blur on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Amount\u003C/strong>: The amount of blur.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/blur.jpg\" alt=\"Figment blur node example\">",{"headings":233,"localImagePaths":238,"remoteImagePaths":239,"frontmatter":240,"imagePaths":241},[234,236,237],{"depth":29,"slug":235,"text":223},"blur",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":223},[],"nodes/blur.md","expressions",{"id":243,"data":245,"body":250,"filePath":251,"digest":252,"rendered":253,"legacyId":293},{"title":246,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":247,"sidebar":248},"Using Expressions",[],{"hidden":17,"attrs":249},{},"# Using Expressions\n\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe  src=\"https://www.youtube-nocookie.com/embed/Zo2Oev1pz10\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\n\u003Cbr/>\n\nAll values in Figment also support expressions. These are tiny snippets of code that allow you to change parameters dynamically. For example, you can use expressions to make a value change over time or react to external input via OSC.\n\nExpressions use a JavaScript-like syntax. A basic expression looks like this:\n\n```js\n$FRAME / 10;\n```\n\nThis takes the current frame (an ever-increasing value) and divides it by 10, to slow down the rate of change.\n\n## Using Expressions in Parameters\n\nTo use an expression, go to the parameter you want to change and click on the three dots, and choose \"Edit Expression\". The text field will turn green, indicating that it accepts an expression.\n\n\u003Cfigure>\u003Cimg src=\"/img/expressions/simple-expression.png\" alt=\"A simple expression on the rotate parameter\"/>\u003Cfigcaption>A simple expression on the rotate parameter\u003C/figcaption>\u003C/figure>\n\n## Built-in Variables\n\nFigment provides a number of built-in variables that you can use in your expressions:\n\n- `$TIME`: The current time in seconds\n- `$FRAME`: The current frame\n- `$NOW`: The current absolute time, in milliseconds\n\n## Built-in Functions\n\nA number of handy functions are built in to Figment as well:\n\n### Math Functions\n\n- `abs(x)`: Returns the absolute value of a number\n- `pow(x, y)`: Returns the value of x to the power of y\n- `sqrt(x)`: Returns the square root of x\n\n### Trigonometric Functions\n\n- `sin(x)`: Returns the sine of x\n- `cos(x)`: Returns the cosine of x\n- `tan(x)`: Returns the tangent of x\n\n### Random Functions\n\n- `random()`: Returns a random number between 0 and 1\n\n### Utility Functions\n\n- `clamp(x, min, max)`: Clamps a value between a minimum and maximum value\n- `lerp(a, b, t)`: Linearly interpolates between two values\n- `map(x, in_min, in_max, out_min, out_max)`: Maps a value from one range to another\n\n### Time Functions\n\n- `pingPong(min, max, period=1, type=\"smooth\", time=$TIME)` : Returns a value that oscillates between min and max over a period of time. The different types are `\"smooth\"` (sine wave), `\"linear\"` (sawtooth wave), or `\"step\"` (square wave).\n\n### Open Sound Control\n\n- `osc(address, defaultValue)` : Returns the value of an OSC address. The second parameter is the default value if no value was received.\n\n### MIDI\n\n- `midi(channel, controller, defaultValue)` : Returns the value of the knob on a MIDI controller. The first parameter is the midi channel; the second parameter is the controller (knob) number. The third parameter is the default value if no value was received. Use [Protokol](https://hexler.net/protokol) to figure out these values.\n- `midipc(channel, defaultValue)` : Returns the program number (0-127) from MIDI Program Change messages on the specified channel. Useful for switching between scenes in conditional nodes, e.g. `midipc(10) == 0` to check if program 0 is active on channel 10.","src/content/docs/expressions.md","71a0b1eaf993f9fd",{"html":254,"metadata":255},"\u003Ch1 id=\"using-expressions\">Using Expressions\u003C/h1>\n\u003Cdiv class=\"video-wrapper\">\n  \u003Ciframe src=\"https://www.youtube-nocookie.com/embed/Zo2Oev1pz10\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003C/div>\n\u003Cbr>\n\u003Cp>All values in Figment also support expressions. These are tiny snippets of code that allow you to change parameters dynamically. For example, you can use expressions to make a value change over time or react to external input via OSC.\u003C/p>\n\u003Cp>Expressions use a JavaScript-like syntax. A basic expression looks like this:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">$FRAME \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 10\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This takes the current frame (an ever-increasing value) and divides it by 10, to slow down the rate of change.\u003C/p>\n\u003Ch2 id=\"using-expressions-in-parameters\">Using Expressions in Parameters\u003C/h2>\n\u003Cp>To use an expression, go to the parameter you want to change and click on the three dots, and choose “Edit Expression”. The text field will turn green, indicating that it accepts an expression.\u003C/p>\n\u003Cfigure>\u003Cimg src=\"/img/expressions/simple-expression.png\" alt=\"A simple expression on the rotate parameter\">\u003Cfigcaption>A simple expression on the rotate parameter\u003C/figcaption>\u003C/figure>\n\u003Ch2 id=\"built-in-variables\">Built-in Variables\u003C/h2>\n\u003Cp>Figment provides a number of built-in variables that you can use in your expressions:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>$TIME\u003C/code>: The current time in seconds\u003C/li>\n\u003Cli>\u003Ccode>$FRAME\u003C/code>: The current frame\u003C/li>\n\u003Cli>\u003Ccode>$NOW\u003C/code>: The current absolute time, in milliseconds\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"built-in-functions\">Built-in Functions\u003C/h2>\n\u003Cp>A number of handy functions are built in to Figment as well:\u003C/p>\n\u003Ch3 id=\"math-functions\">Math Functions\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>abs(x)\u003C/code>: Returns the absolute value of a number\u003C/li>\n\u003Cli>\u003Ccode>pow(x, y)\u003C/code>: Returns the value of x to the power of y\u003C/li>\n\u003Cli>\u003Ccode>sqrt(x)\u003C/code>: Returns the square root of x\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"trigonometric-functions\">Trigonometric Functions\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>sin(x)\u003C/code>: Returns the sine of x\u003C/li>\n\u003Cli>\u003Ccode>cos(x)\u003C/code>: Returns the cosine of x\u003C/li>\n\u003Cli>\u003Ccode>tan(x)\u003C/code>: Returns the tangent of x\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"random-functions\">Random Functions\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>random()\u003C/code>: Returns a random number between 0 and 1\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"utility-functions\">Utility Functions\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>clamp(x, min, max)\u003C/code>: Clamps a value between a minimum and maximum value\u003C/li>\n\u003Cli>\u003Ccode>lerp(a, b, t)\u003C/code>: Linearly interpolates between two values\u003C/li>\n\u003Cli>\u003Ccode>map(x, in_min, in_max, out_min, out_max)\u003C/code>: Maps a value from one range to another\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"time-functions\">Time Functions\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>pingPong(min, max, period=1, type=\"smooth\", time=$TIME)\u003C/code> : Returns a value that oscillates between min and max over a period of time. The different types are \u003Ccode>\"smooth\"\u003C/code> (sine wave), \u003Ccode>\"linear\"\u003C/code> (sawtooth wave), or \u003Ccode>\"step\"\u003C/code> (square wave).\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"open-sound-control\">Open Sound Control\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>osc(address, defaultValue)\u003C/code> : Returns the value of an OSC address. The second parameter is the default value if no value was received.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"midi\">MIDI\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>midi(channel, controller, defaultValue)\u003C/code> : Returns the value of the knob on a MIDI controller. The first parameter is the midi channel; the second parameter is the controller (knob) number. The third parameter is the default value if no value was received. Use \u003Ca href=\"https://hexler.net/protokol\">Protokol\u003C/a> to figure out these values.\u003C/li>\n\u003Cli>\u003Ccode>midipc(channel, defaultValue)\u003C/code> : Returns the program number (0-127) from MIDI Program Change messages on the specified channel. Useful for switching between scenes in conditional nodes, e.g. \u003Ccode>midipc(10) == 0\u003C/code> to check if program 0 is active on channel 10.\u003C/li>\n\u003C/ul>",{"headings":256,"localImagePaths":289,"remoteImagePaths":290,"frontmatter":291,"imagePaths":292},[257,259,262,265,268,271,274,277,280,283,286],{"depth":29,"slug":258,"text":246},"using-expressions",{"depth":32,"slug":260,"text":261},"using-expressions-in-parameters","Using Expressions in Parameters",{"depth":32,"slug":263,"text":264},"built-in-variables","Built-in Variables",{"depth":32,"slug":266,"text":267},"built-in-functions","Built-in Functions",{"depth":178,"slug":269,"text":270},"math-functions","Math Functions",{"depth":178,"slug":272,"text":273},"trigonometric-functions","Trigonometric Functions",{"depth":178,"slug":275,"text":276},"random-functions","Random Functions",{"depth":178,"slug":278,"text":279},"utility-functions","Utility Functions",{"depth":178,"slug":281,"text":282},"time-functions","Time Functions",{"depth":178,"slug":284,"text":285},"open-sound-control","Open Sound Control",{"depth":178,"slug":287,"text":288},"midi","MIDI",[],[],{"title":246},[],"expressions.md","tutorials/custom-nodes",{"id":294,"data":296,"body":301,"filePath":302,"digest":303,"rendered":304,"legacyId":329},{"title":297,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":298,"sidebar":299},"Creating Custom Nodes",[],{"hidden":17,"attrs":300},{},"# Creating Custom Nodes\n\nBy writing a custom JavaScript node you can add brand‑new image‑processing, audio, or data‑generation to any project.\nThis guide walks you through the whole workflow – from the UI basics to the final code.\n\n## What a custom node looks like\n\nEvery Figment node follows the same scaffold:\n\n```js\n/**\n * @name  Node Name\n * @description  Short description of what the node does.\n * @category  category (e.g. image, audio, data)\n */\n\nconst inputPort = node.imageIn(\"in\");\n// Add here other parameters as needed...\nconst outputPort = node.imageOut(\"out\");\n\nnode.onStart = async () => {\n  /* one‑time init (shaders, textures, etc.) */\n};\n\nnode.onRender = () => {\n  /* per‑frame processing */\n};\n\nnode.onStop = () => {\n  /* optional cleanup */\n};\n```\n\nThe block above is the template you’ll paste into your new node after you “fork” a Null node (see the UI steps below).\n\n## Creating a custom node\n\nTo create a custom node you need to \"fork\" it from an existing node. If you want to tweak the code of an existing node, create that one. If you want to start from scratch, use the Null node:\n\n- **Create a Null node:** Double‑click anywhere on the canvas and pick Null from the list.\n- **Open the source editor:** Right‑click the new Null node → View Source.\n- **Fork the node:** At the bottom of the source panel click Fork.\n- **Name your node:** Give it a clear, descriptive name (e.g. “Custom Blur”) and confirm the fork.\n- **Replace the code:** In the editor, delete the existing skeleton and replace with your custom code.\n- **Build the node:** Click the \"Build\" button (or type Shift-Enter) to compile the node. Then switch back to the viewer to see the results.\n\nCustom nodes are stored in the `.fgmt` file as source code.\n\n:::info\nFigment is actually a custom browser (built in Electron). Use the developer tools (View > Toggle Developer Tools) to debug and inspect your custom node!\n:::\n\n## Example: weather forecast node\n\nWe're going to write an image processing node that changes the saturation of the image based on the current weather. To get the current weather, it will fetch it from [Open-Meteo](https://open-meteo.com). This API returns, next to temperature and wind speed, also a [WMO code](https://www.nodc.noaa.gov/archive/arc0021/0002199/1.1/data/0-data/HTML/WMO-CODE/WMO4677.HTM) indicating a global weather condition. We provided a mapping from the weather codes to the saturation values; change these as you see fit.\n\nThe node will take in the following inputs:\n\n- **`in`**: The input image to manipulate\n- **`lat`**: The desired latitude\n- **`lon`**: The desired longitude\n- **`interval`**: How often the node refreshes the weather (3 hours by default)\n\nIt will output the current image.\n\n```js\n/**\n * @name Weather Saturation\n * @description Adjusts image saturation based on current weather at (latitude, longitude). Fetches every N hours.\n * @category image\n */\n\nconst imageIn = node.imageIn(\"in\");\nconst latIn = node.numberIn(\"latitude\", 51.26, {\n  min: -90,\n  max: 90,\n  step: 0.01,\n});\nconst lonIn = node.numberIn(\"longitude\", 4.4, {\n  min: -180,\n  max: 180,\n  step: 0.01,\n});\nconst intervalIn = node.numberIn(\"interval\", 3, {\n  min: 0.25,\n  max: 24,\n  step: 0.25,\n});\nconst imageOut = node.imageOut(\"out\");\n\nconst DEFAULT_SAT = 0.7;\nconst WMO_SAT = {\n  0: 2.0, // Clear sky\n  1: 0.9, // Mainly clear\n  2: 0.8, // Partly cloudy\n  3: 0.65, // Overcast\n  45: 0.45,\n  48: 0.45, // Fog / Depositing rime fog\n  51: 0.55,\n  53: 0.55,\n  55: 0.55, // Drizzle (light/mod/heavy)\n  56: 0.45,\n  57: 0.45, // Freezing drizzle\n  61: 0.5,\n  63: 0.5,\n  65: 0.5, // Rain (light/mod/heavy)\n  66: 0.4,\n  67: 0.4, // Freezing rain\n  71: 0.55,\n  73: 0.55,\n  75: 0.55, // Snow (light/mod/heavy)\n  77: 0.55, // Snow grains\n  80: 0.5,\n  81: 0.5,\n  82: 0.5, // Rain showers (light/mod/heavy)\n  85: 0.55,\n  86: 0.55, // Snow showers (light/heavy)\n  95: 0.35, // Thunderstorm\n  96: 0.3,\n  99: 0.3, // Thunderstorm with hail (slight/heavy)\n};\n\nconst fragmentShader = `\n precision mediump float;\n uniform sampler2D u_input_texture;\n uniform float u_saturation;\n varying vec2 v_uv;\n\n void main() {\n   vec4 c = texture2D(u_input_texture, v_uv);\n   // Calculate perceptual luma (Rec. 709)\n   float luma = dot(c.rgb, vec3(0.2126, 0.7152, 0.0722));\n   vec3 grey = vec3(luma);\n   // Mix original color with saturation value\n   vec3 outRgb = mix(grey, c.rgb, luma);\n   gl_FragColor = vec4(outRgb, c.a);\n }\n `;\n\nlet _program, _framebuffer;\nlet _lastLat, _lastLon, _lastInterval; // This allows us to track changes\nlet _timer; // Interval timer\nlet _saturation = 0.75;\n\nnode.onStart = async () => {\n  _program = figment.createShaderProgram(fragmentShader);\n  _framebuffer = new figment.Framebuffer();\n  // Fetch immediately, then schedule periodic refreshes.\n  await updateWeather();\n  rescheduleTimer();\n};\n\nnode.onRender = () => {\n  if (!imageIn.value) return;\n\n  _framebuffer.setSize(imageIn.value.width, imageIn.value.height);\n  _framebuffer.bind();\n  figment.clear();\n  figment.drawQuad(_program, {\n    u_input_texture: imageIn.value.texture,\n    u_saturation: _saturation,\n  });\n  _framebuffer.unbind();\n  imageOut.set(_framebuffer);\n};\n\nasync function updateWeather() {\n  const lat = latIn.value;\n  const lon = lonIn.value;\n  const url = `https://api.open-meteo.com/v1/forecast?latitude=${lat}&longitude=${lon}&current_weather=true&timezone=auto`;\n  const res = await fetch(url);\n  if (!res.ok) throw new Error(`Open-Meteo HTTP ${res.status}`);\n  const json = await res.json();\n  const cw = json && json.current_weather;\n  if (!cw) throw new Error(\"No current_weather in response\");\n  // cw contains { temperature, windspeed, winddirection, weathercode, time }\n  _saturation = WMO_SAT[cw.weathercode] || DEFAULT_SAT;\n  console.log(\"Weather code:\", cw.weathercode, \"Saturation:\", _saturation);\n}\n\nfunction rescheduleTimer() {\n  if (_timer) clearInterval(_timer);\n  const intervalMs = Math.max(0.01, intervalIn.value) * 3600 * 1000;\n  _timer = setInterval(updateWeather, intervalMs);\n}\n\nlatIn.onChange = updateWeather;\nlonIn.onChange = updateWeather;\nintervalIn.onChange = rescheduleTimer;\n```\n\n## Get AI Help\n\nWe developed a custom ChatGPT and custom Gemini Gem that can help you write or debug custom nodes:\n\n\u003Ca href=\"https://chatgpt.com/g/g-68d3996b835481918330cb7509368404-figmentgpt\" target=\"_blank\" rel=\"noopener noreferrer\"\nstyle=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n\u003Cimg src=\"/img/tutorials/custom-nodes/figment-icon.png\" alt=\"Figment Icon\" style=\"border-radius: 100%; overflow: hidden; width: 44px; height: 44px;\" />\n\n   \u003Cdiv style=\"display: flex; flex-direction: column; row-gap: 0;\">\n    \u003Cspan style=\"font-size: 1rem; font-weight: 600;\">FigmentGPT\u003C/span>\n    \u003Cspan style=\"font-size: 0.75rem; font-weight: 600; opacity: 0.6;\">ChatGPT\u003C/span>\n  \u003C/div>\n\u003C/a>\n\n\u003Cbr/>\n\n\u003Ca href=\"https://gemini.google.com/gem/1GfVUo7C5goh4tB-fNv_TSHigXftFrTt0?usp=sharing\" target=\"_blank\" rel=\"noopener noreferrer\"\nstyle=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n\u003Cimg src=\"/img/tutorials/custom-nodes/figment-icon.png\" alt=\"Figment Icon\" style=\"border-radius: 100%; overflow: hidden; width: 44px; height: 44px;\" />\n\n   \u003Cdiv style=\"display: flex; flex-direction: column; row-gap: 0;\">\n    \u003Cspan style=\"font-size: 1rem; font-weight: 600;\">Figment Gem\u003C/span>\n    \u003Cspan style=\"font-size: 0.75rem; font-weight: 600; opacity: 0.6;\">Gemini\u003C/span>\n  \u003C/div>\n\u003C/a>\n\n## Example Nodes\n\nYou can inspect the code of any node in Figment by right-clicking and choosing \"View Source\". Furthermore, the code of Figment is also open-source, so you can look at the [src/nodes](https://github.com/figmentapp/figment/tree/master/src/nodes) directory of Figment.","src/content/docs/tutorials/custom-nodes.md","727f62e5827e73ca",{"html":305,"metadata":306},"\u003Ch1 id=\"creating-custom-nodes\">Creating Custom Nodes\u003C/h1>\n\u003Cp>By writing a custom JavaScript node you can add brand‑new image‑processing, audio, or data‑generation to any project.\nThis guide walks you through the whole workflow – from the UI basics to the final code.\u003C/p>\n\u003Ch2 id=\"what-a-custom-node-looks-like\">What a custom node looks like\u003C/h2>\n\u003Cp>Every Figment node follows the same scaffold:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">/**\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@name\u003C/span>\u003Cspan style=\"color:#B392F0\">  Node\u003C/span>\u003Cspan style=\"color:#6A737D\"> Name\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@description\u003C/span>\u003Cspan style=\"color:#6A737D\">  Short description of what the node does.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@category\u003C/span>\u003Cspan style=\"color:#6A737D\">  category (e.g. image, audio, data)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> */\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> inputPort\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">imageIn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"in\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">// Add here other parameters as needed...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> outputPort\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">imageOut\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"out\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">node.\u003C/span>\u003Cspan style=\"color:#B392F0\">onStart\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> async\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> () \u003C/span>\u003Cspan style=\"color:#F97583\">=>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">  /* one‑time init (shaders, textures, etc.) */\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">node.\u003C/span>\u003Cspan style=\"color:#B392F0\">onRender\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> () \u003C/span>\u003Cspan style=\"color:#F97583\">=>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">  /* per‑frame processing */\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">node.\u003C/span>\u003Cspan style=\"color:#B392F0\">onStop\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> () \u003C/span>\u003Cspan style=\"color:#F97583\">=>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">  /* optional cleanup */\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The block above is the template you’ll paste into your new node after you “fork” a Null node (see the UI steps below).\u003C/p>\n\u003Ch2 id=\"creating-a-custom-node\">Creating a custom node\u003C/h2>\n\u003Cp>To create a custom node you need to “fork” it from an existing node. If you want to tweak the code of an existing node, create that one. If you want to start from scratch, use the Null node:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Create a Null node:\u003C/strong> Double‑click anywhere on the canvas and pick Null from the list.\u003C/li>\n\u003Cli>\u003Cstrong>Open the source editor:\u003C/strong> Right‑click the new Null node → View Source.\u003C/li>\n\u003Cli>\u003Cstrong>Fork the node:\u003C/strong> At the bottom of the source panel click Fork.\u003C/li>\n\u003Cli>\u003Cstrong>Name your node:\u003C/strong> Give it a clear, descriptive name (e.g. “Custom Blur”) and confirm the fork.\u003C/li>\n\u003Cli>\u003Cstrong>Replace the code:\u003C/strong> In the editor, delete the existing skeleton and replace with your custom code.\u003C/li>\n\u003Cli>\u003Cstrong>Build the node:\u003C/strong> Click the “Build” button (or type Shift-Enter) to compile the node. Then switch back to the viewer to see the results.\u003C/li>\n\u003C/ul>\n\u003Cp>Custom nodes are stored in the \u003Ccode>.fgmt\u003C/code> file as source code.\u003C/p>\n\u003Cp>:::info\nFigment is actually a custom browser (built in Electron). Use the developer tools (View > Toggle Developer Tools) to debug and inspect your custom node!\n:::\u003C/p>\n\u003Ch2 id=\"example-weather-forecast-node\">Example: weather forecast node\u003C/h2>\n\u003Cp>We’re going to write an image processing node that changes the saturation of the image based on the current weather. To get the current weather, it will fetch it from \u003Ca href=\"https://open-meteo.com\">Open-Meteo\u003C/a>. This API returns, next to temperature and wind speed, also a \u003Ca href=\"https://www.nodc.noaa.gov/archive/arc0021/0002199/1.1/data/0-data/HTML/WMO-CODE/WMO4677.HTM\">WMO code\u003C/a> indicating a global weather condition. We provided a mapping from the weather codes to the saturation values; change these as you see fit.\u003C/p>\n\u003Cp>The node will take in the following inputs:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>in\u003C/code>\u003C/strong>: The input image to manipulate\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>lat\u003C/code>\u003C/strong>: The desired latitude\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>lon\u003C/code>\u003C/strong>: The desired longitude\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>interval\u003C/code>\u003C/strong>: How often the node refreshes the weather (3 hours by default)\u003C/li>\n\u003C/ul>\n\u003Cp>It will output the current image.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">/**\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@name\u003C/span>\u003Cspan style=\"color:#B392F0\"> Weather\u003C/span>\u003Cspan style=\"color:#6A737D\"> Saturation\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@description\u003C/span>\u003Cspan style=\"color:#6A737D\"> Adjusts image saturation based on current weather at (latitude, longitude). Fetches every N hours.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> * \u003C/span>\u003Cspan style=\"color:#F97583\">@category\u003C/span>\u003Cspan style=\"color:#6A737D\"> image\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"> */\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> imageIn\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">imageIn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"in\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> latIn\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">numberIn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"latitude\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">51.26\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  min: \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">90\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  max: \u003C/span>\u003Cspan style=\"color:#79B8FF\">90\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  step: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.01\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">});\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> lonIn\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">numberIn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"longitude\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">4.4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  min: \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">180\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  max: \u003C/span>\u003Cspan style=\"color:#79B8FF\">180\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  step: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.01\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">});\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> intervalIn\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">numberIn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"interval\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  min: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.25\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  max: \u003C/span>\u003Cspan style=\"color:#79B8FF\">24\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  step: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.25\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">});\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> imageOut\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> node.\u003C/span>\u003Cspan style=\"color:#B392F0\">imageOut\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"out\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> DEFAULT_SAT\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0.7\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> WMO_SAT\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">2.0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Clear sky\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.9\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Mainly clear\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.8\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Partly cloudy\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.65\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Overcast\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  45\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.45\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  48\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.45\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Fog / Depositing rime fog\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  51\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  53\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Drizzle (light/mod/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  56\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.45\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  57\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.45\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Freezing drizzle\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  61\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  63\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  65\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Rain (light/mod/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  66\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  67\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Freezing rain\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  71\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  73\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  75\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Snow (light/mod/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  77\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Snow grains\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  80\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  81\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  82\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Rain showers (light/mod/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  85\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  86\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.55\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Snow showers (light/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  95\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.35\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Thunderstorm\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  96\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">  99\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#6A737D\">// Thunderstorm with hail (slight/heavy)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> fragmentShader\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> `\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> precision mediump float;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> uniform sampler2D u_input_texture;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> uniform float u_saturation;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> varying vec2 v_uv;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> void main() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   vec4 c = texture2D(u_input_texture, v_uv);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   // Calculate perceptual luma (Rec. 709)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   float luma = dot(c.rgb, vec3(0.2126, 0.7152, 0.0722));\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   vec3 grey = vec3(luma);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   // Mix original color with saturation value\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   vec3 outRgb = mix(grey, c.rgb, luma);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">   gl_FragColor = vec4(outRgb, c.a);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\"> `\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">let\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> _program, _framebuffer;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">let\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> _lastLat, _lastLon, _lastInterval; \u003C/span>\u003Cspan style=\"color:#6A737D\">// This allows us to track changes\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">let\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> _timer; \u003C/span>\u003Cspan style=\"color:#6A737D\">// Interval timer\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">let\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> _saturation \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0.75\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">node.\u003C/span>\u003Cspan style=\"color:#B392F0\">onStart\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> async\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> () \u003C/span>\u003Cspan style=\"color:#F97583\">=>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _program \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> figment.\u003C/span>\u003Cspan style=\"color:#B392F0\">createShaderProgram\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(fragmentShader);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _framebuffer \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#F97583\"> new\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> figment.\u003C/span>\u003Cspan style=\"color:#B392F0\">Framebuffer\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">  // Fetch immediately, then schedule periodic refreshes.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  await\u003C/span>\u003Cspan style=\"color:#B392F0\"> updateWeather\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">  rescheduleTimer\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">node.\u003C/span>\u003Cspan style=\"color:#B392F0\">onRender\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> () \u003C/span>\u003Cspan style=\"color:#F97583\">=>\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">imageIn.value) \u003C/span>\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _framebuffer.\u003C/span>\u003Cspan style=\"color:#B392F0\">setSize\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(imageIn.value.width, imageIn.value.height);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _framebuffer.\u003C/span>\u003Cspan style=\"color:#B392F0\">bind\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  figment.\u003C/span>\u003Cspan style=\"color:#B392F0\">clear\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  figment.\u003C/span>\u003Cspan style=\"color:#B392F0\">drawQuad\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(_program, {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    u_input_texture: imageIn.value.texture,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    u_saturation: _saturation,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  });\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _framebuffer.\u003C/span>\u003Cspan style=\"color:#B392F0\">unbind\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  imageOut.\u003C/span>\u003Cspan style=\"color:#B392F0\">set\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(_framebuffer);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">async\u003C/span>\u003Cspan style=\"color:#F97583\"> function\u003C/span>\u003Cspan style=\"color:#B392F0\"> updateWeather\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> lat\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> latIn.value;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> lon\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> lonIn.value;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> url\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> `https://api.open-meteo.com/v1/forecast?latitude=${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">lat\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}&#x26;longitude=${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">lon\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}&#x26;current_weather=true&#x26;timezone=auto`\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> res\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> await\u003C/span>\u003Cspan style=\"color:#B392F0\"> fetch\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(url);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">res.ok) \u003C/span>\u003Cspan style=\"color:#F97583\">throw\u003C/span>\u003Cspan style=\"color:#F97583\"> new\u003C/span>\u003Cspan style=\"color:#B392F0\"> Error\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">`Open-Meteo HTTP ${\u003C/span>\u003Cspan style=\"color:#E1E4E8\">res\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.\u003C/span>\u003Cspan style=\"color:#E1E4E8\">status\u003C/span>\u003Cspan style=\"color:#9ECBFF\">}`\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> json\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#F97583\"> await\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> res.\u003C/span>\u003Cspan style=\"color:#B392F0\">json\u003C/span>\u003Cspan style=\"color:#E1E4E8\">();\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> cw\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> json \u003C/span>\u003Cspan style=\"color:#F97583\">&#x26;&#x26;\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> json.current_weather;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003Cspan style=\"color:#F97583\">!\u003C/span>\u003Cspan style=\"color:#E1E4E8\">cw) \u003C/span>\u003Cspan style=\"color:#F97583\">throw\u003C/span>\u003Cspan style=\"color:#F97583\"> new\u003C/span>\u003Cspan style=\"color:#B392F0\"> Error\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"No current_weather in response\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">  // cw contains { temperature, windspeed, winddirection, weathercode, time }\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _saturation \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> WMO_SAT\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[cw.weathercode] \u003C/span>\u003Cspan style=\"color:#F97583\">||\u003C/span>\u003Cspan style=\"color:#79B8FF\"> DEFAULT_SAT\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  console.\u003C/span>\u003Cspan style=\"color:#B392F0\">log\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Weather code:\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, cw.weathercode, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Saturation:\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, _saturation);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">function\u003C/span>\u003Cspan style=\"color:#B392F0\"> rescheduleTimer\u003C/span>\u003Cspan style=\"color:#E1E4E8\">() {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (_timer) \u003C/span>\u003Cspan style=\"color:#B392F0\">clearInterval\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(_timer);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">  const\u003C/span>\u003Cspan style=\"color:#79B8FF\"> intervalMs\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Math.\u003C/span>\u003Cspan style=\"color:#B392F0\">max\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.01\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, intervalIn.value) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 3600\u003C/span>\u003Cspan style=\"color:#F97583\"> *\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">  _timer \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#B392F0\"> setInterval\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(updateWeather, intervalMs);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">latIn.onChange \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> updateWeather;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">lonIn.onChange \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> updateWeather;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">intervalIn.onChange \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rescheduleTimer;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"get-ai-help\">Get AI Help\u003C/h2>\n\u003Cp>We developed a custom ChatGPT and custom Gemini Gem that can help you write or debug custom nodes:\u003C/p>\n\u003Cp>\u003Ca href=\"https://chatgpt.com/g/g-68d3996b835481918330cb7509368404-figmentgpt\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n\u003Cimg src=\"/img/tutorials/custom-nodes/figment-icon.png\" alt=\"Figment Icon\" style=\"border-radius: 100%; overflow: hidden; width: 44px; height: 44px;\">\u003C/a>\u003C/p>\u003Ca href=\"https://chatgpt.com/g/g-68d3996b835481918330cb7509368404-figmentgpt\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n   \u003Cdiv style=\"display: flex; flex-direction: column; row-gap: 0;\">\n    \u003Cspan style=\"font-size: 1rem; font-weight: 600;\">FigmentGPT\u003C/span>\n    \u003Cspan style=\"font-size: 0.75rem; font-weight: 600; opacity: 0.6;\">ChatGPT\u003C/span>\n  \u003C/div>\n\u003C/a>\n\u003Cbr>\n\u003Cp>\u003Ca href=\"https://gemini.google.com/gem/1GfVUo7C5goh4tB-fNv_TSHigXftFrTt0?usp=sharing\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n\u003Cimg src=\"/img/tutorials/custom-nodes/figment-icon.png\" alt=\"Figment Icon\" style=\"border-radius: 100%; overflow: hidden; width: 44px; height: 44px;\">\u003C/a>\u003C/p>\u003Ca href=\"https://gemini.google.com/gem/1GfVUo7C5goh4tB-fNv_TSHigXftFrTt0?usp=sharing\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"background-color: #444444; display: flex; flex-direction: row; align-items: center; gap: 0.5rem; padding: 1rem; border-radius: 0.5rem; width: 300px; color: #eee; text-decoration: none;\">\n   \u003Cdiv style=\"display: flex; flex-direction: column; row-gap: 0;\">\n    \u003Cspan style=\"font-size: 1rem; font-weight: 600;\">Figment Gem\u003C/span>\n    \u003Cspan style=\"font-size: 0.75rem; font-weight: 600; opacity: 0.6;\">Gemini\u003C/span>\n  \u003C/div>\n\u003C/a>\n\u003Ch2 id=\"example-nodes\">Example Nodes\u003C/h2>\n\u003Cp>You can inspect the code of any node in Figment by right-clicking and choosing “View Source”. Furthermore, the code of Figment is also open-source, so you can look at the \u003Ca href=\"https://github.com/figmentapp/figment/tree/master/src/nodes\">src/nodes\u003C/a> directory of Figment.\u003C/p>",{"headings":307,"localImagePaths":325,"remoteImagePaths":326,"frontmatter":327,"imagePaths":328},[308,310,313,316,319,322],{"depth":29,"slug":309,"text":297},"creating-custom-nodes",{"depth":32,"slug":311,"text":312},"what-a-custom-node-looks-like","What a custom node looks like",{"depth":32,"slug":314,"text":315},"creating-a-custom-node","Creating a custom node",{"depth":32,"slug":317,"text":318},"example-weather-forecast-node","Example: weather forecast node",{"depth":32,"slug":320,"text":321},"get-ai-help","Get AI Help",{"depth":32,"slug":323,"text":324},"example-nodes","Example Nodes",[],[],{"title":297},[],"tutorials/custom-nodes.md","nodes/canny",{"id":330,"data":332,"body":337,"filePath":338,"digest":339,"rendered":340,"legacyId":352},{"title":333,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":334,"sidebar":335},"Canny",[],{"hidden":17,"attrs":336},{},"# Canny\n\nThis node computes canny edges on an input image.\n\n## Parameters\n\n- **Thickness**\n- **Factor**\n\n## Example\n\n\u003Cimg src=\"/img/nodes/canny.jpg\" alt=\"Figment canny node example\"/>","src/content/docs/nodes/canny.md","a85aa40a547483d1",{"html":341,"metadata":342},"\u003Ch1 id=\"canny\">Canny\u003C/h1>\n\u003Cp>This node computes canny edges on an input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Thickness\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Factor\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/canny.jpg\" alt=\"Figment canny node example\">",{"headings":343,"localImagePaths":348,"remoteImagePaths":349,"frontmatter":350,"imagePaths":351},[344,346,347],{"depth":29,"slug":345,"text":333},"canny",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":333},[],"nodes/canny.md","nodes/brannan",{"id":353,"data":355,"body":360,"filePath":361,"digest":362,"rendered":363,"legacyId":375},{"title":356,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":357,"sidebar":358},"Brannan",[],{"hidden":17,"attrs":359},{},"# Brannan\n\nThe node renders a brannan instagram filter on the input image.\n\n## Parameters\n\n- **Grayscale ratio**: The amount of grayscale that gets mixed with the original color.\n- **Saturation ratio**: The amount of saturation of the original colors.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/brannan.jpg\" alt=\"Figment brannan node example\"/>","src/content/docs/nodes/brannan.md","da92119615dee440",{"html":364,"metadata":365},"\u003Ch1 id=\"brannan\">Brannan\u003C/h1>\n\u003Cp>The node renders a brannan instagram filter on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Grayscale ratio\u003C/strong>: The amount of grayscale that gets mixed with the original color.\u003C/li>\n\u003Cli>\u003Cstrong>Saturation ratio\u003C/strong>: The amount of saturation of the original colors.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/brannan.jpg\" alt=\"Figment brannan node example\">",{"headings":366,"localImagePaths":371,"remoteImagePaths":372,"frontmatter":373,"imagePaths":374},[367,369,370],{"depth":29,"slug":368,"text":356},"brannan",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":356},[],"nodes/brannan.md","nodes/border",{"id":376,"data":378,"body":383,"filePath":384,"digest":385,"rendered":386,"legacyId":398},{"title":379,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":380,"sidebar":381},"Border",[],{"hidden":17,"attrs":382},{},"# Border\n\nThis node creates a border on an input image. The border can be colorised.\n\n## Parameters\n\n- **Border Size** Sets the size of the border.\n- **Border Color** Sets the color of the border. The build in colorpicker allows for colorvalues defined in `rgba`, `hsla` or `hex` format.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/border.jpg\" alt=\"Figment border node example\"/>","src/content/docs/nodes/border.md","ce96a3988377613f",{"html":387,"metadata":388},"\u003Ch1 id=\"border\">Border\u003C/h1>\n\u003Cp>This node creates a border on an input image. The border can be colorised.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Border Size\u003C/strong> Sets the size of the border.\u003C/li>\n\u003Cli>\u003Cstrong>Border Color\u003C/strong> Sets the color of the border. The build in colorpicker allows for colorvalues defined in \u003Ccode>rgba\u003C/code>, \u003Ccode>hsla\u003C/code> or \u003Ccode>hex\u003C/code> format.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/border.jpg\" alt=\"Figment border node example\">",{"headings":389,"localImagePaths":394,"remoteImagePaths":395,"frontmatter":396,"imagePaths":397},[390,392,393],{"depth":29,"slug":391,"text":379},"border",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":379},[],"nodes/border.md","nodes/conditional",{"id":399,"data":401,"body":406,"filePath":407,"digest":408,"rendered":409,"legacyId":432},{"title":402,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":403,"sidebar":404},"Conditional",[],{"hidden":17,"attrs":405},{},"# Conditional\n\nThis node renders one of two images based on a condition, with smooth fading between them when the condition changes.\n\n## Parameters\n\n- **Condition** A boolean value that determines which image to display. When `true`, shows the \"true image\"; when `false`, shows the \"false image\". This parameter supports [expressions](/docs/expressions), making it useful for switching scenes based on MIDI program changes or other dynamic inputs.\n- **True Image** The image to display when the condition is true.\n- **False Image** The image to display when the condition is false.\n- **Fade Time** The duration of the transition between images in seconds. Range: 0.0 to 10.0. Default is 0.5 seconds.\n- **Fade Bias** Controls the asymmetry of the fade transition. Range: 0.0 to 1.0. Default is 0.5 (symmetric fade). Values closer to 0 make the fade-out faster, while values closer to 1 make the fade-in faster.\n\n## Outputs\n\n- **Out** The blended output image, transitioning between the true and false images based on the current condition.\n\n## Example Use Cases\n\n### Scene Switching with MIDI\n\nUse MIDI Program Change messages to switch between different visual scenes:\n\n```js\nmidipc(1, 0) == 0;\n```\n\nThis expression in the Condition parameter will show the \"true image\" when MIDI program 0 is active on channel 1, and the \"false image\" for any other program.\n\n### Time-based Switching\n\nToggle between images based on time:\n\n```js\n$TIME % 10 \u003C 5;\n```\n\nThis alternates between the two images every 5 seconds.","src/content/docs/nodes/conditional.md","e26a55c0c609f980",{"html":410,"metadata":411},"\u003Ch1 id=\"conditional\">Conditional\u003C/h1>\n\u003Cp>This node renders one of two images based on a condition, with smooth fading between them when the condition changes.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Condition\u003C/strong> A boolean value that determines which image to display. When \u003Ccode>true\u003C/code>, shows the “true image”; when \u003Ccode>false\u003C/code>, shows the “false image”. This parameter supports \u003Ca href=\"/docs/expressions\">expressions\u003C/a>, making it useful for switching scenes based on MIDI program changes or other dynamic inputs.\u003C/li>\n\u003Cli>\u003Cstrong>True Image\u003C/strong> The image to display when the condition is true.\u003C/li>\n\u003Cli>\u003Cstrong>False Image\u003C/strong> The image to display when the condition is false.\u003C/li>\n\u003Cli>\u003Cstrong>Fade Time\u003C/strong> The duration of the transition between images in seconds. Range: 0.0 to 10.0. Default is 0.5 seconds.\u003C/li>\n\u003Cli>\u003Cstrong>Fade Bias\u003C/strong> Controls the asymmetry of the fade transition. Range: 0.0 to 1.0. Default is 0.5 (symmetric fade). Values closer to 0 make the fade-out faster, while values closer to 1 make the fade-in faster.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outputs\">Outputs\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Out\u003C/strong> The blended output image, transitioning between the true and false images based on the current condition.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example-use-cases\">Example Use Cases\u003C/h2>\n\u003Ch3 id=\"scene-switching-with-midi\">Scene Switching with MIDI\u003C/h3>\n\u003Cp>Use MIDI Program Change messages to switch between different visual scenes:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">midipc\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This expression in the Condition parameter will show the “true image” when MIDI program 0 is active on channel 1, and the “false image” for any other program.\u003C/p>\n\u003Ch3 id=\"time-based-switching\">Time-based Switching\u003C/h3>\n\u003Cp>Toggle between images based on time:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"js\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">$TIME \u003C/span>\u003Cspan style=\"color:#F97583\">%\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 10\u003C/span>\u003Cspan style=\"color:#F97583\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">;\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This alternates between the two images every 5 seconds.\u003C/p>",{"headings":412,"localImagePaths":428,"remoteImagePaths":429,"frontmatter":430,"imagePaths":431},[413,415,416,419,422,425],{"depth":29,"slug":414,"text":402},"conditional",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":417,"text":418},"outputs","Outputs",{"depth":32,"slug":420,"text":421},"example-use-cases","Example Use Cases",{"depth":178,"slug":423,"text":424},"scene-switching-with-midi","Scene Switching with MIDI",{"depth":178,"slug":426,"text":427},"time-based-switching","Time-based Switching",[],[],{"title":402},[],"nodes/conditional.md","nodes/composite",{"id":433,"data":435,"body":440,"filePath":441,"digest":442,"rendered":443,"legacyId":455},{"title":436,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":437,"sidebar":438},"Composite",[],{"hidden":17,"attrs":439},{},"# Composite\n\nThe node composites two images together by using a blending mode. It needs two images (or a [Constant](./constant)) as input.\n\n## Parameters\n\n- **Factor**: The factor indicates how the two input images are distributed. `0.5` takes both images evenly. `0.0` refers to the first input images and `1.0` refers to the second one.\n- **Operation**: The type of blending mode. The different options are `normal`, `darken`, `multiply`, `color burn` (to darken the image) | `lighten`, `screen`,`color dodge` (to lighten the image) and `difference`\n\n## Example\n\n\u003Cimg src=\"/img/nodes/composite.jpg\" alt=\"Figment composite node example\"/>","src/content/docs/nodes/composite.md","609096710e9e4856",{"html":444,"metadata":445},"\u003Ch1 id=\"composite\">Composite\u003C/h1>\n\u003Cp>The node composites two images together by using a blending mode. It needs two images (or a \u003Ca href=\"./constant\">Constant\u003C/a>) as input.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Factor\u003C/strong>: The factor indicates how the two input images are distributed. \u003Ccode>0.5\u003C/code> takes both images evenly. \u003Ccode>0.0\u003C/code> refers to the first input images and \u003Ccode>1.0\u003C/code> refers to the second one.\u003C/li>\n\u003Cli>\u003Cstrong>Operation\u003C/strong>: The type of blending mode. The different options are \u003Ccode>normal\u003C/code>, \u003Ccode>darken\u003C/code>, \u003Ccode>multiply\u003C/code>, \u003Ccode>color burn\u003C/code> (to darken the image) | \u003Ccode>lighten\u003C/code>, \u003Ccode>screen\u003C/code>,\u003Ccode>color dodge\u003C/code> (to lighten the image) and \u003Ccode>difference\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/composite.jpg\" alt=\"Figment composite node example\">",{"headings":446,"localImagePaths":451,"remoteImagePaths":452,"frontmatter":453,"imagePaths":454},[447,449,450],{"depth":29,"slug":448,"text":436},"composite",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":436},[],"nodes/composite.md","nodes/constant",{"id":456,"data":458,"body":463,"filePath":464,"digest":465,"rendered":466,"legacyId":477},{"title":459,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":460,"sidebar":461},"Constant",[],{"hidden":17,"attrs":462},{},"# Constant\n\nThis node renders a constant color.\n\n## Parameters\n\n- **Color** Sets the color of the colorplane. The build in colorpicker allows for colorvalues defined as `rgba`, `hsla` or `hex` format.\n- **Width** Sets the width of the colorplane.\n- **Height** Sets the height of the colorplane.","src/content/docs/nodes/constant.md","5c5fbb654b164100",{"html":467,"metadata":468},"\u003Ch1 id=\"constant\">Constant\u003C/h1>\n\u003Cp>This node renders a constant color.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Color\u003C/strong> Sets the color of the colorplane. The build in colorpicker allows for colorvalues defined as \u003Ccode>rgba\u003C/code>, \u003Ccode>hsla\u003C/code> or \u003Ccode>hex\u003C/code> format.\u003C/li>\n\u003Cli>\u003Cstrong>Width\u003C/strong> Sets the width of the colorplane.\u003C/li>\n\u003Cli>\u003Cstrong>Height\u003C/strong> Sets the height of the colorplane.\u003C/li>\n\u003C/ul>",{"headings":469,"localImagePaths":473,"remoteImagePaths":474,"frontmatter":475,"imagePaths":476},[470,472],{"depth":29,"slug":471,"text":459},"constant",{"depth":32,"slug":131,"text":132},[],[],{"title":459},[],"nodes/constant.md","nodes/denoise",{"id":478,"data":480,"body":485,"filePath":486,"digest":487,"rendered":488,"legacyId":500},{"title":481,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":482,"sidebar":483},"Denoise",[],{"hidden":17,"attrs":484},{},"# Denoise\n\nThe node removes noise on the input image.\n\n## Parameters\n\n- **Denoise factor**: The amount of denoising.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/denoise.jpg\" alt=\"Figment noise reduction node example\"/>","src/content/docs/nodes/denoise.md","6927bb84fd8c67b1",{"html":489,"metadata":490},"\u003Ch1 id=\"denoise\">Denoise\u003C/h1>\n\u003Cp>The node removes noise on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Denoise factor\u003C/strong>: The amount of denoising.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/denoise.jpg\" alt=\"Figment noise reduction node example\">",{"headings":491,"localImagePaths":496,"remoteImagePaths":497,"frontmatter":498,"imagePaths":499},[492,494,495],{"depth":29,"slug":493,"text":481},"denoise",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":481},[],"nodes/denoise.md","nodes/color-key",{"id":501,"data":503,"body":508,"filePath":509,"digest":510,"rendered":511,"legacyId":523},{"title":504,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":505,"sidebar":506},"Color Key",[],{"hidden":17,"attrs":507},{},"# Color Key\n\nThe node allow color keying on the input image. It returns the key color as a alpha transparent layer which can be recolered by using a constant color node and a composite node.\n\n## Parameters\n\n- **Key color**: The color that needs to be keyed out.\n- **Threshold**: The threshold value for the color.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/colorKey.jpg\" alt=\"Figment color key node example\"/>","src/content/docs/nodes/color-key.md","aa6d16159ddd8145",{"html":512,"metadata":513},"\u003Ch1 id=\"color-key\">Color Key\u003C/h1>\n\u003Cp>The node allow color keying on the input image. It returns the key color as a alpha transparent layer which can be recolered by using a constant color node and a composite node.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Key color\u003C/strong>: The color that needs to be keyed out.\u003C/li>\n\u003Cli>\u003Cstrong>Threshold\u003C/strong>: The threshold value for the color.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/colorKey.jpg\" alt=\"Figment color key node example\">",{"headings":514,"localImagePaths":519,"remoteImagePaths":520,"frontmatter":521,"imagePaths":522},[515,517,518],{"depth":29,"slug":516,"text":504},"color-key",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":504},[],"nodes/color-key.md","nodes/cartoon",{"id":524,"data":526,"body":531,"filePath":532,"digest":533,"rendered":534,"legacyId":546},{"title":527,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":528,"sidebar":529},"Cartoon",[],{"hidden":17,"attrs":530},{},"# Cartoon\n\nThe node renders a cartoon filter on the input image.\n\n## Parameters\n\n- **Amount**: The amount of fx applied to the image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/cartoon.jpg\" alt=\"Figment cartoon node example\"/>","src/content/docs/nodes/cartoon.md","9469b033313101d6",{"html":535,"metadata":536},"\u003Ch1 id=\"cartoon\">Cartoon\u003C/h1>\n\u003Cp>The node renders a cartoon filter on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Amount\u003C/strong>: The amount of fx applied to the image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/cartoon.jpg\" alt=\"Figment cartoon node example\">",{"headings":537,"localImagePaths":542,"remoteImagePaths":543,"frontmatter":544,"imagePaths":545},[538,540,541],{"depth":29,"slug":539,"text":527},"cartoon",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":527},[],"nodes/cartoon.md","nodes/crop",{"id":547,"data":549,"body":554,"filePath":555,"digest":556,"rendered":557,"legacyId":568},{"title":550,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":551,"sidebar":552},"Crop",[],{"hidden":17,"attrs":553},{},"# Crop\n\nCrops the input image to the defined size.\n\n## Parameters\n\n- **Width** The target crop width.\n- **Height** The target crop height.\n- **Anchor** Controls the position of the crop area within the input image. Options include:\n  - `top-left` - Crop from the top-left corner\n  - `top-center` - Crop from the top center\n  - `top-right` - Crop from the top-right corner\n  - `center-left` - Crop from the center left\n  - `center` - Crop from the center (default)\n  - `center-right` - Crop from the center right\n  - `bottom-left` - Crop from the bottom-left corner\n  - `bottom-center` - Crop from the bottom center\n  - `bottom-right` - Crop from the bottom-right corner\n- **Output Size** Determines the output behavior:\n  - `cropped` - Output dimensions match the crop size (width × height). The cropped area is zoomed to fill the output.\n  - `original` - Output dimensions match the input image. Areas outside the crop are made transparent (masked out).","src/content/docs/nodes/crop.md","bb585a351993da3b",{"html":558,"metadata":559},"\u003Ch1 id=\"crop\">Crop\u003C/h1>\n\u003Cp>Crops the input image to the defined size.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Width\u003C/strong> The target crop width.\u003C/li>\n\u003Cli>\u003Cstrong>Height\u003C/strong> The target crop height.\u003C/li>\n\u003Cli>\u003Cstrong>Anchor\u003C/strong> Controls the position of the crop area within the input image. Options include:\n\u003Cul>\n\u003Cli>\u003Ccode>top-left\u003C/code> - Crop from the top-left corner\u003C/li>\n\u003Cli>\u003Ccode>top-center\u003C/code> - Crop from the top center\u003C/li>\n\u003Cli>\u003Ccode>top-right\u003C/code> - Crop from the top-right corner\u003C/li>\n\u003Cli>\u003Ccode>center-left\u003C/code> - Crop from the center left\u003C/li>\n\u003Cli>\u003Ccode>center\u003C/code> - Crop from the center (default)\u003C/li>\n\u003Cli>\u003Ccode>center-right\u003C/code> - Crop from the center right\u003C/li>\n\u003Cli>\u003Ccode>bottom-left\u003C/code> - Crop from the bottom-left corner\u003C/li>\n\u003Cli>\u003Ccode>bottom-center\u003C/code> - Crop from the bottom center\u003C/li>\n\u003Cli>\u003Ccode>bottom-right\u003C/code> - Crop from the bottom-right corner\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Cstrong>Output Size\u003C/strong> Determines the output behavior:\n\u003Cul>\n\u003Cli>\u003Ccode>cropped\u003C/code> - Output dimensions match the crop size (width × height). The cropped area is zoomed to fill the output.\u003C/li>\n\u003Cli>\u003Ccode>original\u003C/code> - Output dimensions match the input image. Areas outside the crop are made transparent (masked out).\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>",{"headings":560,"localImagePaths":564,"remoteImagePaths":565,"frontmatter":566,"imagePaths":567},[561,563],{"depth":29,"slug":562,"text":550},"crop",{"depth":32,"slug":131,"text":132},[],[],{"title":550},[],"nodes/crop.md","nodes/detect-hands",{"id":569,"data":571,"body":576,"filePath":577,"digest":578,"rendered":579,"legacyId":590},{"title":572,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":573,"sidebar":574},"Detect Hands",[],{"hidden":17,"attrs":575},{},"# Detect Hands\n\nDetect one or more hands in the input image and draw it as a skeleton-like shape.\n\n## Parameters\n\n- **Background**: The color of the background.\n- **Draw Points**: Whether to draw the landmarks of the hands as points.\n- **Points Color**: The color of the points.\n- **Points Radius**: The size of the points.\n- **Draw Lines**: Whether to connect the landmarks of the hands with lines.\n- **Lines Color**: The color of the lines.\n- **Lines Width**: The thickness of the lines.","src/content/docs/nodes/detect-hands.md","5e060889fb68494a",{"html":580,"metadata":581},"\u003Ch1 id=\"detect-hands\">Detect Hands\u003C/h1>\n\u003Cp>Detect one or more hands in the input image and draw it as a skeleton-like shape.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Background\u003C/strong>: The color of the background.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Points\u003C/strong>: Whether to draw the landmarks of the hands as points.\u003C/li>\n\u003Cli>\u003Cstrong>Points Color\u003C/strong>: The color of the points.\u003C/li>\n\u003Cli>\u003Cstrong>Points Radius\u003C/strong>: The size of the points.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Lines\u003C/strong>: Whether to connect the landmarks of the hands with lines.\u003C/li>\n\u003Cli>\u003Cstrong>Lines Color\u003C/strong>: The color of the lines.\u003C/li>\n\u003Cli>\u003Cstrong>Lines Width\u003C/strong>: The thickness of the lines.\u003C/li>\n\u003C/ul>",{"headings":582,"localImagePaths":586,"remoteImagePaths":587,"frontmatter":588,"imagePaths":589},[583,585],{"depth":29,"slug":584,"text":572},"detect-hands",{"depth":32,"slug":131,"text":132},[],[],{"title":572},[],"nodes/detect-hands.md","nodes/distortion",{"id":591,"data":593,"body":598,"filePath":599,"digest":600,"rendered":601,"legacyId":613},{"title":594,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":595,"sidebar":596},"Distortion",[],{"hidden":17,"attrs":597},{},"# Distortion\n\nThe node computes a simple (wave) distortion on the input image.\n\n## Parameters\n\n- **Distortion**: The amount of distortion.\n- **Wave**: The wave of the distorted image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/distortion.jpg\" alt=\"Figment simple distortion node example\"/>","src/content/docs/nodes/distortion.md","b4e0c87d110df432",{"html":602,"metadata":603},"\u003Ch1 id=\"distortion\">Distortion\u003C/h1>\n\u003Cp>The node computes a simple (wave) distortion on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Distortion\u003C/strong>: The amount of distortion.\u003C/li>\n\u003Cli>\u003Cstrong>Wave\u003C/strong>: The wave of the distorted image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/distortion.jpg\" alt=\"Figment simple distortion node example\">",{"headings":604,"localImagePaths":609,"remoteImagePaths":610,"frontmatter":611,"imagePaths":612},[605,607,608],{"depth":29,"slug":606,"text":594},"distortion",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":594},[],"nodes/distortion.md","nodes/detect-pose",{"id":614,"data":616,"body":621,"filePath":622,"digest":623,"rendered":624,"legacyId":635},{"title":617,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":618,"sidebar":619},"Detect Pose",[],{"hidden":17,"attrs":620},{},"# Detect Pose\n\nDetect a single human pose in the input image and draw it as a skeleton-like shape.\n\n## Parameters\n\n- **Background**: The color of the background.\n- **Draw Points**: Whether to draw the landmarks of the pose as points.\n- **Points Color**: The color of the points.\n- **Points Radius**: The size of the points.\n- **Draw Lines**: Whether to connect the landmarks of the pose with lines.\n- **Lines Color**: The color of the lines.\n- **Lines Width**: The thickness of the lines.","src/content/docs/nodes/detect-pose.md","c1d72352238bf3f0",{"html":625,"metadata":626},"\u003Ch1 id=\"detect-pose\">Detect Pose\u003C/h1>\n\u003Cp>Detect a single human pose in the input image and draw it as a skeleton-like shape.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Background\u003C/strong>: The color of the background.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Points\u003C/strong>: Whether to draw the landmarks of the pose as points.\u003C/li>\n\u003Cli>\u003Cstrong>Points Color\u003C/strong>: The color of the points.\u003C/li>\n\u003Cli>\u003Cstrong>Points Radius\u003C/strong>: The size of the points.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Lines\u003C/strong>: Whether to connect the landmarks of the pose with lines.\u003C/li>\n\u003Cli>\u003Cstrong>Lines Color\u003C/strong>: The color of the lines.\u003C/li>\n\u003Cli>\u003Cstrong>Lines Width\u003C/strong>: The thickness of the lines.\u003C/li>\n\u003C/ul>",{"headings":627,"localImagePaths":631,"remoteImagePaths":632,"frontmatter":633,"imagePaths":634},[628,630],{"depth":29,"slug":629,"text":617},"detect-pose",{"depth":32,"slug":131,"text":132},[],[],{"title":617},[],"nodes/detect-pose.md","nodes/emboss",{"id":636,"data":638,"body":643,"filePath":644,"digest":645,"rendered":646,"legacyId":658},{"title":639,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":640,"sidebar":641},"Emboss",[],{"hidden":17,"attrs":642},{},"# Emboss\n\nThis node calculates the emboss convolution on an input image.\n\n## Parameters\n\n- **Width** Sets the width of the convolution.\n- **Height** Sets the height of the convolution.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/emboss.jpg\" alt=\"Figment emboss node example\"/>","src/content/docs/nodes/emboss.md","04b4756114f553e2",{"html":647,"metadata":648},"\u003Ch1 id=\"emboss\">Emboss\u003C/h1>\n\u003Cp>This node calculates the emboss convolution on an input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Width\u003C/strong> Sets the width of the convolution.\u003C/li>\n\u003Cli>\u003Cstrong>Height\u003C/strong> Sets the height of the convolution.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/emboss.jpg\" alt=\"Figment emboss node example\">",{"headings":649,"localImagePaths":654,"remoteImagePaths":655,"frontmatter":656,"imagePaths":657},[650,652,653],{"depth":29,"slug":651,"text":639},"emboss",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":639},[],"nodes/emboss.md","nodes/fetch-image",{"id":659,"data":661,"body":666,"filePath":667,"digest":668,"rendered":669,"legacyId":680},{"title":662,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":663,"sidebar":664},"Fetch Image",[],{"hidden":17,"attrs":665},{},"# Fetch Image\n\nDownload an image from the internet. If refresh is set, download the image repeatedly.\n\n## Parameters\n\n- **Url** The url of the image file.\n- **Refresh** If checked, will try to reload the image repeatedly. Useful for webcam feeds, for example.\n- **Refresh Time** How fast the image is reloaded, in seconds.","src/content/docs/nodes/fetch-image.md","28f36b39096eb495",{"html":670,"metadata":671},"\u003Ch1 id=\"fetch-image\">Fetch Image\u003C/h1>\n\u003Cp>Download an image from the internet. If refresh is set, download the image repeatedly.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Url\u003C/strong> The url of the image file.\u003C/li>\n\u003Cli>\u003Cstrong>Refresh\u003C/strong> If checked, will try to reload the image repeatedly. Useful for webcam feeds, for example.\u003C/li>\n\u003Cli>\u003Cstrong>Refresh Time\u003C/strong> How fast the image is reloaded, in seconds.\u003C/li>\n\u003C/ul>",{"headings":672,"localImagePaths":676,"remoteImagePaths":677,"frontmatter":678,"imagePaths":679},[673,675],{"depth":29,"slug":674,"text":662},"fetch-image",{"depth":32,"slug":131,"text":132},[],[],{"title":662},[],"nodes/fetch-image.md","nodes/glitch",{"id":681,"data":683,"body":688,"filePath":689,"digest":690,"rendered":691,"legacyId":703},{"title":684,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":685,"sidebar":686},"Glitch",[],{"hidden":17,"attrs":687},{},"# Glitch\n\nThe node renders a glitch filter on the input image.\n\n## Parameters\n\n- **Seed**: The random seed that triggers the glitch.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/glitch.jpg\" alt=\"Figment glitch node example\"/>","src/content/docs/nodes/glitch.md","e5160e42fc27d0df",{"html":692,"metadata":693},"\u003Ch1 id=\"glitch\">Glitch\u003C/h1>\n\u003Cp>The node renders a glitch filter on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Seed\u003C/strong>: The random seed that triggers the glitch.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/glitch.jpg\" alt=\"Figment glitch node example\">",{"headings":694,"localImagePaths":699,"remoteImagePaths":700,"frontmatter":701,"imagePaths":702},[695,697,698],{"depth":29,"slug":696,"text":684},"glitch",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":684},[],"nodes/glitch.md","nodes/glow-edges",{"id":704,"data":706,"body":711,"filePath":712,"digest":713,"rendered":714,"legacyId":726},{"title":707,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":708,"sidebar":709},"Glow Edges",[],{"hidden":17,"attrs":710},{},"# Glow Edges\n\nThe node renders glowing edges on the input image.\n\n## Parameters\n\nEdge color\nStroke width\n\n- **Edge color**: The color for the glowing edge.\n- **Stroke width**: The stroke width for the glowing edge.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/glowEdges.jpg\" alt=\"Figment glowing edges node example\"/>","src/content/docs/nodes/glow-edges.md","c0d51a6581f0ea32",{"html":715,"metadata":716},"\u003Ch1 id=\"glow-edges\">Glow Edges\u003C/h1>\n\u003Cp>The node renders glowing edges on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cp>Edge color\nStroke width\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Edge color\u003C/strong>: The color for the glowing edge.\u003C/li>\n\u003Cli>\u003Cstrong>Stroke width\u003C/strong>: The stroke width for the glowing edge.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/glowEdges.jpg\" alt=\"Figment glowing edges node example\">",{"headings":717,"localImagePaths":722,"remoteImagePaths":723,"frontmatter":724,"imagePaths":725},[718,720,721],{"depth":29,"slug":719,"text":707},"glow-edges",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":707},[],"nodes/glow-edges.md","nodes/gray-cluster",{"id":727,"data":729,"body":734,"filePath":735,"digest":736,"rendered":737,"legacyId":749},{"title":730,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":731,"sidebar":732},"Gray Cluster",[],{"hidden":17,"attrs":733},{},"# Gray Cluster\n\nThe node returns the input image in a cluster of gray colors / black and white. The total amount of colors is 6\n\n## Parameters\n\n## Example\n\n\u003Cimg src=\"/img/nodes/grayCluster.jpg\" alt=\"Figment gray cluster node example\"/>","src/content/docs/nodes/gray-cluster.md","03f92c617e9e2b1f",{"html":738,"metadata":739},"\u003Ch1 id=\"gray-cluster\">Gray Cluster\u003C/h1>\n\u003Cp>The node returns the input image in a cluster of gray colors / black and white. The total amount of colors is 6\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/grayCluster.jpg\" alt=\"Figment gray cluster node example\">",{"headings":740,"localImagePaths":745,"remoteImagePaths":746,"frontmatter":747,"imagePaths":748},[741,743,744],{"depth":29,"slug":742,"text":730},"gray-cluster",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":730},[],"nodes/gray-cluster.md","nodes/detect-faces",{"id":750,"data":752,"body":757,"filePath":758,"digest":759,"rendered":760,"legacyId":771},{"title":753,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":754,"sidebar":755},"Detect Faces",[],{"hidden":17,"attrs":756},{},"# Detect Faces\n\nDetect a face in the input image using FaceMesh and draw a shape around it.\n\n## Parameters\n\n- **Background**: The color of the background.\n- **Draw Contour**: Whether to draw the contour of the face.\n- **Contour Color**: The color of the contour.\n- **Contour Line Width**: The thickness of the lines of the contour.\n- **Draw Tesselation**: Whether to draw a detailed tesselated face. Note that the tesselation and the contour overlap, so you only need one.\n- **Tesselation Color**: The color of the tesselation.\n- **Tesselation Line Width**: The thickness of the lines of the tesselation.\n- **Draw Bounding Box**: Whether to draw a bounding box around the face.\n- **Bounding Box Color**: The color of the bounding box.\n- **Bounding Box Line Width**: The thickness of the lines of the bounding box.","src/content/docs/nodes/detect-faces.md","84d9d4aa81a2e293",{"html":761,"metadata":762},"\u003Ch1 id=\"detect-faces\">Detect Faces\u003C/h1>\n\u003Cp>Detect a face in the input image using FaceMesh and draw a shape around it.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Background\u003C/strong>: The color of the background.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Contour\u003C/strong>: Whether to draw the contour of the face.\u003C/li>\n\u003Cli>\u003Cstrong>Contour Color\u003C/strong>: The color of the contour.\u003C/li>\n\u003Cli>\u003Cstrong>Contour Line Width\u003C/strong>: The thickness of the lines of the contour.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Tesselation\u003C/strong>: Whether to draw a detailed tesselated face. Note that the tesselation and the contour overlap, so you only need one.\u003C/li>\n\u003Cli>\u003Cstrong>Tesselation Color\u003C/strong>: The color of the tesselation.\u003C/li>\n\u003Cli>\u003Cstrong>Tesselation Line Width\u003C/strong>: The thickness of the lines of the tesselation.\u003C/li>\n\u003Cli>\u003Cstrong>Draw Bounding Box\u003C/strong>: Whether to draw a bounding box around the face.\u003C/li>\n\u003Cli>\u003Cstrong>Bounding Box Color\u003C/strong>: The color of the bounding box.\u003C/li>\n\u003Cli>\u003Cstrong>Bounding Box Line Width\u003C/strong>: The thickness of the lines of the bounding box.\u003C/li>\n\u003C/ul>",{"headings":763,"localImagePaths":767,"remoteImagePaths":768,"frontmatter":769,"imagePaths":770},[764,766],{"depth":29,"slug":765,"text":753},"detect-faces",{"depth":32,"slug":131,"text":132},[],[],{"title":753},[],"nodes/detect-faces.md","nodes/detect-objects",{"id":772,"data":774,"body":779,"filePath":780,"digest":781,"rendered":782,"legacyId":793},{"title":775,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":776,"sidebar":777},"Detect Objects",[],{"hidden":17,"attrs":778},{},"# Detect Objects\n\nThis node can show the detected objects in an image. It can also mask around the detected objects. Currently, only rectangular masks are supported for performance.\n\nThis uses the [coco-ssd](https://www.npmjs.com/package/@tensorflow-models/coco-ssd) model.\n\nThis node outputs both the image and the detected objects, as a list.\n\n## Parameters\n\n- **Drawing Mode** Whether to draw boxes around the objects or to mask the background so only the objects remain.\n- **Filter** The type of objects to filter. There are about 80 [types of objects](https://github.com/tensorflow/tfjs-models/blob/master/coco-ssd/src/classes.ts).","src/content/docs/nodes/detect-objects.md","8b1d603a638ae940",{"html":783,"metadata":784},"\u003Ch1 id=\"detect-objects\">Detect Objects\u003C/h1>\n\u003Cp>This node can show the detected objects in an image. It can also mask around the detected objects. Currently, only rectangular masks are supported for performance.\u003C/p>\n\u003Cp>This uses the \u003Ca href=\"https://www.npmjs.com/package/@tensorflow-models/coco-ssd\">coco-ssd\u003C/a> model.\u003C/p>\n\u003Cp>This node outputs both the image and the detected objects, as a list.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Drawing Mode\u003C/strong> Whether to draw boxes around the objects or to mask the background so only the objects remain.\u003C/li>\n\u003Cli>\u003Cstrong>Filter\u003C/strong> The type of objects to filter. There are about 80 \u003Ca href=\"https://github.com/tensorflow/tfjs-models/blob/master/coco-ssd/src/classes.ts\">types of objects\u003C/a>.\u003C/li>\n\u003C/ul>",{"headings":785,"localImagePaths":789,"remoteImagePaths":790,"frontmatter":791,"imagePaths":792},[786,788],{"depth":29,"slug":787,"text":775},"detect-objects",{"depth":32,"slug":131,"text":132},[],[],{"title":775},[],"nodes/detect-objects.md","nodes/grayscale",{"id":794,"data":796,"body":801,"filePath":802,"digest":803,"rendered":804,"legacyId":815},{"title":797,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":798,"sidebar":799},"Grayscale",[],{"hidden":17,"attrs":800},{},"# Grayscale\n\nThis node returns an input image converted to a grayscale range.\n\nThe node doesn't have any parameters.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/grayscale.jpg\" alt=\"Figment grayscale node example\"/>","src/content/docs/nodes/grayscale.md","799bc7cef127dd2d",{"html":805,"metadata":806},"\u003Ch1 id=\"grayscale\">Grayscale\u003C/h1>\n\u003Cp>This node returns an input image converted to a grayscale range.\u003C/p>\n\u003Cp>The node doesn’t have any parameters.\u003C/p>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/grayscale.jpg\" alt=\"Figment grayscale node example\">",{"headings":807,"localImagePaths":811,"remoteImagePaths":812,"frontmatter":813,"imagePaths":814},[808,810],{"depth":29,"slug":809,"text":797},"grayscale",{"depth":32,"slug":213,"text":214},[],[],{"title":797},[],"nodes/grayscale.md","nodes/image-to-image",{"id":816,"data":818,"body":823,"filePath":824,"digest":825,"rendered":826,"legacyId":837},{"title":819,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":820,"sidebar":821},"Image to Image",[],{"hidden":17,"attrs":822},{},"# Image to Image\n\nRun a PIX2PIX image to image model. Note that there is also a ONNX version of this node: [ONNX Image Model](onnx-image-model.md).\n\nThis currently only runs 512x512 image models. We're working on a version that supports smaller image models as well.\n\nTo work well, it needs an input that is similar to what it has seen in training; e.g. if you trained it on a hand model, you need to feed it hand models with the same color and line thickness.\n\n## Parameters\n\n- **Model** The directory of the model. Note that this is the **directory** (often called `tfjs`), not a file in the directory.","src/content/docs/nodes/image-to-image.md","3c7c470aca87a78d",{"html":827,"metadata":828},"\u003Ch1 id=\"image-to-image\">Image to Image\u003C/h1>\n\u003Cp>Run a PIX2PIX image to image model. Note that there is also a ONNX version of this node: \u003Ca href=\"onnx-image-model.md\">ONNX Image Model\u003C/a>.\u003C/p>\n\u003Cp>This currently only runs 512x512 image models. We’re working on a version that supports smaller image models as well.\u003C/p>\n\u003Cp>To work well, it needs an input that is similar to what it has seen in training; e.g. if you trained it on a hand model, you need to feed it hand models with the same color and line thickness.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Model\u003C/strong> The directory of the model. Note that this is the \u003Cstrong>directory\u003C/strong> (often called \u003Ccode>tfjs\u003C/code>), not a file in the directory.\u003C/li>\n\u003C/ul>",{"headings":829,"localImagePaths":833,"remoteImagePaths":834,"frontmatter":835,"imagePaths":836},[830,832],{"depth":29,"slug":831,"text":819},"image-to-image",{"depth":32,"slug":131,"text":132},[],[],{"title":819},[],"nodes/image-to-image.md","nodes/inms",{"id":838,"data":840,"body":845,"filePath":846,"digest":847,"rendered":848,"legacyId":860},{"title":841,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":842,"sidebar":843},"INMS",[],{"hidden":17,"attrs":844},{},"# INMS\n\nThis node computes INMS edges on an input image.\nINMS stands for Intensity-based Non-Maximum Suppression.\n\n## Parameters\n\n- **Blur**\n- **Increase Fx**\n- **Threshold**\n\n## Example\n\n\u003Cimg src=\"/img/nodes/inms.jpg\" alt=\"Figment inms node example\"/>","src/content/docs/nodes/inms.md","6b41b2c588129faa",{"html":849,"metadata":850},"\u003Ch1 id=\"inms\">INMS\u003C/h1>\n\u003Cp>This node computes INMS edges on an input image.\nINMS stands for Intensity-based Non-Maximum Suppression.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Blur\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Increase Fx\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Threshold\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/inms.jpg\" alt=\"Figment inms node example\">",{"headings":851,"localImagePaths":856,"remoteImagePaths":857,"frontmatter":858,"imagePaths":859},[852,854,855],{"depth":29,"slug":853,"text":841},"inms",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":841},[],"nodes/inms.md","nodes/lens-distortion",{"id":861,"data":863,"body":868,"filePath":869,"digest":870,"rendered":871,"legacyId":883},{"title":864,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":865,"sidebar":866},"Lens Distortion",[],{"hidden":17,"attrs":867},{},"# Lens Distortion\n\nEmulates the distortion effect of a physical lens.\n\n## Parameters\n\n- **K1 / K2** The distortion parameters. Honestly, they're too complex to explain - just play around with them.\n- **Offset X** X offset of the input image.\n- **Offset Y** Y offset of the input image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/lens-distortion.jpg\" alt=\"Figment lens distortion node example\"/>","src/content/docs/nodes/lens-distortion.md","bcb3f9fb9641705c",{"html":872,"metadata":873},"\u003Ch1 id=\"lens-distortion\">Lens Distortion\u003C/h1>\n\u003Cp>Emulates the distortion effect of a physical lens.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>K1 / K2\u003C/strong> The distortion parameters. Honestly, they’re too complex to explain - just play around with them.\u003C/li>\n\u003Cli>\u003Cstrong>Offset X\u003C/strong> X offset of the input image.\u003C/li>\n\u003Cli>\u003Cstrong>Offset Y\u003C/strong> Y offset of the input image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/lens-distortion.jpg\" alt=\"Figment lens distortion node example\">",{"headings":874,"localImagePaths":879,"remoteImagePaths":880,"frontmatter":881,"imagePaths":882},[875,877,878],{"depth":29,"slug":876,"text":864},"lens-distortion",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":864},[],"nodes/lens-distortion.md","nodes",{"id":884,"data":886,"body":891,"filePath":892,"digest":893,"rendered":894,"legacyId":922},{"title":887,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":888,"sidebar":889},"Figment Nodes",[],{"hidden":17,"attrs":890},{},"# Figment Nodes\n\nThis is a list of all the nodes in Figment:\n\n## Core\n\n- [Null](./null): Does nothing.\n- [Out](./out): Signifies that this is the output of the network.\n\n## Image Operations\n\n### Creating / Loading Images\n\n- [Constant](./constant): Render a constant color.\n- [Fetch Image](./fetch-image): Fetch an image from the internet.\n- [Load Image](./load-image): Load an image from a file.\n- [Load Image Folder](./load-image-folder): Load a folder of images.\n- [Load Movie](./load-movie): Load a movie file.\n- [Unsplash](./unsplash): Fetch a random image from Unsplash.\n- [Webcam Image](./webcam-image): Return a webcam stream.\n\n### Resizing / Cropping / Combining images\n\n- [Resize](./resize): Resize the input image.\n- [Crop](./crop): Crop an input image.\n- [Composite](./composite): Combine two images together.\n- [Stack](./stack): Combine 2 images horizontally / vertically.\n\n### Filters / Effects\n\nThe images of the examples courtesy of [John Mark Arnold](https://unsplash.com/@johnmarkarnold) and [Sergey Shmidt](https://unsplash.com/@monstercritic).\n\n- [Blur](./blur): Blur an input image.\n- [Border](./border): Generate a border around the image.\n- [Canny](./canny): Canny edge detection on input image.\n- [Mask Ellipse](./mask-ellipse): Draw a circular mask of an image or color.\n- [Emboss](./emboss): Emboss convolution on an input image.\n- [Grayscale](./grayscale): Convert the input image to grayscale.\n- [Invert](./invert): Invert the colors of input image.\n- [Lens Distortion](./lens-distortion): Distort an image using a lens distortion shader.\n- [Levels](./levels): Change the brightness/contrast/saturation of an image.\n- [Lookup](./lookup): Map the colors of one image to another image.\n- [Mirror](./mirror): Mirror the input image over a specific axis.\n- [Modulate Color](./modulate-color): Adjust the colors of the input image.\n- [Pixelate](./pixelate): Pixelate the input image.\n- [Reduce Color](./reduce-color): Reduce the amount of colors of input image.\n- [Sharpen](./sharpen): Sharpen an input image\n- [Sobel](./sobel): Sobel edge detection on input image.\n- [Squares](./squares): Return input image as squares.\n- [Threshold](./threshold): Change brightness threshold of input image.\n- [Trail](./trail): Don't erase the previous input image, creating a trail.\n- [Transform](./transform): Transform the image.\n\n### Machine Learning\n\n- [Detect Objects](./detect-objects): Detect objects in an image and draws labels around them.\n- [Detect Faces](./detect-faces): Detect faces in an image.\n- [Detect Pose](./detect-pose): Detect human poses in input image.\n- [Segment Pose](./segment-pose): Remove the background from an image.\n- [Detect Hands](./detect-hands): Detect hands in an input image.\n- [Image to Image](./image-to-image): Run a generative image to image model (pix2pix).","src/content/docs/nodes/index.md","156eade4f9faba84",{"html":895,"metadata":896},"\u003Ch1 id=\"figment-nodes\">Figment Nodes\u003C/h1>\n\u003Cp>This is a list of all the nodes in Figment:\u003C/p>\n\u003Ch2 id=\"core\">Core\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Ca href=\"./null\">Null\u003C/a>: Does nothing.\u003C/li>\n\u003Cli>\u003Ca href=\"./out\">Out\u003C/a>: Signifies that this is the output of the network.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"image-operations\">Image Operations\u003C/h2>\n\u003Ch3 id=\"creating--loading-images\">Creating / Loading Images\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ca href=\"./constant\">Constant\u003C/a>: Render a constant color.\u003C/li>\n\u003Cli>\u003Ca href=\"./fetch-image\">Fetch Image\u003C/a>: Fetch an image from the internet.\u003C/li>\n\u003Cli>\u003Ca href=\"./load-image\">Load Image\u003C/a>: Load an image from a file.\u003C/li>\n\u003Cli>\u003Ca href=\"./load-image-folder\">Load Image Folder\u003C/a>: Load a folder of images.\u003C/li>\n\u003Cli>\u003Ca href=\"./load-movie\">Load Movie\u003C/a>: Load a movie file.\u003C/li>\n\u003Cli>\u003Ca href=\"./unsplash\">Unsplash\u003C/a>: Fetch a random image from Unsplash.\u003C/li>\n\u003Cli>\u003Ca href=\"./webcam-image\">Webcam Image\u003C/a>: Return a webcam stream.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"resizing--cropping--combining-images\">Resizing / Cropping / Combining images\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ca href=\"./resize\">Resize\u003C/a>: Resize the input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./crop\">Crop\u003C/a>: Crop an input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./composite\">Composite\u003C/a>: Combine two images together.\u003C/li>\n\u003Cli>\u003Ca href=\"./stack\">Stack\u003C/a>: Combine 2 images horizontally / vertically.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"filters--effects\">Filters / Effects\u003C/h3>\n\u003Cp>The images of the examples courtesy of \u003Ca href=\"https://unsplash.com/@johnmarkarnold\">John Mark Arnold\u003C/a> and \u003Ca href=\"https://unsplash.com/@monstercritic\">Sergey Shmidt\u003C/a>.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"./blur\">Blur\u003C/a>: Blur an input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./border\">Border\u003C/a>: Generate a border around the image.\u003C/li>\n\u003Cli>\u003Ca href=\"./canny\">Canny\u003C/a>: Canny edge detection on input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./mask-ellipse\">Mask Ellipse\u003C/a>: Draw a circular mask of an image or color.\u003C/li>\n\u003Cli>\u003Ca href=\"./emboss\">Emboss\u003C/a>: Emboss convolution on an input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./grayscale\">Grayscale\u003C/a>: Convert the input image to grayscale.\u003C/li>\n\u003Cli>\u003Ca href=\"./invert\">Invert\u003C/a>: Invert the colors of input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./lens-distortion\">Lens Distortion\u003C/a>: Distort an image using a lens distortion shader.\u003C/li>\n\u003Cli>\u003Ca href=\"./levels\">Levels\u003C/a>: Change the brightness/contrast/saturation of an image.\u003C/li>\n\u003Cli>\u003Ca href=\"./lookup\">Lookup\u003C/a>: Map the colors of one image to another image.\u003C/li>\n\u003Cli>\u003Ca href=\"./mirror\">Mirror\u003C/a>: Mirror the input image over a specific axis.\u003C/li>\n\u003Cli>\u003Ca href=\"./modulate-color\">Modulate Color\u003C/a>: Adjust the colors of the input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./pixelate\">Pixelate\u003C/a>: Pixelate the input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./reduce-color\">Reduce Color\u003C/a>: Reduce the amount of colors of input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./sharpen\">Sharpen\u003C/a>: Sharpen an input image\u003C/li>\n\u003Cli>\u003Ca href=\"./sobel\">Sobel\u003C/a>: Sobel edge detection on input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./squares\">Squares\u003C/a>: Return input image as squares.\u003C/li>\n\u003Cli>\u003Ca href=\"./threshold\">Threshold\u003C/a>: Change brightness threshold of input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./trail\">Trail\u003C/a>: Don’t erase the previous input image, creating a trail.\u003C/li>\n\u003Cli>\u003Ca href=\"./transform\">Transform\u003C/a>: Transform the image.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"machine-learning\">Machine Learning\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ca href=\"./detect-objects\">Detect Objects\u003C/a>: Detect objects in an image and draws labels around them.\u003C/li>\n\u003Cli>\u003Ca href=\"./detect-faces\">Detect Faces\u003C/a>: Detect faces in an image.\u003C/li>\n\u003Cli>\u003Ca href=\"./detect-pose\">Detect Pose\u003C/a>: Detect human poses in input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./segment-pose\">Segment Pose\u003C/a>: Remove the background from an image.\u003C/li>\n\u003Cli>\u003Ca href=\"./detect-hands\">Detect Hands\u003C/a>: Detect hands in an input image.\u003C/li>\n\u003Cli>\u003Ca href=\"./image-to-image\">Image to Image\u003C/a>: Run a generative image to image model (pix2pix).\u003C/li>\n\u003C/ul>",{"headings":897,"localImagePaths":918,"remoteImagePaths":919,"frontmatter":920,"imagePaths":921},[898,900,903,906,909,912,915],{"depth":29,"slug":899,"text":887},"figment-nodes",{"depth":32,"slug":901,"text":902},"core","Core",{"depth":32,"slug":904,"text":905},"image-operations","Image Operations",{"depth":178,"slug":907,"text":908},"creating--loading-images","Creating / Loading Images",{"depth":178,"slug":910,"text":911},"resizing--cropping--combining-images","Resizing / Cropping / Combining images",{"depth":178,"slug":913,"text":914},"filters--effects","Filters / Effects",{"depth":178,"slug":916,"text":917},"machine-learning","Machine Learning",[],[],{"title":887},[],"nodes/index.md","nodes/levels",{"id":923,"data":925,"body":930,"filePath":931,"digest":932,"rendered":933,"legacyId":945},{"title":926,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":927,"sidebar":928},"Levels",[],{"hidden":17,"attrs":929},{},"# Levels\n\nThis node changes the brightness, contrast and saturation of an input image.\n\nThe scale for each individual parameter is in percent: `0` to `100`.\n\n## Parameters\n\n- **Brightness** Sets the brigthness of the input image.\n- **Contrast** Sets the contrast of the input image.\n- **Saturation** Sets the saturation of the input image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/levels.jpg\" alt=\"Figment levels node example\"/>","src/content/docs/nodes/levels.md","7b2cab840c06b922",{"html":934,"metadata":935},"\u003Ch1 id=\"levels\">Levels\u003C/h1>\n\u003Cp>This node changes the brightness, contrast and saturation of an input image.\u003C/p>\n\u003Cp>The scale for each individual parameter is in percent: \u003Ccode>0\u003C/code> to \u003Ccode>100\u003C/code>.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Brightness\u003C/strong> Sets the brigthness of the input image.\u003C/li>\n\u003Cli>\u003Cstrong>Contrast\u003C/strong> Sets the contrast of the input image.\u003C/li>\n\u003Cli>\u003Cstrong>Saturation\u003C/strong> Sets the saturation of the input image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/levels.jpg\" alt=\"Figment levels node example\">",{"headings":936,"localImagePaths":941,"remoteImagePaths":942,"frontmatter":943,"imagePaths":944},[937,939,940],{"depth":29,"slug":938,"text":926},"levels",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":926},[],"nodes/levels.md","nodes/load-image-folder",{"id":946,"data":948,"body":953,"filePath":954,"digest":955,"rendered":956,"legacyId":967},{"title":949,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":950,"sidebar":951},"Load Image Folder",[],{"hidden":17,"attrs":952},{},"# Load Image Folder\n\nThis node loads a list of images from disk.\n\n## Parameters\n\n- **Folder** The folder that needs to be selected from. Figment will use [relative paths](/docs/structuring#relative-paths) if the file was saved.\n- **Filter** Specifies the image type (jpg / png)\n- **Animate** Play through the list of images. Turn this off to stop playing.\n- **Frame Rate** The speed in fps to run through the selected images","src/content/docs/nodes/load-image-folder.md","3250d37c48fafb2d",{"html":957,"metadata":958},"\u003Ch1 id=\"load-image-folder\">Load Image Folder\u003C/h1>\n\u003Cp>This node loads a list of images from disk.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Folder\u003C/strong> The folder that needs to be selected from. Figment will use \u003Ca href=\"/docs/structuring#relative-paths\">relative paths\u003C/a> if the file was saved.\u003C/li>\n\u003Cli>\u003Cstrong>Filter\u003C/strong> Specifies the image type (jpg / png)\u003C/li>\n\u003Cli>\u003Cstrong>Animate\u003C/strong> Play through the list of images. Turn this off to stop playing.\u003C/li>\n\u003Cli>\u003Cstrong>Frame Rate\u003C/strong> The speed in fps to run through the selected images\u003C/li>\n\u003C/ul>",{"headings":959,"localImagePaths":963,"remoteImagePaths":964,"frontmatter":965,"imagePaths":966},[960,962],{"depth":29,"slug":961,"text":949},"load-image-folder",{"depth":32,"slug":131,"text":132},[],[],{"title":949},[],"nodes/load-image-folder.md","nodes/instagram-filters",{"id":968,"data":970,"body":975,"filePath":976,"digest":977,"rendered":978,"legacyId":990},{"title":971,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":972,"sidebar":973},"Instagram Filters",[],{"hidden":17,"attrs":974},{},"# Instagram Filters\n\nThis node places an instagram like filter on an input image.\nThe available filters are:\n\nAmaro: A filter that adds a light, hazy effect and enhances the highlights and shadows.\nClarendon: A filter that adds a blue-purple tint and enhances the contrast and saturation.\nJuno: A warm filter that enhances the colors and adds a slight vignette.\nLark: A bright and airy filter that boosts the blues and greens in the image.\nNashville: A vintage filter that adds a warm, yellowish tint and desaturates the image.\nValencia: A filter that adds a warm, sepia-like tone and a slight vignette.\n\n## Parameters\n\n- **Filter**\n\n## Example\n\n\u003Cimg src=\"/img/nodes/instagram.jpg\" alt=\"Figment instagram filter node example\"/>","src/content/docs/nodes/instagram-filters.md","460fbec6c789b044",{"html":979,"metadata":980},"\u003Ch1 id=\"instagram-filters\">Instagram Filters\u003C/h1>\n\u003Cp>This node places an instagram like filter on an input image.\nThe available filters are:\u003C/p>\n\u003Cp>Amaro: A filter that adds a light, hazy effect and enhances the highlights and shadows.\nClarendon: A filter that adds a blue-purple tint and enhances the contrast and saturation.\nJuno: A warm filter that enhances the colors and adds a slight vignette.\nLark: A bright and airy filter that boosts the blues and greens in the image.\nNashville: A vintage filter that adds a warm, yellowish tint and desaturates the image.\nValencia: A filter that adds a warm, sepia-like tone and a slight vignette.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Filter\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/instagram.jpg\" alt=\"Figment instagram filter node example\">",{"headings":981,"localImagePaths":986,"remoteImagePaths":987,"frontmatter":988,"imagePaths":989},[982,984,985],{"depth":29,"slug":983,"text":971},"instagram-filters",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":971},[],"nodes/instagram-filters.md","nodes/invert",{"id":991,"data":993,"body":998,"filePath":999,"digest":1000,"rendered":1001,"legacyId":1012},{"title":994,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":995,"sidebar":996},"Invert",[],{"hidden":17,"attrs":997},{},"# Invert\n\nThis node inverts the colors of an input image.\n\nThe node doesn't have any parameters.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/invert.jpg\" alt=\"Figment invert node example\"/>","src/content/docs/nodes/invert.md","45ac68defcfae3af",{"html":1002,"metadata":1003},"\u003Ch1 id=\"invert\">Invert\u003C/h1>\n\u003Cp>This node inverts the colors of an input image.\u003C/p>\n\u003Cp>The node doesn’t have any parameters.\u003C/p>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/invert.jpg\" alt=\"Figment invert node example\">",{"headings":1004,"localImagePaths":1008,"remoteImagePaths":1009,"frontmatter":1010,"imagePaths":1011},[1005,1007],{"depth":29,"slug":1006,"text":994},"invert",{"depth":32,"slug":213,"text":214},[],[],{"title":994},[],"nodes/invert.md","nodes/load-image",{"id":1013,"data":1015,"body":1020,"filePath":1021,"digest":1022,"rendered":1023,"legacyId":1034},{"title":1016,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1017,"sidebar":1018},"Load Image",[],{"hidden":17,"attrs":1019},{},"# Load Image\n\nThis node loads an image from disk.\n\n## Parameters\n\n- **File** The file that needs to be retrieved. Figment will use [relative paths](/docs/structuring#relative-paths) if the file was saved.","src/content/docs/nodes/load-image.md","2b526d5351abb5bb",{"html":1024,"metadata":1025},"\u003Ch1 id=\"load-image\">Load Image\u003C/h1>\n\u003Cp>This node loads an image from disk.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>File\u003C/strong> The file that needs to be retrieved. Figment will use \u003Ca href=\"/docs/structuring#relative-paths\">relative paths\u003C/a> if the file was saved.\u003C/li>\n\u003C/ul>",{"headings":1026,"localImagePaths":1030,"remoteImagePaths":1031,"frontmatter":1032,"imagePaths":1033},[1027,1029],{"depth":29,"slug":1028,"text":1016},"load-image",{"depth":32,"slug":131,"text":132},[],[],{"title":1016},[],"nodes/load-image.md","nodes/load-movie",{"id":1035,"data":1037,"body":1042,"filePath":1043,"digest":1044,"rendered":1045,"legacyId":1057},{"title":1038,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1039,"sidebar":1040},"Load Movie",[],{"hidden":17,"attrs":1041},{},"# Load Movie\n\nThis node plays a movie from disk.\n\n## Parameters\n\n- **File** The file that needs to be played. Figment will use [relative paths](/docs/structuring#relative-paths) if the file was saved.\n- **Quality** Choose between `fast` (uses native video element) or `accurate` (frame-perfect playback using [MediaBunny](https://mediabunny.dev/)). Default is `fast`.\n- **Play** Play the video. Turn this off to pause playback.\n- **Loop** Loop the video when it reaches the end. Default is on.\n- **Pause Mode** Determines behavior when pausing:\n  - `hold` - Keep the current frame (default)\n  - `restart` - Jump to the first frame when resuming\n  - `rewind` - Jump to the first frame immediately when pausing\n- **Speed** The playback speed of the video. A speed of `1` means the original rate, `0.5` is half speed, `2` is double speed, and so on. Range: 0.0 to 10.\n- **Restart** Button to restart the movie from the beginning.\n- **Frame** Manually select which frame to display (1-based). Only works when playback is paused.\n\n## Outputs\n\n- **Out** The current video frame as an image.\n- **Frame Count** The total number of frames in the video.\n- **Current Frame** The current frame number (1-based).\n- **FPS** The detected frame rate of the video.\n- **Duration** The duration of the video in seconds.","src/content/docs/nodes/load-movie.md","1284734646c6f3b1",{"html":1046,"metadata":1047},"\u003Ch1 id=\"load-movie\">Load Movie\u003C/h1>\n\u003Cp>This node plays a movie from disk.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>File\u003C/strong> The file that needs to be played. Figment will use \u003Ca href=\"/docs/structuring#relative-paths\">relative paths\u003C/a> if the file was saved.\u003C/li>\n\u003Cli>\u003Cstrong>Quality\u003C/strong> Choose between \u003Ccode>fast\u003C/code> (uses native video element) or \u003Ccode>accurate\u003C/code> (frame-perfect playback using \u003Ca href=\"https://mediabunny.dev/\">MediaBunny\u003C/a>). Default is \u003Ccode>fast\u003C/code>.\u003C/li>\n\u003Cli>\u003Cstrong>Play\u003C/strong> Play the video. Turn this off to pause playback.\u003C/li>\n\u003Cli>\u003Cstrong>Loop\u003C/strong> Loop the video when it reaches the end. Default is on.\u003C/li>\n\u003Cli>\u003Cstrong>Pause Mode\u003C/strong> Determines behavior when pausing:\n\u003Cul>\n\u003Cli>\u003Ccode>hold\u003C/code> - Keep the current frame (default)\u003C/li>\n\u003Cli>\u003Ccode>restart\u003C/code> - Jump to the first frame when resuming\u003C/li>\n\u003Cli>\u003Ccode>rewind\u003C/code> - Jump to the first frame immediately when pausing\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Cstrong>Speed\u003C/strong> The playback speed of the video. A speed of \u003Ccode>1\u003C/code> means the original rate, \u003Ccode>0.5\u003C/code> is half speed, \u003Ccode>2\u003C/code> is double speed, and so on. Range: 0.0 to 10.\u003C/li>\n\u003Cli>\u003Cstrong>Restart\u003C/strong> Button to restart the movie from the beginning.\u003C/li>\n\u003Cli>\u003Cstrong>Frame\u003C/strong> Manually select which frame to display (1-based). Only works when playback is paused.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outputs\">Outputs\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Out\u003C/strong> The current video frame as an image.\u003C/li>\n\u003Cli>\u003Cstrong>Frame Count\u003C/strong> The total number of frames in the video.\u003C/li>\n\u003Cli>\u003Cstrong>Current Frame\u003C/strong> The current frame number (1-based).\u003C/li>\n\u003Cli>\u003Cstrong>FPS\u003C/strong> The detected frame rate of the video.\u003C/li>\n\u003Cli>\u003Cstrong>Duration\u003C/strong> The duration of the video in seconds.\u003C/li>\n\u003C/ul>",{"headings":1048,"localImagePaths":1053,"remoteImagePaths":1054,"frontmatter":1055,"imagePaths":1056},[1049,1051,1052],{"depth":29,"slug":1050,"text":1038},"load-movie",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":417,"text":418},[],[],{"title":1038},[],"nodes/load-movie.md","nodes/log",{"id":1058,"data":1060,"body":1065,"filePath":1066,"digest":1067,"rendered":1068,"legacyId":1080},{"title":1061,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1062,"sidebar":1063},"LoG",[],{"hidden":17,"attrs":1064},{},"# LoG\n\nThis node computes LoG edges on an input image.\nLog stands for Laplacian of Gaussian.\n\n## Parameters\n\n- **Blur**\n- **Increase Fx**\n\n## Example\n\n\u003Cimg src=\"/img/nodes/log.jpg\" alt=\"Figment LoG node example\"/>","src/content/docs/nodes/log.md","588edb84f41d5fa8",{"html":1069,"metadata":1070},"\u003Ch1 id=\"log\">LoG\u003C/h1>\n\u003Cp>This node computes LoG edges on an input image.\nLog stands for Laplacian of Gaussian.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Blur\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Increase Fx\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/log.jpg\" alt=\"Figment LoG node example\">",{"headings":1071,"localImagePaths":1076,"remoteImagePaths":1077,"frontmatter":1078,"imagePaths":1079},[1072,1074,1075],{"depth":29,"slug":1073,"text":1061},"log",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1061},[],"nodes/log.md","nodes/lookup",{"id":1081,"data":1083,"body":1088,"filePath":1089,"digest":1090,"rendered":1091,"legacyId":1103},{"title":1084,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1085,"sidebar":1086},"Lookup",[],{"hidden":17,"attrs":1087},{},"# Lookup\n\nThis node maps the colors of an image to the colors of another image. It needs two images as input.\n\n## Parameters\n\n- **Method** Sets the method of mapping. Values are `luminance`, `red`, `green`, `blue` and `alpha`.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/lookup.jpg\" alt=\"Figment lookup node example\"/>","src/content/docs/nodes/lookup.md","707e71773443f9e4",{"html":1092,"metadata":1093},"\u003Ch1 id=\"lookup\">Lookup\u003C/h1>\n\u003Cp>This node maps the colors of an image to the colors of another image. It needs two images as input.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Method\u003C/strong> Sets the method of mapping. Values are \u003Ccode>luminance\u003C/code>, \u003Ccode>red\u003C/code>, \u003Ccode>green\u003C/code>, \u003Ccode>blue\u003C/code> and \u003Ccode>alpha\u003C/code>.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/lookup.jpg\" alt=\"Figment lookup node example\">",{"headings":1094,"localImagePaths":1099,"remoteImagePaths":1100,"frontmatter":1101,"imagePaths":1102},[1095,1097,1098],{"depth":29,"slug":1096,"text":1084},"lookup",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1084},[],"nodes/lookup.md","nodes/mask-ellipse",{"id":1104,"data":1106,"body":1111,"filePath":1112,"digest":1113,"rendered":1114,"legacyId":1126},{"title":1107,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1108,"sidebar":1109},"Mask Ellipse",[],{"hidden":17,"attrs":1110},{},"# Mask Ellipse\n\nThis node renders an ellipse shaped mask on an input image.\n\n## Parameters\n\n- **Radius** Sets the radius of the ellipse shaped mask.\n- **Invert** Toggle on keeps the image inside the shape. Toggle off returns inverted mask.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/mask-ellipse.jpg\" alt=\"Figment mask ellipse node example\"/>","src/content/docs/nodes/mask-ellipse.md","84c9ce03218092e5",{"html":1115,"metadata":1116},"\u003Ch1 id=\"mask-ellipse\">Mask Ellipse\u003C/h1>\n\u003Cp>This node renders an ellipse shaped mask on an input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Radius\u003C/strong> Sets the radius of the ellipse shaped mask.\u003C/li>\n\u003Cli>\u003Cstrong>Invert\u003C/strong> Toggle on keeps the image inside the shape. Toggle off returns inverted mask.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/mask-ellipse.jpg\" alt=\"Figment mask ellipse node example\">",{"headings":1117,"localImagePaths":1122,"remoteImagePaths":1123,"frontmatter":1124,"imagePaths":1125},[1118,1120,1121],{"depth":29,"slug":1119,"text":1107},"mask-ellipse",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1107},[],"nodes/mask-ellipse.md","nodes/mask",{"id":1127,"data":1129,"body":1134,"filePath":1135,"digest":1136,"rendered":1137,"legacyId":1149},{"title":1130,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1131,"sidebar":1132},"Mask image",[],{"hidden":17,"attrs":1133},{},"# Mask image\n\nThe node masks the input image with another image.\n\n## Parameters\n\n- **Mask Method**: The type of masking operation. Options are `white` and `alpha`.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/mask.jpg\" alt=\"Figment mask node example\"/>","src/content/docs/nodes/mask.md","1ca849a50478d75a",{"html":1138,"metadata":1139},"\u003Ch1 id=\"mask-image\">Mask image\u003C/h1>\n\u003Cp>The node masks the input image with another image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Mask Method\u003C/strong>: The type of masking operation. Options are \u003Ccode>white\u003C/code> and \u003Ccode>alpha\u003C/code>.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/mask.jpg\" alt=\"Figment mask node example\">",{"headings":1140,"localImagePaths":1145,"remoteImagePaths":1146,"frontmatter":1147,"imagePaths":1148},[1141,1143,1144],{"depth":29,"slug":1142,"text":1130},"mask-image",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1130},[],"nodes/mask.md","nodes/modulate-color",{"id":1150,"data":1152,"body":1157,"filePath":1158,"digest":1159,"rendered":1160,"legacyId":1172},{"title":1153,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1154,"sidebar":1155},"Modulate Color",[],{"hidden":17,"attrs":1156},{},"# Modulate Color\n\nThis node reduces the amount of colors in an input image. It allows to access the red, green and blue channel seperatly.\n\n## Parameters\n\n- **Red** Reduces the amount of colors on the red channel.\n- **Green** Reduces the amount of colors on the green channel.\n- **Blue** Reduces the amount of colors on the blue channel.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/modulate-color.jpg\" alt=\"Figment modulate color node example\"/>","src/content/docs/nodes/modulate-color.md","f13f41a1c0026df6",{"html":1161,"metadata":1162},"\u003Ch1 id=\"modulate-color\">Modulate Color\u003C/h1>\n\u003Cp>This node reduces the amount of colors in an input image. It allows to access the red, green and blue channel seperatly.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Red\u003C/strong> Reduces the amount of colors on the red channel.\u003C/li>\n\u003Cli>\u003Cstrong>Green\u003C/strong> Reduces the amount of colors on the green channel.\u003C/li>\n\u003Cli>\u003Cstrong>Blue\u003C/strong> Reduces the amount of colors on the blue channel.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/modulate-color.jpg\" alt=\"Figment modulate color node example\">",{"headings":1163,"localImagePaths":1168,"remoteImagePaths":1169,"frontmatter":1170,"imagePaths":1171},[1164,1166,1167],{"depth":29,"slug":1165,"text":1153},"modulate-color",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1153},[],"nodes/modulate-color.md","nodes/null",{"id":1173,"data":1175,"body":1180,"filePath":1181,"digest":1182,"rendered":1183,"legacyId":1193},{"title":1176,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1177,"sidebar":1178},"Null",[],{"hidden":17,"attrs":1179},{},"# Null\n\nThe Null node has no effect: it just passes input to output.\n\nThe reason the null node exists is as an organisational tool.\nThe node can be a single point where you can connect multiple other nodes to.\n\nOften you have some processing of the input image, after which you want to do a number of things to it. The null node sits in the middle, avoiding you to have to reconnect all the nodes:\n\n\u003Cimg src=\"/img/nodes/null.jpg\" alt=\"Figment null node setup\"/>\n\nIf you now want to add another node after the \"crop\" node, you can put it in between the \"crop\" and \"null\" node, without having to reconnect all the nodes below it.","src/content/docs/nodes/null.md","3732e626d00d06d2",{"html":1184,"metadata":1185},"\u003Ch1 id=\"null\">Null\u003C/h1>\n\u003Cp>The Null node has no effect: it just passes input to output.\u003C/p>\n\u003Cp>The reason the null node exists is as an organisational tool.\nThe node can be a single point where you can connect multiple other nodes to.\u003C/p>\n\u003Cp>Often you have some processing of the input image, after which you want to do a number of things to it. The null node sits in the middle, avoiding you to have to reconnect all the nodes:\u003C/p>\n\u003Cimg src=\"/img/nodes/null.jpg\" alt=\"Figment null node setup\">\n\u003Cp>If you now want to add another node after the “crop” node, you can put it in between the “crop” and “null” node, without having to reconnect all the nodes below it.\u003C/p>",{"headings":1186,"localImagePaths":1189,"remoteImagePaths":1190,"frontmatter":1191,"imagePaths":1192},[1187],{"depth":29,"slug":1188,"text":1176},"null",[],[],{"title":1176},[],"nodes/null.md","nodes/mirror",{"id":1194,"data":1196,"body":1201,"filePath":1202,"digest":1203,"rendered":1204,"legacyId":1216},{"title":1197,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1198,"sidebar":1199},"Mirror",[],{"hidden":17,"attrs":1200},{},"# Mirror\n\nThis node mirrors an input image over an axis.\n\n## Parameters\n\n- **Pivot X** Sets the x value of the pivoting point.\n- **Pivot Y** Sets the y value of the pivoting point.\n- **Angle** Sets the angle for the rotation around the pivoting point. Angle is set as degrees so values go from `0.0` or `360.0`\n\n## Example\n\n\u003Cimg src=\"/img/nodes/mirror.jpg\" alt=\"Figment mirror node example\"/>","src/content/docs/nodes/mirror.md","6d7b97e6ebbe4195",{"html":1205,"metadata":1206},"\u003Ch1 id=\"mirror\">Mirror\u003C/h1>\n\u003Cp>This node mirrors an input image over an axis.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Pivot X\u003C/strong> Sets the x value of the pivoting point.\u003C/li>\n\u003Cli>\u003Cstrong>Pivot Y\u003C/strong> Sets the y value of the pivoting point.\u003C/li>\n\u003Cli>\u003Cstrong>Angle\u003C/strong> Sets the angle for the rotation around the pivoting point. Angle is set as degrees so values go from \u003Ccode>0.0\u003C/code> or \u003Ccode>360.0\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/mirror.jpg\" alt=\"Figment mirror node example\">",{"headings":1207,"localImagePaths":1212,"remoteImagePaths":1213,"frontmatter":1214,"imagePaths":1215},[1208,1210,1211],{"depth":29,"slug":1209,"text":1197},"mirror",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1197},[],"nodes/mirror.md","nodes/onnx-image-model",{"id":1217,"data":1219,"body":1224,"filePath":1225,"digest":1226,"rendered":1227,"legacyId":1241},{"title":1220,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1221,"sidebar":1222},"ONNX Image Model",[],{"hidden":17,"attrs":1223},{},"# ONNX Image Model\n\nRun a PIX2PIX image to image model developed in ONNX. This is similar to [Image to Image](image-to-image.md) but uses an ONNX model instead of a TensorFlow.js model.\n\nThis currently only runs 512x512 image models.\n\nTo work well, it needs an input that is similar to what it has seen in training; e.g. if you trained it on a hand model, you need to feed it hand models with the same color and line thickness.\n\n## Parameters\n\n- **Model** The .onnx model file.\n\n## Training\n\nHere's a simple example of how to train a PIX2PIX model using PyTorch. You will need torch, torchvision, onnx and onnxruntime installed.\n\n```bash\npip install torch==2.4.0 torchvision==0.19.0 onnx==1.16.1 onnxruntime==1.19.0\n```\n\nRun the code with `python train.py --input_dir datasets/trees --output_dir output`.\n\n```python\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.onnx\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport os\nimport random\nimport argparse\nfrom tqdm import tqdm\n\n\n# Create the dataset class\nclass Pix2PixDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = [\n            f for f in os.listdir(root_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")\n        ]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.image_files[idx])\n        image = Image.open(img_name)\n\n        # Split the image into input and target\n        w, h = image.size\n        target_image = image.crop((0, 0, w // 2, h))\n        input_image = image.crop((w // 2, 0, w, h))\n\n        if self.transform:\n            input_image = self.transform(input_image)\n            target_image = self.transform(target_image)\n\n        return input_image, target_image\n\n\n# Implement the UNet architecture for the generator\nclass UNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, bn=True, dropout=False):\n        super(UNetBlock, self).__init__()\n        self.conv = (\n            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)\n            if down\n            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n        )\n        self.bn = nn.BatchNorm2d(out_channels) if bn else None\n        self.dropout = nn.Dropout(0.5) if dropout else None\n        self.act = nn.LeakyReLU(0.2) if down else nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn:\n            x = self.bn(x)\n        if self.dropout:\n            x = self.dropout(x)\n        return self.act(x)\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.down1 = UNetBlock(3, 64, down=True, bn=False)\n        self.down2 = UNetBlock(64, 128)\n        self.down3 = UNetBlock(128, 256)\n        self.down4 = UNetBlock(256, 512)\n        self.down5 = UNetBlock(512, 512)\n        self.down6 = UNetBlock(512, 512)\n        self.down7 = UNetBlock(512, 512)\n        self.down8 = UNetBlock(512, 512, bn=False)\n\n        self.up1 = UNetBlock(512, 512, down=False, dropout=True)\n        self.up2 = UNetBlock(1024, 512, down=False, dropout=True)\n        self.up3 = UNetBlock(1024, 512, down=False, dropout=True)\n        self.up4 = UNetBlock(1024, 512, down=False)\n        self.up5 = UNetBlock(1024, 256, down=False)\n        self.up6 = UNetBlock(512, 128, down=False)\n        self.up7 = UNetBlock(256, 64, down=False)\n\n        self.final = nn.Sequential(nn.ConvTranspose2d(128, 3, 4, 2, 1), nn.Tanh())\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        u1 = self.up1(d8)\n        u2 = self.up2(torch.cat([u1, d7], 1))\n        u3 = self.up3(torch.cat([u2, d6], 1))\n        u4 = self.up4(torch.cat([u3, d5], 1))\n        u5 = self.up5(torch.cat([u4, d4], 1))\n        u6 = self.up6(torch.cat([u5, d3], 1))\n        u7 = self.up7(torch.cat([u6, d2], 1))\n        return self.final(torch.cat([u7, d1], 1))\n\n\n# Implement the discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            UNetBlock(6, 64, bn=False),\n            UNetBlock(64, 128),\n            UNetBlock(128, 256),\n            UNetBlock(256, 512),\n            nn.Conv2d(512, 1, 4, 1, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, y):\n        return self.model(torch.cat([x, y], 1))\n\n\n# Define the loss functions and optimizers\ncriterion_gan = nn.BCELoss()\ncriterion_pixel = nn.L1Loss()\n\n\n# Load snapshot if available\ndef get_latest_snapshot(output_dir):\n    snapshots = glob.glob(os.path.join(output_dir, \"snapshot_epoch_*.pth\"))\n    if not snapshots:\n        return None\n    return max(snapshots, key=os.path.getctime)\n\n\ndef load_snapshot(generator, discriminator, g_optimizer, d_optimizer, snapshot_path):\n    checkpoint = torch.load(snapshot_path, map_location=device, weights_only=False)\n    generator.load_state_dict(checkpoint[\"generator\"])\n    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n    g_optimizer.load_state_dict(checkpoint[\"g_optimizer\"])\n    d_optimizer.load_state_dict(checkpoint[\"d_optimizer\"])\n    start_epoch = int(os.path.basename(snapshot_path).split(\"_\")[2].split(\".\")[0])\n    return start_epoch\n\n\n# 6. Create the training loop\ndef train(generator, discriminator, dataloader, args):\n    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    # Get fixed input/output for visualization\n    fixed_set = next(iter(dataloader))\n    fixed_input = fixed_set[0][0].unsqueeze(0)\n    fixed_target = fixed_set[1][0].unsqueeze(0)\n    # fixed_input = next(iter(dataloader))[0][0].unsqueeze(0)  # Get a fixed input for visualization\n\n    start_epoch = 0\n    if not args.restart:\n        latest_snapshot = get_latest_snapshot(args.output_dir)\n        if latest_snapshot:\n            start_epoch = load_snapshot(\n                generator, discriminator, g_optimizer, d_optimizer, latest_snapshot\n            )\n            print(f\"Resuming training from epoch {start_epoch}\")\n        else:\n            print(\"No snapshots found. Starting from scratch.\")\n    else:\n        print(\"Restarting training from scratch.\")\n\n    for epoch in range(args.epochs):\n        for i, (input_img, target_img) in enumerate(tqdm(dataloader)):\n            input_img = input_img.to(device)\n            target_img = target_img.to(device)\n\n            # Train Discriminator\n            d_optimizer.zero_grad()\n            fake_img = generator(input_img)\n            d_real = discriminator(input_img, target_img)\n            d_fake = discriminator(input_img, fake_img.detach())\n            d_loss_real = criterion_gan(d_real, torch.ones_like(d_real))\n            d_loss_fake = criterion_gan(d_fake, torch.zeros_like(d_fake))\n            d_loss = (d_loss_real + d_loss_fake) / 2\n            d_loss.backward()\n            d_optimizer.step()\n\n            # Train Generator\n            g_optimizer.zero_grad()\n            fake_img = generator(input_img)\n            d_fake = discriminator(input_img, fake_img)\n            g_loss_gan = criterion_gan(d_fake, torch.ones_like(d_fake))\n            g_loss_pixel = criterion_pixel(fake_img, target_img) * 100\n            g_loss = g_loss_gan + g_loss_pixel\n            g_loss.backward()\n            g_optimizer.step()\n\n            if i % args.sample_interval == 0:\n                with torch.no_grad():\n                    fake_img = generator(fixed_input.to(device))\n                    img_sample = torch.cat(\n                        (fixed_input.cpu(), fake_img.cpu(), fixed_target.cpu()), -1\n                    )\n                    save_image(\n                        img_sample,\n                        f\"{args.output_dir}/epoch_{epoch}_iter_{i}.jpg\",\n                        nrow=3,\n                        normalize=True,\n                    )\n\n        if (epoch + 1) % args.snapshot_interval == 0:\n            torch.save(\n                {\n                    \"generator\": generator.state_dict(),\n                    \"discriminator\": discriminator.state_dict(),\n                    \"g_optimizer\": g_optimizer.state_dict(),\n                    \"d_optimizer\": d_optimizer.state_dict(),\n                },\n                f\"{args.output_dir}/snapshot_epoch_{epoch + 1}.pth\",\n            )\n\n            # Save to ONNX format\n            onnx_path = f\"{args.output_dir}/generator_epoch_{epoch + 1}.onnx\"\n            generator.eval()\n            dummy_input = torch.randn(1, 3, 512, 512).to(device)\n            traced_script_module = torch.jit.trace(generator, dummy_input)\n            torch.onnx.export(\n                traced_script_module,\n                dummy_input,\n                onnx_path,\n                export_params=True,\n                opset_version=11,\n                do_constant_folding=True,\n                input_names=[\"input\"],\n                output_names=[\"output\"],\n                dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n            )\n            print(f\"ONNX model exported to {onnx_path}\")\n            generator.train()\n\n\n# 7. Implement the argument parser for configuration\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Conditional GAN with pix2pix architecture\"\n    )\n    parser.add_argument(\n        \"--input_dir\",\n        type=str,\n        default=\"datasets/trees\",\n        help=\"Input dataset directory\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"output\",\n        help=\"Output directory for generated images\",\n    )\n    parser.add_argument(\n        \"--sample_interval\",\n        type=int,\n        default=100,\n        help=\"Interval for saving sample images\",\n    )\n    parser.add_argument(\n        \"--snapshot_interval\",\n        type=int,\n        default=1,\n        help=\"Interval for saving model snapshots\",\n    )\n    parser.add_argument(\n        \"--epochs\", type=int, default=200, help=\"Number of epochs to train\"\n    )\n    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size\")\n    parser.add_argument(\n        \"--restart\", action=\"store_true\", help=\"Restart training from scratch\"\n    )\n    return parser.parse_args()\n\n\n# 8. Set up the main function to run the training\ndef main():\n    args = parse_args()\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize((512, 512)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    dataset = Pix2PixDataset(args.input_dir, transform=transform)\n    dataloader = DataLoader(\n        dataset, batch_size=args.batch_size, shuffle=True, num_workers=4\n    )\n\n    generator = Generator().to(device)\n    discriminator = Discriminator().to(device)\n\n    train(generator, discriminator, dataloader, args)\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    main()\n```","src/content/docs/nodes/onnx-image-model.md","3d6c950001e32025",{"html":1228,"metadata":1229},"\u003Ch1 id=\"onnx-image-model\">ONNX Image Model\u003C/h1>\n\u003Cp>Run a PIX2PIX image to image model developed in ONNX. This is similar to \u003Ca href=\"image-to-image.md\">Image to Image\u003C/a> but uses an ONNX model instead of a TensorFlow.js model.\u003C/p>\n\u003Cp>This currently only runs 512x512 image models.\u003C/p>\n\u003Cp>To work well, it needs an input that is similar to what it has seen in training; e.g. if you trained it on a hand model, you need to feed it hand models with the same color and line thickness.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Model\u003C/strong> The .onnx model file.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"training\">Training\u003C/h2>\n\u003Cp>Here’s a simple example of how to train a PIX2PIX model using PyTorch. You will need torch, torchvision, onnx and onnxruntime installed.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">pip\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> install\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> torch==\u003C/span>\u003Cspan style=\"color:#79B8FF\">2.4.0\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> torchvision==\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.19.0\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> onnx==\u003C/span>\u003Cspan style=\"color:#79B8FF\">1.16.1\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> onnxruntime==\u003C/span>\u003Cspan style=\"color:#79B8FF\">1.19.0\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Run the code with \u003Ccode>python train.py --input_dir datasets/trees --output_dir output\u003C/code>.\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> glob\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.nn \u003C/span>\u003Cspan style=\"color:#F97583\">as\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.optim \u003C/span>\u003Cspan style=\"color:#F97583\">as\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> optim\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.onnx\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.utils.data \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Dataset, DataLoader\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torchvision \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> transforms\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torchvision.utils \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> save_image\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#79B8FF\"> PIL\u003C/span>\u003Cspan style=\"color:#F97583\"> import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Image\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> os\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> random\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> argparse\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> tqdm \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> tqdm\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Create the dataset class\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">class\u003C/span>\u003Cspan style=\"color:#B392F0\"> Pix2PixDataset\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">Dataset\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, root_dir, transform\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.root_dir \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> root_dir\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.transform \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> transform\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.image_files \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            f \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> f \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> os.listdir(root_dir) \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> f.endswith(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\".jpg\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#F97583\">or\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> f.endswith(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\".png\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __len__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.image_files)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __getitem__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, idx):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        img_name \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> os.path.join(\u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.root_dir, \u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.image_files[idx])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        image \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Image.open(img_name)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">        # Split the image into input and target\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        w, h \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> image.size\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        target_image \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> image.crop((\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, w \u003C/span>\u003Cspan style=\"color:#F97583\">//\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, h))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        input_image \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> image.crop((w \u003C/span>\u003Cspan style=\"color:#F97583\">//\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, w, h))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.transform:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            input_image \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.transform(input_image)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            target_image \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.transform(target_image)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> input_image, target_image\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Implement the UNet architecture for the generator\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">class\u003C/span>\u003Cspan style=\"color:#B392F0\"> UNetBlock\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">nn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Module\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, in_channels, out_channels, down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, bn\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, dropout\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        super\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(UNetBlock, \u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).\u003C/span>\u003Cspan style=\"color:#79B8FF\">__init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            nn.Conv2d(in_channels, out_channels, \u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">bias\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">            if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> down\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">            else\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.ConvTranspose2d(in_channels, out_channels, \u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">bias\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.BatchNorm2d(out_channels) \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> bn \u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#79B8FF\"> None\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.dropout \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Dropout(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> dropout \u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#79B8FF\"> None\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.act \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.LeakyReLU(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> down \u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.ReLU()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#B392F0\"> forward\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, x):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.dropout:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.dropout(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.act(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">class\u003C/span>\u003Cspan style=\"color:#B392F0\"> Generator\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">nn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Module\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        super\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(Generator, \u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).\u003C/span>\u003Cspan style=\"color:#79B8FF\">__init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">bn\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down3 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down4 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down5 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down6 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down7 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down8 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">bn\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dropout\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1024\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dropout\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up3 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1024\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dropout\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up4 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1024\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up5 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1024\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up6 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up7 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">down\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.final \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Sequential(nn.ConvTranspose2d(\u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), nn.Tanh())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#B392F0\"> forward\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, x):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down1(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down2(d1)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d3 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down3(d2)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d4 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down4(d3)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d5 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down5(d4)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d6 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down6(d5)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d7 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down7(d6)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        d8 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.down8(d7)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up1(d8)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up2(torch.cat([u1, d7], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u3 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up3(torch.cat([u2, d6], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u4 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up4(torch.cat([u3, d5], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u5 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up5(torch.cat([u4, d4], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u6 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up6(torch.cat([u5, d3], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        u7 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.up7(torch.cat([u6, d2], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.final(torch.cat([u7, d1], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Implement the discriminator\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">class\u003C/span>\u003Cspan style=\"color:#B392F0\"> Discriminator\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">nn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Module\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        super\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(Discriminator, \u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).\u003C/span>\u003Cspan style=\"color:#79B8FF\">__init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.model \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Sequential(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">6\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">bn\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">64\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            UNetBlock(\u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            nn.Conv2d(\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            nn.Sigmoid(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#B392F0\"> forward\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, x, y):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.model(torch.cat([x, y], \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Define the loss functions and optimizers\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">criterion_gan \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.BCELoss()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">criterion_pixel \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.L1Loss()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># Load snapshot if available\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> get_latest_snapshot\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(output_dir):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    snapshots \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> glob.glob(os.path.join(output_dir, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"snapshot_epoch_*.pth\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#F97583\"> not\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> snapshots:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> None\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#79B8FF\"> max\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(snapshots, \u003C/span>\u003Cspan style=\"color:#FFAB70\">key\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">os.path.getctime)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> load_snapshot\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(generator, discriminator, g_optimizer, d_optimizer, snapshot_path):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    checkpoint \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.load(snapshot_path, \u003C/span>\u003Cspan style=\"color:#FFAB70\">map_location\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">device, \u003C/span>\u003Cspan style=\"color:#FFAB70\">weights_only\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">False\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    generator.load_state_dict(checkpoint[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"generator\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    discriminator.load_state_dict(checkpoint[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"discriminator\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    g_optimizer.load_state_dict(checkpoint[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"g_optimizer\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    d_optimizer.load_state_dict(checkpoint[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"d_optimizer\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    start_epoch \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> int\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(os.path.basename(snapshot_path).split(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"_\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)[\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">].split(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\".\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)[\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> start_epoch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># 6. Create the training loop\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> train\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(generator, discriminator, dataloader, args):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    g_optimizer \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> optim.Adam(generator.parameters(), \u003C/span>\u003Cspan style=\"color:#FFAB70\">lr\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.0002\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">betas\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.999\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    d_optimizer \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> optim.Adam(discriminator.parameters(), \u003C/span>\u003Cspan style=\"color:#FFAB70\">lr\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.0002\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">betas\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.999\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # Get fixed input/output for visualization\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    fixed_set \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> next\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">iter\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataloader))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    fixed_input \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> fixed_set[\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">][\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">].unsqueeze(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    fixed_target \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> fixed_set[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">][\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">].unsqueeze(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # fixed_input = next(iter(dataloader))[0][0].unsqueeze(0)  # Get a fixed input for visualization\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    start_epoch \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#F97583\"> not\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> args.restart:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        latest_snapshot \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> get_latest_snapshot(args.output_dir)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> latest_snapshot:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            start_epoch \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> load_snapshot(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                generator, discriminator, g_optimizer, d_optimizer, latest_snapshot\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">            print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Resuming training from epoch \u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">start_epoch\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        else\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">            print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"No snapshots found. Starting from scratch.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    else\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Restarting training from scratch.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> epoch \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(args.epochs):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, (input_img, target_img) \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(tqdm(dataloader)):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            input_img \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> input_img.to(device)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            target_img \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> target_img.to(device)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">            # Train Discriminator\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_optimizer.zero_grad()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            fake_img \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> generator(input_img)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_real \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> discriminator(input_img, target_img)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_fake \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> discriminator(input_img, fake_img.detach())\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_loss_real \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> criterion_gan(d_real, torch.ones_like(d_real))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_loss_fake \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> criterion_gan(d_fake, torch.zeros_like(d_fake))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_loss \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (d_loss_real \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> d_loss_fake) \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_loss.backward()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_optimizer.step()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">            # Train Generator\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_optimizer.zero_grad()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            fake_img \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> generator(input_img)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            d_fake \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> discriminator(input_img, fake_img)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_loss_gan \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> criterion_gan(d_fake, torch.ones_like(d_fake))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_loss_pixel \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> criterion_pixel(fake_img, target_img) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 100\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_loss \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> g_loss_gan \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> g_loss_pixel\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_loss.backward()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            g_optimizer.step()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">            if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">%\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> args.sample_interval \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">                with\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.no_grad():\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                    fake_img \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> generator(fixed_input.to(device))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                    img_sample \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                        (fixed_input.cpu(), fake_img.cpu(), fixed_target.cpu()), \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                    save_image(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                        img_sample,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">                        f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">args.output_dir\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/epoch_\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">epoch\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">_iter_\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">i\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.jpg\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                        nrow\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                        normalize\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (epoch \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#F97583\">%\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> args.snapshot_interval \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            torch.save(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                    \"generator\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: generator.state_dict(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                    \"discriminator\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: discriminator.state_dict(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                    \"g_optimizer\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: g_optimizer.state_dict(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">                    \"d_optimizer\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: d_optimizer.state_dict(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                },\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">                f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">args.output_dir\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/snapshot_epoch_\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">epoch \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.pth\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">            # Save to ONNX format\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            onnx_path \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#F97583\"> f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">args.output_dir\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">/generator_epoch_\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">epoch \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.onnx\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            generator.eval()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            dummy_input \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.randn(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).to(device)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            traced_script_module \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.jit.trace(generator, dummy_input)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            torch.onnx.export(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                traced_script_module,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                dummy_input,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                onnx_path,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                export_params\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                opset_version\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">11\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                do_constant_folding\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                input_names\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"input\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                output_names\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"output\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">],\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                dynamic_axes\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">{\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"input\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"batch_size\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">}, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"output\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: {\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">: \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"batch_size\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">}},\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">            print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"ONNX model exported to \u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">onnx_path\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            generator.train()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># 7. Implement the argument parser for configuration\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> parse_args\u003C/span>\u003Cspan style=\"color:#E1E4E8\">():\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> argparse.ArgumentParser(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        description\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Conditional GAN with pix2pix architecture\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--input_dir\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">str\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"datasets/trees\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Input dataset directory\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--output_dir\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">str\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"output\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Output directory for generated images\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--sample_interval\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">100\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Interval for saving sample images\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--snapshot_interval\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">        help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Interval for saving model snapshots\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--epochs\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">200\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Number of epochs to train\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"--batch_size\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">type\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">int\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">default\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">8\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Batch size\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    parser.add_argument(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">        \"--restart\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">action\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"store_true\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">help\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Restart training from scratch\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> parser.parse_args()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># 8. Set up the main function to run the training\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> main\u003C/span>\u003Cspan style=\"color:#E1E4E8\">():\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    args \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> parse_args()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    os.makedirs(args.output_dir, \u003C/span>\u003Cspan style=\"color:#FFAB70\">exist_ok\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    transform \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> transforms.Compose(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        [\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            transforms.Resize((\u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">512\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            transforms.ToTensor(),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            transforms.Normalize((\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">), (\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">0.5\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        ]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    dataset \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Pix2PixDataset(args.input_dir, \u003C/span>\u003Cspan style=\"color:#FFAB70\">transform\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">transform)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    dataloader \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> DataLoader(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        dataset, \u003C/span>\u003Cspan style=\"color:#FFAB70\">batch_size\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">args.batch_size, \u003C/span>\u003Cspan style=\"color:#FFAB70\">shuffle\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">num_workers\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    generator \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Generator().to(device)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    discriminator \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Discriminator().to(device)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    train(generator, discriminator, dataloader, args)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __name__\u003C/span>\u003Cspan style=\"color:#F97583\"> ==\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"__main__\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    device \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.device(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"cuda\"\u003C/span>\u003Cspan style=\"color:#F97583\"> if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cuda.is_available() \u003C/span>\u003Cspan style=\"color:#F97583\">else\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"cpu\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Using device: \u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">device\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    main()\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":1230,"localImagePaths":1237,"remoteImagePaths":1238,"frontmatter":1239,"imagePaths":1240},[1231,1233,1234],{"depth":29,"slug":1232,"text":1220},"onnx-image-model",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":1235,"text":1236},"training","Training",[],[],{"title":1220},[],"nodes/onnx-image-model.md","nodes/out",{"id":1242,"data":1244,"body":1249,"filePath":1250,"digest":1251,"rendered":1252,"legacyId":1262},{"title":1245,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1246,"sidebar":1247},"Out",[],{"hidden":17,"attrs":1248},{},"# Out\n\nThe Out node indicates what should come _out_ of your network, so the Out node should be the the last node in your network.\n\n- When using fullscreen mode, it will take the output of the out node.\n\nIn the Out node you can enable the \"stats\" option. This allows you to monitor the framerate of a running project.\n\nThe Out node is created by default in a new project so there's probably no need to create one (having more than one Out node doesn't make sense and is probably confusing).","src/content/docs/nodes/out.md","83e8bb5ce6d48c82",{"html":1253,"metadata":1254},"\u003Ch1 id=\"out\">Out\u003C/h1>\n\u003Cp>The Out node indicates what should come \u003Cem>out\u003C/em> of your network, so the Out node should be the the last node in your network.\u003C/p>\n\u003Cul>\n\u003Cli>When using fullscreen mode, it will take the output of the out node.\u003C/li>\n\u003C/ul>\n\u003Cp>In the Out node you can enable the “stats” option. This allows you to monitor the framerate of a running project.\u003C/p>\n\u003Cp>The Out node is created by default in a new project so there’s probably no need to create one (having more than one Out node doesn’t make sense and is probably confusing).\u003C/p>",{"headings":1255,"localImagePaths":1258,"remoteImagePaths":1259,"frontmatter":1260,"imagePaths":1261},[1256],{"depth":29,"slug":1257,"text":1245},"out",[],[],{"title":1245},[],"nodes/out.md","nodes/reduce-color",{"id":1263,"data":1265,"body":1270,"filePath":1271,"digest":1272,"rendered":1273,"legacyId":1285},{"title":1266,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1267,"sidebar":1268},"Reduce Color",[],{"hidden":17,"attrs":1269},{},"# Reduce Color\n\nThis node reduces the amount of colors of input image.\n\n## Parameters\n\n- **Reduce colors** Sets the amount of colors of the image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/reduce-colors.jpg\" alt=\"Figment reduce color node example\"/>","src/content/docs/nodes/reduce-color.md","3fa831754df040eb",{"html":1274,"metadata":1275},"\u003Ch1 id=\"reduce-color\">Reduce Color\u003C/h1>\n\u003Cp>This node reduces the amount of colors of input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Reduce colors\u003C/strong> Sets the amount of colors of the image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/reduce-colors.jpg\" alt=\"Figment reduce color node example\">",{"headings":1276,"localImagePaths":1281,"remoteImagePaths":1282,"frontmatter":1283,"imagePaths":1284},[1277,1279,1280],{"depth":29,"slug":1278,"text":1266},"reduce-color",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1266},[],"nodes/reduce-color.md","nodes/radial-distortion",{"id":1286,"data":1288,"body":1293,"filePath":1294,"digest":1295,"rendered":1296,"legacyId":1308},{"title":1289,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1290,"sidebar":1291},"Radial Distortion",[],{"hidden":17,"attrs":1292},{},"# Radial Distortion\n\nThe node computes a radial distortion on the input image.\n\n## Parameters\n\n- **Distortion**: The amount of distortion.\n- **Rotate**: The rotation of the distortion.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/radial.jpg\" alt=\"Figment radial distortion node example\"/>","src/content/docs/nodes/radial-distortion.md","ed68dd552ed02bfa",{"html":1297,"metadata":1298},"\u003Ch1 id=\"radial-distortion\">Radial Distortion\u003C/h1>\n\u003Cp>The node computes a radial distortion on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Distortion\u003C/strong>: The amount of distortion.\u003C/li>\n\u003Cli>\u003Cstrong>Rotate\u003C/strong>: The rotation of the distortion.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/radial.jpg\" alt=\"Figment radial distortion node example\">",{"headings":1299,"localImagePaths":1304,"remoteImagePaths":1305,"frontmatter":1306,"imagePaths":1307},[1300,1302,1303],{"depth":29,"slug":1301,"text":1289},"radial-distortion",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1289},[],"nodes/radial-distortion.md","nodes/resize",{"id":1309,"data":1311,"body":1316,"filePath":1317,"digest":1318,"rendered":1319,"legacyId":1330},{"title":1312,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1313,"sidebar":1314},"Resize",[],{"hidden":17,"attrs":1315},{},"# Resize\n\nResize the incoming image to the given size. The resize node can use different ways of fitting the content in the target size if they do not match the aspect:\n\n- **Fill**: The image will be stretched so it fits the image size exactly.\n- **Contain**: The image will fit in the target size without stretching, taking the smallest size and filling the rest with the background color.\n- **Cover**: The image will cover the target size, removing the left/right side or top/bottom side to fill the entire frame.\n\n## Parameters\n\n- **Width**: The target width, in pixels.\n- **Height**: The target height, in pixels.\n- **Fit**: The method to fit the image in the target size.\n- **Background**: The background color to use if the method is set to `contain`.","src/content/docs/nodes/resize.md","ff2c3ccaf23a510e",{"html":1320,"metadata":1321},"\u003Ch1 id=\"resize\">Resize\u003C/h1>\n\u003Cp>Resize the incoming image to the given size. The resize node can use different ways of fitting the content in the target size if they do not match the aspect:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Fill\u003C/strong>: The image will be stretched so it fits the image size exactly.\u003C/li>\n\u003Cli>\u003Cstrong>Contain\u003C/strong>: The image will fit in the target size without stretching, taking the smallest size and filling the rest with the background color.\u003C/li>\n\u003Cli>\u003Cstrong>Cover\u003C/strong>: The image will cover the target size, removing the left/right side or top/bottom side to fill the entire frame.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Width\u003C/strong>: The target width, in pixels.\u003C/li>\n\u003Cli>\u003Cstrong>Height\u003C/strong>: The target height, in pixels.\u003C/li>\n\u003Cli>\u003Cstrong>Fit\u003C/strong>: The method to fit the image in the target size.\u003C/li>\n\u003Cli>\u003Cstrong>Background\u003C/strong>: The background color to use if the method is set to \u003Ccode>contain\u003C/code>.\u003C/li>\n\u003C/ul>",{"headings":1322,"localImagePaths":1326,"remoteImagePaths":1327,"frontmatter":1328,"imagePaths":1329},[1323,1325],{"depth":29,"slug":1324,"text":1312},"resize",{"depth":32,"slug":131,"text":132},[],[],{"title":1312},[],"nodes/resize.md","nodes/rgb-cluster",{"id":1331,"data":1333,"body":1338,"filePath":1339,"digest":1340,"rendered":1341,"legacyId":1353},{"title":1334,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1335,"sidebar":1336},"Rgb Cluster",[],{"hidden":17,"attrs":1337},{},"# Rgb Cluster\n\nThe node returns the input image in a cluster of RGB.\n\n## Parameters\n\n## Example\n\n\u003Cimg src=\"/img/nodes/rgbCluster.jpg\" alt=\"Figment rgb cluster node example\"/>","src/content/docs/nodes/rgb-cluster.md","57c08952b53f0257",{"html":1342,"metadata":1343},"\u003Ch1 id=\"rgb-cluster\">Rgb Cluster\u003C/h1>\n\u003Cp>The node returns the input image in a cluster of RGB.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/rgbCluster.jpg\" alt=\"Figment rgb cluster node example\">",{"headings":1344,"localImagePaths":1349,"remoteImagePaths":1350,"frontmatter":1351,"imagePaths":1352},[1345,1347,1348],{"depth":29,"slug":1346,"text":1334},"rgb-cluster",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1334},[],"nodes/rgb-cluster.md","nodes/save-image",{"id":1354,"data":1356,"body":1361,"filePath":1362,"digest":1363,"rendered":1364,"legacyId":1375},{"title":1357,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1358,"sidebar":1359},"Save Image",[],{"hidden":17,"attrs":1360},{},"# Save Image\n\nThis node saves its input to an image output. By default, this works in coordination with the Render/Export functionality, only saving out images when you choose `File > Render`.\n\nHere's how that works:\n\n- Add a \"Save Image\" node to the end of your node chain.\n- In the properties, select the output folder.\n- Optionally, change the filename template. The `#####` will be replaced with the sequence number, e.g. `image-#####.png` will turn into `image-00001.png`, `image-00002.png`, and so on.\n- Go to File > Render, choose the amount of frames and frame rate.\n- This will render out the \"save image\" node\n\n## Parameters\n\n- **Enable** When to save the images. By default, images are saved when rendering/exporting. Change this to \"always\" to also save images during normal operation. Note that this can slow down the network. Change this to \"never\" to disable the export.\n- **Folder** Folder to image sequence\n- **Template** Image filename template. The `#####` will be replaced with the sequence number, e.g. `image-#####.png` will turn into `image-00001.png`, `image-00002.png`, and so on.","src/content/docs/nodes/save-image.md","738390b9e28d5180",{"html":1365,"metadata":1366},"\u003Ch1 id=\"save-image\">Save Image\u003C/h1>\n\u003Cp>This node saves its input to an image output. By default, this works in coordination with the Render/Export functionality, only saving out images when you choose \u003Ccode>File > Render\u003C/code>.\u003C/p>\n\u003Cp>Here’s how that works:\u003C/p>\n\u003Cul>\n\u003Cli>Add a “Save Image” node to the end of your node chain.\u003C/li>\n\u003Cli>In the properties, select the output folder.\u003C/li>\n\u003Cli>Optionally, change the filename template. The \u003Ccode>#####\u003C/code> will be replaced with the sequence number, e.g. \u003Ccode>image-#####.png\u003C/code> will turn into \u003Ccode>image-00001.png\u003C/code>, \u003Ccode>image-00002.png\u003C/code>, and so on.\u003C/li>\n\u003Cli>Go to File > Render, choose the amount of frames and frame rate.\u003C/li>\n\u003Cli>This will render out the “save image” node\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Enable\u003C/strong> When to save the images. By default, images are saved when rendering/exporting. Change this to “always” to also save images during normal operation. Note that this can slow down the network. Change this to “never” to disable the export.\u003C/li>\n\u003Cli>\u003Cstrong>Folder\u003C/strong> Folder to image sequence\u003C/li>\n\u003Cli>\u003Cstrong>Template\u003C/strong> Image filename template. The \u003Ccode>#####\u003C/code> will be replaced with the sequence number, e.g. \u003Ccode>image-#####.png\u003C/code> will turn into \u003Ccode>image-00001.png\u003C/code>, \u003Ccode>image-00002.png\u003C/code>, and so on.\u003C/li>\n\u003C/ul>",{"headings":1367,"localImagePaths":1371,"remoteImagePaths":1372,"frontmatter":1373,"imagePaths":1374},[1368,1370],{"depth":29,"slug":1369,"text":1357},"save-image",{"depth":32,"slug":131,"text":132},[],[],{"title":1357},[],"nodes/save-image.md","nodes/pixelate",{"id":1376,"data":1378,"body":1383,"filePath":1384,"digest":1385,"rendered":1386,"legacyId":1398},{"title":1379,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1380,"sidebar":1381},"Pixelate",[],{"hidden":17,"attrs":1382},{},"# Pixelate\n\nThis node pixelates the input image.\n\n## Parameters\n\n- **Amount X** The amount of pixels on the width of the image\n- **Amount Y** The amount of pixels on the height of the image\n\n## Example\n\n\u003Cimg src=\"/img/nodes/pixelate.jpg\" alt=\"Figment pixelate node example\"/>","src/content/docs/nodes/pixelate.md","90340eb2ef452c7f",{"html":1387,"metadata":1388},"\u003Ch1 id=\"pixelate\">Pixelate\u003C/h1>\n\u003Cp>This node pixelates the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Amount X\u003C/strong> The amount of pixels on the width of the image\u003C/li>\n\u003Cli>\u003Cstrong>Amount Y\u003C/strong> The amount of pixels on the height of the image\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/pixelate.jpg\" alt=\"Figment pixelate node example\">",{"headings":1389,"localImagePaths":1394,"remoteImagePaths":1395,"frontmatter":1396,"imagePaths":1397},[1390,1392,1393],{"depth":29,"slug":1391,"text":1379},"pixelate",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1379},[],"nodes/pixelate.md","nodes/segment-pose",{"id":1399,"data":1401,"body":1406,"filePath":1407,"digest":1408,"rendered":1409,"legacyId":1421},{"title":1402,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1403,"sidebar":1404},"segment pose",[],{"hidden":17,"attrs":1405},{},"## Segment Pose\n\nIsolate a single person from the background, either removing or keeping only the background.\n\n## Parameters\n\n- **Remove** Whether to remove the background or the foreground.","src/content/docs/nodes/segment-pose.md","122f7891ef446c0f",{"html":1410,"metadata":1411},"\u003Ch2 id=\"segment-pose\">Segment Pose\u003C/h2>\n\u003Cp>Isolate a single person from the background, either removing or keeping only the background.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Remove\u003C/strong> Whether to remove the background or the foreground.\u003C/li>\n\u003C/ul>",{"headings":1412,"localImagePaths":1417,"remoteImagePaths":1418,"frontmatter":1419,"imagePaths":1420},[1413,1416],{"depth":32,"slug":1414,"text":1415},"segment-pose","Segment Pose",{"depth":32,"slug":131,"text":132},[],[],{"title":1402},[],"nodes/segment-pose.md","nodes/send-osc",{"id":1422,"data":1424,"body":1429,"filePath":1430,"digest":1431,"rendered":1432,"legacyId":1462},{"title":1425,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1426,"sidebar":1427},"Send OSC",[],{"hidden":17,"attrs":1428},{},"# Send OSC\n\nThis node sends OSC messages to a specified address. The messages can be a number, string, or an object.\n\n:::tip\nTo see if the right messages are being sent, use the free [Protokol](https://hexler.net/protokol) app.\n:::\n\n## Send OSC + Landmark Detection\n\nThere is a special mode for sending landmark detection data from pose, hand, or face detection nodes. When connecting the output of a [Detect Pose](./detect-pose), Detect Hands, or Detect Face node to this node, it will send out all individual landmarks as separate messages.\n\n### Message Format\n\nMessages are formatted as: `/address/entityIndex/landmarkName`\n\nFor example, if the address is `/landmarks` and detecting a pose, the messages will be:\n\n```text\n/landmarks/0/nose 0.35431 0.24342 0.23423478 0.9823\n/landmarks/0/left_shoulder 0.25431 0.34342 0.13423478 0.9523\n```\n\nThe entity index allows tracking multiple detected entities (multiple people, hands, or faces). The values sent are `x`, `y`, `z`, and `visibility` (confidence that the landmark is visible).\n\n### Landmark Filter\n\nUse the Landmark Filter parameter to specify which landmarks to send. By default it sends everything (`*`). The filter supports:\n\n- **Named landmarks**: `right_shoulder, left_shoulder`\n- **Wildcards**: `*_shoulder` (matches all shoulders), `left_*` (matches all left-side landmarks)\n- **Numeric indices**: `0, 5, 10` (useful for face landmarks)\n- **Numeric ranges**: `0-20, 50-100` (useful for face landmarks)\n\n### Pose Landmarks\n\nHere is the full list of pose landmarks:\n\n```text\nnose,\nleft_eye_inner, left_eye, left_eye_outer, right_eye_inner, right_eye, right_eye_outer,\nleft_ear, right_ear, mouth_left, mouth_right,\nleft_shoulder, right_shoulder, left_elbow, right_elbow,\nleft_wrist, right_wrist, left_pinky, right_pinky, left_index, right_index, left_thumb, right_thumb,\nleft_hip, right_hip, left_knee, right_knee,\nleft_ankle, right_ankle, left_heel, right_heel, left_foot_index, right_foot_index\n```\n\n### Hand Landmarks\n\nHere is the full list of hand landmarks:\n\n```text\nwrist,\nthumb_cmc, thumb_mcp, thumb_ip, thumb_tip,\nindex_finger_mcp, index_finger_pip, index_finger_dip, index_finger_tip,\nmiddle_finger_mcp, middle_finger_pip, middle_finger_dip, middle_finger_tip,\nring_finger_mcp, ring_finger_pip, ring_finger_dip, ring_finger_tip,\npinky_mcp, pinky_pip, pinky_dip, pinky_tip\n```\n\n### Face Landmarks\n\nFace detection provides 468 landmarks using numeric indices (0-467). Use numeric filters to select specific regions. For example:\n\n- `0-20` for the face outline\n- `33, 133, 362, 263` for eye corners\n- `50-100` for a specific facial region\n\n## Parameters\n\n- **IP**: The IP address or host name of the machine to send to.\n- **Port**: The port to send to.\n- **Address**: The OSC address to send to. This should start with a `/`.\n- **Landmark Filter**: Filter which landmarks to send. Supports named landmarks, wildcards (`*`), numeric indices, and ranges. Default is `*` (send all).\n\n## Example\n\n\u003Cimg src=\"/img/nodes/send-osc-pose.jpg\" alt=\"Figment send osc node sending out pose data\"/>","src/content/docs/nodes/send-osc.md","5d7a6bc0dbf64c5e",{"html":1433,"metadata":1434},"\u003Ch1 id=\"send-osc\">Send OSC\u003C/h1>\n\u003Cp>This node sends OSC messages to a specified address. The messages can be a number, string, or an object.\u003C/p>\n\u003Cp>:::tip\nTo see if the right messages are being sent, use the free \u003Ca href=\"https://hexler.net/protokol\">Protokol\u003C/a> app.\n:::\u003C/p>\n\u003Ch2 id=\"send-osc--landmark-detection\">Send OSC + Landmark Detection\u003C/h2>\n\u003Cp>There is a special mode for sending landmark detection data from pose, hand, or face detection nodes. When connecting the output of a \u003Ca href=\"./detect-pose\">Detect Pose\u003C/a>, Detect Hands, or Detect Face node to this node, it will send out all individual landmarks as separate messages.\u003C/p>\n\u003Ch3 id=\"message-format\">Message Format\u003C/h3>\n\u003Cp>Messages are formatted as: \u003Ccode>/address/entityIndex/landmarkName\u003C/code>\u003C/p>\n\u003Cp>For example, if the address is \u003Ccode>/landmarks\u003C/code> and detecting a pose, the messages will be:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"text\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>/landmarks/0/nose 0.35431 0.24342 0.23423478 0.9823\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>/landmarks/0/left_shoulder 0.25431 0.34342 0.13423478 0.9523\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The entity index allows tracking multiple detected entities (multiple people, hands, or faces). The values sent are \u003Ccode>x\u003C/code>, \u003Ccode>y\u003C/code>, \u003Ccode>z\u003C/code>, and \u003Ccode>visibility\u003C/code> (confidence that the landmark is visible).\u003C/p>\n\u003Ch3 id=\"landmark-filter\">Landmark Filter\u003C/h3>\n\u003Cp>Use the Landmark Filter parameter to specify which landmarks to send. By default it sends everything (\u003Ccode>*\u003C/code>). The filter supports:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Named landmarks\u003C/strong>: \u003Ccode>right_shoulder, left_shoulder\u003C/code>\u003C/li>\n\u003Cli>\u003Cstrong>Wildcards\u003C/strong>: \u003Ccode>*_shoulder\u003C/code> (matches all shoulders), \u003Ccode>left_*\u003C/code> (matches all left-side landmarks)\u003C/li>\n\u003Cli>\u003Cstrong>Numeric indices\u003C/strong>: \u003Ccode>0, 5, 10\u003C/code> (useful for face landmarks)\u003C/li>\n\u003Cli>\u003Cstrong>Numeric ranges\u003C/strong>: \u003Ccode>0-20, 50-100\u003C/code> (useful for face landmarks)\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"pose-landmarks\">Pose Landmarks\u003C/h3>\n\u003Cp>Here is the full list of pose landmarks:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"text\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>nose,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_eye_inner, left_eye, left_eye_outer, right_eye_inner, right_eye, right_eye_outer,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_ear, right_ear, mouth_left, mouth_right,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_shoulder, right_shoulder, left_elbow, right_elbow,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_wrist, right_wrist, left_pinky, right_pinky, left_index, right_index, left_thumb, right_thumb,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_hip, right_hip, left_knee, right_knee,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>left_ankle, right_ankle, left_heel, right_heel, left_foot_index, right_foot_index\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"hand-landmarks\">Hand Landmarks\u003C/h3>\n\u003Cp>Here is the full list of hand landmarks:\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"text\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>wrist,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>thumb_cmc, thumb_mcp, thumb_ip, thumb_tip,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>index_finger_mcp, index_finger_pip, index_finger_dip, index_finger_tip,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>middle_finger_mcp, middle_finger_pip, middle_finger_dip, middle_finger_tip,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>ring_finger_mcp, ring_finger_pip, ring_finger_dip, ring_finger_tip,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>pinky_mcp, pinky_pip, pinky_dip, pinky_tip\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"face-landmarks\">Face Landmarks\u003C/h3>\n\u003Cp>Face detection provides 468 landmarks using numeric indices (0-467). Use numeric filters to select specific regions. For example:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>0-20\u003C/code> for the face outline\u003C/li>\n\u003Cli>\u003Ccode>33, 133, 362, 263\u003C/code> for eye corners\u003C/li>\n\u003Cli>\u003Ccode>50-100\u003C/code> for a specific facial region\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>IP\u003C/strong>: The IP address or host name of the machine to send to.\u003C/li>\n\u003Cli>\u003Cstrong>Port\u003C/strong>: The port to send to.\u003C/li>\n\u003Cli>\u003Cstrong>Address\u003C/strong>: The OSC address to send to. This should start with a \u003Ccode>/\u003C/code>.\u003C/li>\n\u003Cli>\u003Cstrong>Landmark Filter\u003C/strong>: Filter which landmarks to send. Supports named landmarks, wildcards (\u003Ccode>*\u003C/code>), numeric indices, and ranges. Default is \u003Ccode>*\u003C/code> (send all).\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/send-osc-pose.jpg\" alt=\"Figment send osc node sending out pose data\">",{"headings":1435,"localImagePaths":1458,"remoteImagePaths":1459,"frontmatter":1460,"imagePaths":1461},[1436,1438,1441,1444,1447,1450,1453,1456,1457],{"depth":29,"slug":1437,"text":1425},"send-osc",{"depth":32,"slug":1439,"text":1440},"send-osc--landmark-detection","Send OSC + Landmark Detection",{"depth":178,"slug":1442,"text":1443},"message-format","Message Format",{"depth":178,"slug":1445,"text":1446},"landmark-filter","Landmark Filter",{"depth":178,"slug":1448,"text":1449},"pose-landmarks","Pose Landmarks",{"depth":178,"slug":1451,"text":1452},"hand-landmarks","Hand Landmarks",{"depth":178,"slug":1454,"text":1455},"face-landmarks","Face Landmarks",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1425},[],"nodes/send-osc.md","nodes/sepia",{"id":1463,"data":1465,"body":1470,"filePath":1471,"digest":1472,"rendered":1473,"legacyId":1485},{"title":1466,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1467,"sidebar":1468},"Sepia",[],{"hidden":17,"attrs":1469},{},"# Sepia\n\nThe node renders a sepia filter on the input image.\n\n## Parameters\n\n- **Sepia Factor**: The amount of fx applied to the image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/sepia.jpg\" alt=\"Figment sepia node example\"/>","src/content/docs/nodes/sepia.md","e9af23ef0600620a",{"html":1474,"metadata":1475},"\u003Ch1 id=\"sepia\">Sepia\u003C/h1>\n\u003Cp>The node renders a sepia filter on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Sepia Factor\u003C/strong>: The amount of fx applied to the image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/sepia.jpg\" alt=\"Figment sepia node example\">",{"headings":1476,"localImagePaths":1481,"remoteImagePaths":1482,"frontmatter":1483,"imagePaths":1484},[1477,1479,1480],{"depth":29,"slug":1478,"text":1466},"sepia",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1466},[],"nodes/sepia.md","nodes/sharpen",{"id":1486,"data":1488,"body":1493,"filePath":1494,"digest":1495,"rendered":1496,"legacyId":1508},{"title":1489,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1490,"sidebar":1491},"Sharpen",[],{"hidden":17,"attrs":1492},{},"# Sharpen\n\nThe node sharpens the input image.\n\n## Parameters\n\n- **Amount**: The amount of sharpening.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/sharpen.jpg\" alt=\"Figment sharpen node example\"/>","src/content/docs/nodes/sharpen.md","d5c23960f7b23448",{"html":1497,"metadata":1498},"\u003Ch1 id=\"sharpen\">Sharpen\u003C/h1>\n\u003Cp>The node sharpens the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Amount\u003C/strong>: The amount of sharpening.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/sharpen.jpg\" alt=\"Figment sharpen node example\">",{"headings":1499,"localImagePaths":1504,"remoteImagePaths":1505,"frontmatter":1506,"imagePaths":1507},[1500,1502,1503],{"depth":29,"slug":1501,"text":1489},"sharpen",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1489},[],"nodes/sharpen.md","nodes/threshold",{"id":1509,"data":1511,"body":1516,"filePath":1517,"digest":1518,"rendered":1519,"legacyId":1531},{"title":1512,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1513,"sidebar":1514},"Threshold",[],{"hidden":17,"attrs":1515},{},"# Threshold\n\nThis node changes the brightness threshold of an input image.\n\n## Parameters\n\n- **Threshold** The amount of threshold.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/threshold.jpg\" alt=\"Figment threshold node example\"/>","src/content/docs/nodes/threshold.md","8c53c1a12cec860a",{"html":1520,"metadata":1521},"\u003Ch1 id=\"threshold\">Threshold\u003C/h1>\n\u003Cp>This node changes the brightness threshold of an input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Threshold\u003C/strong> The amount of threshold.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/threshold.jpg\" alt=\"Figment threshold node example\">",{"headings":1522,"localImagePaths":1527,"remoteImagePaths":1528,"frontmatter":1529,"imagePaths":1530},[1523,1525,1526],{"depth":29,"slug":1524,"text":1512},"threshold",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1512},[],"nodes/threshold.md","nodes/squares",{"id":1532,"data":1534,"body":1539,"filePath":1540,"digest":1541,"rendered":1542,"legacyId":1554},{"title":1535,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1536,"sidebar":1537},"Squares",[],{"hidden":17,"attrs":1538},{},"# Squares\n\nThis node returns the input image as squares.\n\n## Parameters\n\n- **Squares** The amount of squares.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/squares.jpg\" alt=\"Figment squares node example\"/>","src/content/docs/nodes/squares.md","884c14a18af501e1",{"html":1543,"metadata":1544},"\u003Ch1 id=\"squares\">Squares\u003C/h1>\n\u003Cp>This node returns the input image as squares.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Squares\u003C/strong> The amount of squares.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/squares.jpg\" alt=\"Figment squares node example\">",{"headings":1545,"localImagePaths":1550,"remoteImagePaths":1551,"frontmatter":1552,"imagePaths":1553},[1546,1548,1549],{"depth":29,"slug":1547,"text":1535},"squares",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1535},[],"nodes/squares.md","nodes/sobel",{"id":1555,"data":1557,"body":1562,"filePath":1563,"digest":1564,"rendered":1565,"legacyId":1576},{"title":1558,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1559,"sidebar":1560},"Sobel",[],{"hidden":17,"attrs":1561},{},"# Sobel\n\nThis node calculates sobel edges on an input image.\n\nThe node doesn't have any parameters.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/sobel.jpg\" alt=\"Figment sobel node example\"/>","src/content/docs/nodes/sobel.md","c763e4b9f04f7ee3",{"html":1566,"metadata":1567},"\u003Ch1 id=\"sobel\">Sobel\u003C/h1>\n\u003Cp>This node calculates sobel edges on an input image.\u003C/p>\n\u003Cp>The node doesn’t have any parameters.\u003C/p>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/sobel.jpg\" alt=\"Figment sobel node example\">",{"headings":1568,"localImagePaths":1572,"remoteImagePaths":1573,"frontmatter":1574,"imagePaths":1575},[1569,1571],{"depth":29,"slug":1570,"text":1558},"sobel",{"depth":32,"slug":213,"text":214},[],[],{"title":1558},[],"nodes/sobel.md","nodes/stack",{"id":1577,"data":1579,"body":1584,"filePath":1585,"digest":1586,"rendered":1587,"legacyId":1598},{"title":1580,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1581,"sidebar":1582},"Stack",[],{"hidden":17,"attrs":1583},{},"# Stack\n\nThe node stacks two images horizontally or vertically. It needs two images as input.\n\n## Parameters\n\n- **Direction**: The direction in which the stacking needs to occur. Options are `horizontally` and `vertically`.","src/content/docs/nodes/stack.md","9a1f43aef9e93946",{"html":1588,"metadata":1589},"\u003Ch1 id=\"stack\">Stack\u003C/h1>\n\u003Cp>The node stacks two images horizontally or vertically. It needs two images as input.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Direction\u003C/strong>: The direction in which the stacking needs to occur. Options are \u003Ccode>horizontally\u003C/code> and \u003Ccode>vertically\u003C/code>.\u003C/li>\n\u003C/ul>",{"headings":1590,"localImagePaths":1594,"remoteImagePaths":1595,"frontmatter":1596,"imagePaths":1597},[1591,1593],{"depth":29,"slug":1592,"text":1580},"stack",{"depth":32,"slug":131,"text":132},[],[],{"title":1580},[],"nodes/stack.md","nodes/trail",{"id":1599,"data":1601,"body":1606,"filePath":1607,"digest":1608,"rendered":1609,"legacyId":1622},{"title":1602,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1603,"sidebar":1604},"Trail",[],{"hidden":17,"attrs":1605},{},"# Trail\n\nThis node does not erase the previous input image, creating a trailing effect by accumulating frames over time.\n\n## Parameters\n\n- **In** The input image to accumulate.\n- **Fade** Controls how quickly the trail fades away using Monte Carlo sampling. Each pixel has a probability of being cleared based on this value. Range: 0.0 to 1.0. A value of `0` means no fading (infinite trail), while higher values fade faster. Uses a power curve so low values produce very slow, subtle fades.\n- **Clear** Button to clear the accumulated trail.\n\n## Outputs\n\n- **Out** The accumulated trail image.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/trail.jpg\" alt=\"Figment trail node example\"/>","src/content/docs/nodes/trail.md","a87a2b1565c3fc32",{"html":1610,"metadata":1611},"\u003Ch1 id=\"trail\">Trail\u003C/h1>\n\u003Cp>This node does not erase the previous input image, creating a trailing effect by accumulating frames over time.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>In\u003C/strong> The input image to accumulate.\u003C/li>\n\u003Cli>\u003Cstrong>Fade\u003C/strong> Controls how quickly the trail fades away using Monte Carlo sampling. Each pixel has a probability of being cleared based on this value. Range: 0.0 to 1.0. A value of \u003Ccode>0\u003C/code> means no fading (infinite trail), while higher values fade faster. Uses a power curve so low values produce very slow, subtle fades.\u003C/li>\n\u003Cli>\u003Cstrong>Clear\u003C/strong> Button to clear the accumulated trail.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"outputs\">Outputs\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Out\u003C/strong> The accumulated trail image.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/trail.jpg\" alt=\"Figment trail node example\">",{"headings":1612,"localImagePaths":1618,"remoteImagePaths":1619,"frontmatter":1620,"imagePaths":1621},[1613,1615,1616,1617],{"depth":29,"slug":1614,"text":1602},"trail",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":417,"text":418},{"depth":32,"slug":213,"text":214},[],[],{"title":1602},[],"nodes/trail.md","nodes/unsplash",{"id":1623,"data":1625,"body":1630,"filePath":1631,"digest":1632,"rendered":1633,"legacyId":1644},{"title":1626,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1627,"sidebar":1628},"Unsplash",[],{"hidden":17,"attrs":1629},{},"# Unsplash\n\nThis node retrieves an image from unsplash. Unsplash is an online database that allows to search on free high-resolution images.\n\n## Parameters\n\n- **Query** The term to search for in the database.\n- **Width** The width of the image.\n- **Height** The height of the image.","src/content/docs/nodes/unsplash.md","fb96fd5890710295",{"html":1634,"metadata":1635},"\u003Ch1 id=\"unsplash\">Unsplash\u003C/h1>\n\u003Cp>This node retrieves an image from unsplash. Unsplash is an online database that allows to search on free high-resolution images.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Query\u003C/strong> The term to search for in the database.\u003C/li>\n\u003Cli>\u003Cstrong>Width\u003C/strong> The width of the image.\u003C/li>\n\u003Cli>\u003Cstrong>Height\u003C/strong> The height of the image.\u003C/li>\n\u003C/ul>",{"headings":1636,"localImagePaths":1640,"remoteImagePaths":1641,"frontmatter":1642,"imagePaths":1643},[1637,1639],{"depth":29,"slug":1638,"text":1626},"unsplash",{"depth":32,"slug":131,"text":132},[],[],{"title":1626},[],"nodes/unsplash.md","nodes/transform",{"id":1645,"data":1647,"body":1652,"filePath":1653,"digest":1654,"rendered":1655,"legacyId":1667},{"title":1648,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1649,"sidebar":1650},"Transform",[],{"hidden":17,"attrs":1651},{},"# Transform\n\nThis node can be used to apply transformations to the input image, meaning it can translate, scale, or rotate the image.\n\n:::tip\nTo flip an image horizontally, set the `Scale X` parameter to `-1.0`.\u003Cbr/>To flip an image vertically, set the `Scale Y` parameter to `-1.0`.\n:::\n\n## Parameters\n\n- **Translate X** Sets the amount of translation on the X axis.\n- **Translate Y** Sets the amount of translation on the Y axis.\n- **Scale X** Sets the scale amount of the width of the canvas.\n- **Scale Y** Sets the scale amount of the height of the canvas.\n- **Rotate** Sets the angle for rotating the canvas. Angle is set as degrees so values go from `0.0` or `360.0`\n\n## Example\n\n\u003Cimg src=\"/img/nodes/transform.jpg\" alt=\"Figment transform node example\"/>","src/content/docs/nodes/transform.md","42b9184b88636074",{"html":1656,"metadata":1657},"\u003Ch1 id=\"transform\">Transform\u003C/h1>\n\u003Cp>This node can be used to apply transformations to the input image, meaning it can translate, scale, or rotate the image.\u003C/p>\n\u003Cp>:::tip\nTo flip an image horizontally, set the \u003Ccode>Scale X\u003C/code> parameter to \u003Ccode>-1.0\u003C/code>.\u003Cbr>To flip an image vertically, set the \u003Ccode>Scale Y\u003C/code> parameter to \u003Ccode>-1.0\u003C/code>.\n:::\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Translate X\u003C/strong> Sets the amount of translation on the X axis.\u003C/li>\n\u003Cli>\u003Cstrong>Translate Y\u003C/strong> Sets the amount of translation on the Y axis.\u003C/li>\n\u003Cli>\u003Cstrong>Scale X\u003C/strong> Sets the scale amount of the width of the canvas.\u003C/li>\n\u003Cli>\u003Cstrong>Scale Y\u003C/strong> Sets the scale amount of the height of the canvas.\u003C/li>\n\u003Cli>\u003Cstrong>Rotate\u003C/strong> Sets the angle for rotating the canvas. Angle is set as degrees so values go from \u003Ccode>0.0\u003C/code> or \u003Ccode>360.0\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/transform.jpg\" alt=\"Figment transform node example\">",{"headings":1658,"localImagePaths":1663,"remoteImagePaths":1664,"frontmatter":1665,"imagePaths":1666},[1659,1661,1662],{"depth":29,"slug":1660,"text":1648},"transform",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1648},[],"nodes/transform.md","nodes/vignette",{"id":1668,"data":1670,"body":1675,"filePath":1676,"digest":1677,"rendered":1678,"legacyId":1690},{"title":1671,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1672,"sidebar":1673},"Vignette",[],{"hidden":17,"attrs":1674},{},"# Vignette\n\nThe node renders a vignette on the input image.\n\n## Parameters\n\n- **Radius**: The radius of the vignette.\n- **Center X**: The horizontal location of the vignette.\n- **Center Y**: The vertical location of the vignette.\n\n## Example\n\n\u003Cimg src=\"/img/nodes/vignette.jpg\" alt=\"Figment vignette node example\"/>","src/content/docs/nodes/vignette.md","0d5e4f40e2b7c861",{"html":1679,"metadata":1680},"\u003Ch1 id=\"vignette\">Vignette\u003C/h1>\n\u003Cp>The node renders a vignette on the input image.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Radius\u003C/strong>: The radius of the vignette.\u003C/li>\n\u003Cli>\u003Cstrong>Center X\u003C/strong>: The horizontal location of the vignette.\u003C/li>\n\u003Cli>\u003Cstrong>Center Y\u003C/strong>: The vertical location of the vignette.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"example\">Example\u003C/h2>\n\u003Cimg src=\"/img/nodes/vignette.jpg\" alt=\"Figment vignette node example\">",{"headings":1681,"localImagePaths":1686,"remoteImagePaths":1687,"frontmatter":1688,"imagePaths":1689},[1682,1684,1685],{"depth":29,"slug":1683,"text":1671},"vignette",{"depth":32,"slug":131,"text":132},{"depth":32,"slug":213,"text":214},[],[],{"title":1671},[],"nodes/vignette.md","nodes/webcam-image",{"id":1691,"data":1693,"body":1698,"filePath":1699,"digest":1700,"rendered":1701,"legacyId":1712},{"title":1694,"editUrl":15,"template":16,"pagefind":15,"draft":17,"head":1695,"sidebar":1696},"Webcam Image",[],{"hidden":17,"attrs":1697},{},"# Webcam Image\n\nThis node captures the webcam stream.\n\n## Parameters\n\n- **Frame Rate** The speed in fps.\n- **Camera** Switch between the available cameras.","src/content/docs/nodes/webcam-image.md","1e6c0cc69b4e4131",{"html":1702,"metadata":1703},"\u003Ch1 id=\"webcam-image\">Webcam Image\u003C/h1>\n\u003Cp>This node captures the webcam stream.\u003C/p>\n\u003Ch2 id=\"parameters\">Parameters\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Frame Rate\u003C/strong> The speed in fps.\u003C/li>\n\u003Cli>\u003Cstrong>Camera\u003C/strong> Switch between the available cameras.\u003C/li>\n\u003C/ul>",{"headings":1704,"localImagePaths":1708,"remoteImagePaths":1709,"frontmatter":1710,"imagePaths":1711},[1705,1707],{"depth":29,"slug":1706,"text":1694},"webcam-image",{"depth":32,"slug":131,"text":132},[],[],{"title":1694},[],"nodes/webcam-image.md"]